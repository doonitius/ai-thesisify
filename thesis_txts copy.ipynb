{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\doonl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\doonl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import snowball\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from keras import *\n",
    "from keras import layers\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PyPDF2\n",
    "from glob import glob, glob1\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
      "C:\\Users\\doonl\\AppData\\Local\\Temp\\ipykernel_25912\\2526147678.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>[artificial intelligence, combinatorial optimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Word segmentation is an important pre-processi...</td>\n",
       "      <td>[attention, deep learning, thai language, word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Loss estimation is considerably significant fo...</td>\n",
       "      <td>[network resource planning, traffic model, que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>This thesis  presents a computational study on...</td>\n",
       "      <td>[audio-synchronization, facial expression, fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>The financial problem is a challenging task fo...</td>\n",
       "      <td>[deep learning, distributional reinforcement l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Since a classical computer has a limitation to...</td>\n",
       "      <td>[circuit depth, computing wall-time, quantum r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Hyperparameter Tuning chooses optimal hyperpar...</td>\n",
       "      <td>[abc, anova, classification, grid search, hype...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>The honeybee is a social insect that communica...</td>\n",
       "      <td>[convolutional neural networks, feature extrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>System logs contain the complete information o...</td>\n",
       "      <td>[alice, anomaly detection, cern, convolutional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>Handling an imbalanced class problem is a chal...</td>\n",
       "      <td>[classification, imbalanced dataset, machine l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Machine learning and predictive modeling have ...</td>\n",
       "      <td>[genetic algorithm, gradient boosting, hyper-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Resource estimation is a technique that has ma...</td>\n",
       "      <td>[alice, cern, regression, resource estimation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Due to the emerging of Long Short-Term Memory ...</td>\n",
       "      <td>[deep learning, long short term memory, neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>Since the spread of Coronavirus disease or Cov...</td>\n",
       "      <td>[abstractive text summarization, coronavirus, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>This thesis proposed a speech production acqui...</td>\n",
       "      <td>[acoustic-to-articulatory inversion, deep lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>In recent studies, many NLP tasks could gain b...</td>\n",
       "      <td>[feature extraction, natural language processi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>The number of documents in a dataset available...</td>\n",
       "      <td>[document mining, graph analysis, graph cluste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>[abstract meaning representation, bert, corefe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           abstract  \\\n",
       "0    1  In the logistic management, the cost reduction...   \n",
       "1   10  Word segmentation is an important pre-processi...   \n",
       "2   11  Loss estimation is considerably significant fo...   \n",
       "3   12  This thesis  presents a computational study on...   \n",
       "4   13  The financial problem is a challenging task fo...   \n",
       "5   14  Since a classical computer has a limitation to...   \n",
       "6   15  Hyperparameter Tuning chooses optimal hyperpar...   \n",
       "7   16  The honeybee is a social insect that communica...   \n",
       "8   17  System logs contain the complete information o...   \n",
       "9   18  Handling an imbalanced class problem is a chal...   \n",
       "10   2  Machine learning and predictive modeling have ...   \n",
       "11   3  Resource estimation is a technique that has ma...   \n",
       "12   4  Due to the emerging of Long Short-Term Memory ...   \n",
       "13   5  Since the spread of Coronavirus disease or Cov...   \n",
       "14   6  This thesis proposed a speech production acqui...   \n",
       "15   7  In recent studies, many NLP tasks could gain b...   \n",
       "16   8  The number of documents in a dataset available...   \n",
       "17   9  Understanding and interpretation of legislativ...   \n",
       "\n",
       "                                             keywords  \n",
       "0   [artificial intelligence, combinatorial optimi...  \n",
       "1   [attention, deep learning, thai language, word...  \n",
       "2   [network resource planning, traffic model, que...  \n",
       "3   [audio-synchronization, facial expression, fac...  \n",
       "4   [deep learning, distributional reinforcement l...  \n",
       "5   [circuit depth, computing wall-time, quantum r...  \n",
       "6   [abc, anova, classification, grid search, hype...  \n",
       "7   [convolutional neural networks, feature extrac...  \n",
       "8   [alice, anomaly detection, cern, convolutional...  \n",
       "9   [classification, imbalanced dataset, machine l...  \n",
       "10  [genetic algorithm, gradient boosting, hyper-p...  \n",
       "11  [alice, cern, regression, resource estimation,...  \n",
       "12  [deep learning, long short term memory, neural...  \n",
       "13  [abstractive text summarization, coronavirus, ...  \n",
       "14  [acoustic-to-articulatory inversion, deep lear...  \n",
       "15  [feature extraction, natural language processi...  \n",
       "16  [document mining, graph analysis, graph cluste...  \n",
       "17  [abstract meaning representation, bert, corefe...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = './txtFile/'\n",
    "\n",
    "data = pd.DataFrame(columns=['id', 'abstract', 'keywords'])\n",
    "for i in glob1('./txtFile/', '*'):\n",
    "    with open(path + i, 'r', encoding='utf8') as f:\n",
    "        doc_id = i.split('.')[-2]\n",
    "        readingAbstract = False\n",
    "        readingKeywords = False\n",
    "        abstract_sent = []\n",
    "        keyword_list = []\n",
    "        keyword_clean = []\n",
    "        for j in f:\n",
    "            line = j.strip()\n",
    "            if line == 'Keywords:' and readingKeywords is False:\n",
    "                readingKeywords = True\n",
    "                readingAbstract = False\n",
    "                continue\n",
    "            if line == 'Abstract' and readingAbstract is False:\n",
    "                readingAbstract = True\n",
    "                continue\n",
    "            if readingAbstract is True:\n",
    "                abstract_sent.append(line)\n",
    "            if readingKeywords is True:\n",
    "                keyword_list.append(line)\n",
    "        abstract_para = ' '.join(abstract_sent)\n",
    "        keyword_split = keyword_list[0].split(',')\n",
    "        # print(abstract_para)\n",
    "        # print(keyword_split)\n",
    "        for x in keyword_split:\n",
    "            keyword_lower = x.lower()\n",
    "            keyword_no_space = keyword_lower.replace(' ', '')\n",
    "            keyword_underscore = keyword_no_space.replace('_', ' ')\n",
    "            keyword_clean.append(keyword_underscore)\n",
    "        # print(keyword_clean)\n",
    "\n",
    "        data = data.append({\"id\": doc_id, \"abstract\": abstract_para, \"keywords\": keyword_clean}, ignore_index=True)\n",
    "    f.close()\n",
    "\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          18\n",
       "abstract    18\n",
       "keywords    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_explode = data.explode('keywords').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>artificial intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>combinatorial optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>vehicle routing problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>hard time windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>meta heuristic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>coreference resolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>legal processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>natural language processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>semantic meaning representations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>spanbert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           abstract  \\\n",
       "0   1  In the logistic management, the cost reduction...   \n",
       "1   1  In the logistic management, the cost reduction...   \n",
       "2   1  In the logistic management, the cost reduction...   \n",
       "3   1  In the logistic management, the cost reduction...   \n",
       "4   1  In the logistic management, the cost reduction...   \n",
       ".. ..                                                ...   \n",
       "78  9  Understanding and interpretation of legislativ...   \n",
       "79  9  Understanding and interpretation of legislativ...   \n",
       "80  9  Understanding and interpretation of legislativ...   \n",
       "81  9  Understanding and interpretation of legislativ...   \n",
       "82  9  Understanding and interpretation of legislativ...   \n",
       "\n",
       "                            keywords  \n",
       "0            artificial intelligence  \n",
       "1         combinatorial optimization  \n",
       "2            vehicle routing problem  \n",
       "3                  hard time windows  \n",
       "4                     meta heuristic  \n",
       "..                               ...  \n",
       "78            coreference resolution  \n",
       "79                  legal processing  \n",
       "80       natural language processing  \n",
       "81  semantic meaning representations  \n",
       "82                          spanbert  \n",
       "\n",
       "[83 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['artificial intelligence', 'combinatorial optimization',\n",
       "       'vehicle routing problem', 'hard time windows', 'meta heuristic',\n",
       "       'genetic algorithm', 'attention', 'deep learning', 'thai language',\n",
       "       'word segmentation', 'network resource planning', 'traffic model',\n",
       "       'queuing network', 'stochastic model', 'audio-synchronization',\n",
       "       'facial expression', 'facial movement', 'speech production',\n",
       "       'distributional reinforcement learning', 'portfolio management',\n",
       "       'circuit depth', 'computing wall-time', 'quantum random walk',\n",
       "       'quantum computer simulator', 'abc', 'anova', 'classification',\n",
       "       'grid search', 'hyperparameter tuning', 'random search',\n",
       "       'convolutional neural networks', 'feature extraction',\n",
       "       'hissing signal', 'machine learning', 'monitoring beehive',\n",
       "       'alice', 'anomaly detection', 'cern',\n",
       "       'convolutional neural network', 'self-attention', 'transformer',\n",
       "       'imbalanced dataset', 'oversampling', 'probabilistic distribution',\n",
       "       'gradient boosting', 'hyper-parameter tuning', 'optimization',\n",
       "       'regression', 'resource estimation', 'resource planning',\n",
       "       'long short term memory', 'neural language model',\n",
       "       'natural language processing', 'abstractive text summarization',\n",
       "       'coronavirus', 'covid-19', 'acoustic-to-articulatory inversion',\n",
       "       'speech production acquisition model', 'topic modeling',\n",
       "       'document mining', 'graph analysis', 'graph clustering',\n",
       "       'information retrieval system', 'network science',\n",
       "       'abstract meaning representation', 'bert',\n",
       "       'coreference resolution', 'legal processing',\n",
       "       'semantic meaning representations', 'spanbert'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_u = data_explode['keywords'].unique()\n",
    "data_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame._add_numeric_operations.<locals>.any of 0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "      ...  \n",
       "78    False\n",
       "79    False\n",
       "80     True\n",
       "81    False\n",
       "82    False\n",
       "Name: keywords, Length: 83, dtype: bool>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_explode['keywords'].duplicated().any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>combinatorial optimization</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>vehicle routing problem</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>hard time windows</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In the logistic management, the cost reduction...</td>\n",
       "      <td>meta heuristic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>coreference resolution</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>legal processing</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>natural language processing</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>semantic meaning representations</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>9</td>\n",
       "      <td>Understanding and interpretation of legislativ...</td>\n",
       "      <td>spanbert</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           abstract  \\\n",
       "0   1  In the logistic management, the cost reduction...   \n",
       "1   1  In the logistic management, the cost reduction...   \n",
       "2   1  In the logistic management, the cost reduction...   \n",
       "3   1  In the logistic management, the cost reduction...   \n",
       "4   1  In the logistic management, the cost reduction...   \n",
       ".. ..                                                ...   \n",
       "78  9  Understanding and interpretation of legislativ...   \n",
       "79  9  Understanding and interpretation of legislativ...   \n",
       "80  9  Understanding and interpretation of legislativ...   \n",
       "81  9  Understanding and interpretation of legislativ...   \n",
       "82  9  Understanding and interpretation of legislativ...   \n",
       "\n",
       "                            keywords  word_id  \n",
       "0            artificial intelligence        0  \n",
       "1         combinatorial optimization        1  \n",
       "2            vehicle routing problem        2  \n",
       "3                  hard time windows        3  \n",
       "4                     meta heuristic        4  \n",
       "..                               ...      ...  \n",
       "78            coreference resolution       66  \n",
       "79                  legal processing       67  \n",
       "80       natural language processing       52  \n",
       "81  semantic meaning representations       68  \n",
       "82                          spanbert       69  \n",
       "\n",
       "[83 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_explode['word_id'] = pd.factorize(data_explode.keywords)[0]\n",
    "data_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>Word segmentation is an important pre-processi...</td>\n",
       "      <td>deep learning</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>The financial problem is a challenging task fo...</td>\n",
       "      <td>deep learning</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>Due to the emerging of Long Short-Term Memory ...</td>\n",
       "      <td>deep learning</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>This thesis proposed a speech production acqui...</td>\n",
       "      <td>deep learning</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           abstract       keywords  \\\n",
       "7   10  Word segmentation is an important pre-processi...  deep learning   \n",
       "19  13  The financial problem is a challenging task fo...  deep learning   \n",
       "57   4  Due to the emerging of Long Short-Term Memory ...  deep learning   \n",
       "66   6  This thesis proposed a speech production acqui...  deep learning   \n",
       "\n",
       "    word_id  \n",
       "7         7  \n",
       "19        7  \n",
       "57        7  \n",
       "66        7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_explode[data_explode['keywords'] == 'deep learning'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.337349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.157476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_id\n",
       "count  83.000000\n",
       "mean   33.337349\n",
       "std    20.157476\n",
       "min     0.000000\n",
       "25%    15.500000\n",
       "50%    33.000000\n",
       "75%    51.500000\n",
       "max    69.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_explode.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data_explode['abstract'].values)\n",
    "y = np.array(data_explode['word_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.',\n",
       "       'The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.',\n",
       "       'The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.',\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       \"This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakers’ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /ɛ:/, /ɯ:/, /ɤ:/, /o:/, and /ɔ:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the model’s estimation, using a vowel identification model. While this model was perfectly identifying ฃ the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.\",\n",
       "       \"This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakers’ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /ɛ:/, /ɯ:/, /ɤ:/, /o:/, and /ɔ:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the model’s estimation, using a vowel identification model. While this model was perfectly identifying ฃ the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.\",\n",
       "       \"This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakers’ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /ɛ:/, /ɯ:/, /ɤ:/, /o:/, and /ɔ:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the model’s estimation, using a vowel identification model. While this model was perfectly identifying ฃ the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.\",\n",
       "       'In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this research , we propose Deep Word -Topic Latent Dirichlet  Allocation  (DWT -LDA), a ne w process for training LDA with word embedding. DWT -LDA  augment s the samp ling process of an original LDA by incorporating  word embedding  technique  to allow the model to capture topics based embedding.  A neural network is applied  to the Collapsed Gibbs Sampling process as another choice for  word topic assignment.  A dataset crawled from Panti p.com  and Amazon Customer Review . To quantitatively evaluate our model, the  topic coherence framework , topic diversity , and topic quality  were  used to compare between  our approach and LDA.  The experimental result on both Thai and English dataset indicate  that DWT - LDA  performs better than LDA on both datasets.',\n",
       "       'In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this research , we propose Deep Word -Topic Latent Dirichlet  Allocation  (DWT -LDA), a ne w process for training LDA with word embedding. DWT -LDA  augment s the samp ling process of an original LDA by incorporating  word embedding  technique  to allow the model to capture topics based embedding.  A neural network is applied  to the Collapsed Gibbs Sampling process as another choice for  word topic assignment.  A dataset crawled from Panti p.com  and Amazon Customer Review . To quantitatively evaluate our model, the  topic coherence framework , topic diversity , and topic quality  were  used to compare between  our approach and LDA.  The experimental result on both Thai and English dataset indicate  that DWT - LDA  performs better than LDA on both datasets.',\n",
       "       'In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this research , we propose Deep Word -Topic Latent Dirichlet  Allocation  (DWT -LDA), a ne w process for training LDA with word embedding. DWT -LDA  augment s the samp ling process of an original LDA by incorporating  word embedding  technique  to allow the model to capture topics based embedding.  A neural network is applied  to the Collapsed Gibbs Sampling process as another choice for  word topic assignment.  A dataset crawled from Panti p.com  and Amazon Customer Review . To quantitatively evaluate our model, the  topic coherence framework , topic diversity , and topic quality  were  used to compare between  our approach and LDA.  The experimental result on both Thai and English dataset indicate  that DWT - LDA  performs better than LDA on both datasets.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this research , we propose Deep Word -Topic Latent Dirichlet  Allocation  (DWT -LDA), a ne w process for training LDA with word embedding. DWT -LDA  augment s the samp ling process of an original LDA by incorporating  word embedding  technique  to allow the model to capture topics based embedding.  A neural network is applied  to the Collapsed Gibbs Sampling process as another choice for  word topic assignment.  A dataset crawled from Panti p.com  and Amazon Customer Review . To quantitatively evaluate our model, the  topic coherence framework , topic diversity , and topic quality  were  used to compare between  our approach and LDA.  The experimental result on both Thai and English dataset indicate  that DWT - LDA  performs better than LDA on both datasets.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       'In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this research , we propose Deep Word -Topic Latent Dirichlet  Allocation  (DWT -LDA), a ne w process for training LDA with word embedding. DWT -LDA  augment s the samp ling process of an original LDA by incorporating  word embedding  technique  to allow the model to capture topics based embedding.  A neural network is applied  to the Collapsed Gibbs Sampling process as another choice for  word topic assignment.  A dataset crawled from Panti p.com  and Amazon Customer Review . To quantitatively evaluate our model, the  topic coherence framework , topic diversity , and topic quality  were  used to compare between  our approach and LDA.  The experimental result on both Thai and English dataset indicate  that DWT - LDA  performs better than LDA on both datasets.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       \"This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakers’ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /ɛ:/, /ɯ:/, /ɤ:/, /o:/, and /ɔ:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the model’s estimation, using a vowel identification model. While this model was perfectly identifying ฃ the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.\",\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The number of documents in a dataset available on the Internet is increasing. However, the limitation of using textual information derived from documents in data analysis requires more computation time and resources as the data grow. An analysis of the documents in a dataset can be conducted using other types of features. This thesis presented a computational study on documents with references. The analysis chose Thai legal documents as a dataset. The data were collected from an information service system of the Supreme Court of Thailand’s website. The study utilized text mining and network analysis to gain insight from the corpus. The analysis method included connected component analysis, graph clustering, and induced graphs. The study also built an informatio n retrieval system from the network features obtained from the network analysis. The evaluation revealed an increase in the performance after incorporating the network features with textual features. This indicated the potential benefit of using network fe atures in a real -world information retrieval system.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'Resource estimation is a technique that has many uses. In this research, we examine how it can be used to estimate computing resources of systems based on historical data in order to make them more efficient. Many researchers have applied machine learning to estimate computing resources. The European Organization for Nuclear Research (CERN) is currently developing a new logging system for A Large Ion Collider Experiment (ALICE) detector based on the Elasticsearch, Logstash, and Kibana (ELK) software stack. Beats, a shipper installed on the First Level Processor nodes, will receive the logged data and transfer it to Logstash, a data preprocessing pipeline. Logstash ingests the data and sends the ingested data to Elasticsearch which is used for search and analytics. The system faces difficult problems when working with large clusters which are hard to estimate. Moreover, in the future, the number of nodes and the number of services in the machine can increase or decrease. Resource estimation was used to estimate and plan the number of resources by using Metricbeat to get the historical computing metrics of machines and by using Logstash to make the system more reliable and adaptable to changes. We applied different machine learning algorithms including Random Forest Regression, Multiple Linear Regression, and Multi-Layer Perceptron to create models. The efficiency of these models is measured and compared using the coefficients of determination which are Mean Absolute Error and Mean Square Error.',\n",
       "       'The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.',\n",
       "       'Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Machine learning and predictive modeling have become widely us ed in many fields. Utilizing algorithm without tuning  hyper -parameter can lead to inefficient performance of model . Gradient Boosting Machine (GBM) is one of the tree -based models in which the performance can differ greatly depending on its setting. Tuning hyper -parameters of the model requires background knowledge  of the algorithm. Moreover, both the performance and cost of the tuning process need to be kept in consideration. In this paper, we proposed an approach based on Genetic algorithm (GA) in process of GBM tuning. The GA is often used in the optimization pro blem because of the ability to handle more complex problems. We implemented the GA variation called hyper -genetic to tune the hyper -parameter of GBM and explored the best setting possible of the genetic parameters. In addition, we compared our best setting  with the results from Grid -search, Bayesian optimization, and random approach over four sets of data. Our proposed algorithm had competitive performance and outperformed the other algorithms in the dataset with a high dimension while requiring smaller com putation time at the optimum point on the majority of experimental datasets.',\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'This thesis  presents a computational study on the relation between the movement of visual facial and acoustic features. Audio -visual corpus on expressive speech production was collected for the study. For the corpus, the relevant stimuli consist ed of 4 fac ial expression s and 4 Thai sentences, thus in total , there are  16 combinations. Video and audio data were captured from 10 native Thai speakers. Each speaker pronounce d the sentence of all combinations in the stimuli . In total, there were  160 audi o-visual track s for the analysis. Facial features were extracted and tracked by using visual markers through the pronunciation. At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. Computati onal analysis on the landmark and dynamic features of both visual and audio data were performed. This result provides the relation  of the expressive facial movement together with the acoustic adjustments.',\n",
       "       'In the logistic management, the cost reduction for delivering the  goods to customer s is crucial to save the budget of the company . Since decades ago, various Vehicle Routing Problems (VRPs ) have been emerged enormously to improve the productivity and to reduce the logistic cost of the industry . Among them, Vehicle Routing Problem with Time Windows  (VRPTW ) is one of the most fundamental VRP variants and one of the most applicable variants in the real-world case studies. In this study , VRPTW with  hard time windows is solved  by developing a special Genetic Algorithm (GA), composed of a problem -specific crossover operator and seven different mutation operators. The proposed GA has better results with  the heuristic mutation among seven operators while exploring the new and better features in large search space. The results of the algorithm are tested on the popular Solomon benchmark 100 - customer datasets. The results show that the proposed GA is quite comparable  with the best - known solutions on the C set of Solomon benchmark and even better than the best -known solutions on the R and RC sets. The motivation behind this research is to find a new problem - specific configurations of GA for VRP TW domain which can have the desirable outputs that can be applied in most of the real -world use cases.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       'The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       'Since a classical computer has a limitation to solve a large size of data and complex problems, researchers have be en trying to find new solutions. Q uantum computing is one of them. A number of quantum algorithms have been invented. Some of them were theoretically proved that they could solve problems faster than their classical versions. According to the literature review, a quantum random walk has various advantages. Moreover, it can be a tool to construct other quantum algorithms. In this thesis, the one - dimensional quantum random walk is studied. Then, the quantum random walk circuit is implemented. Forest from Rigetti and Qiskit from IBM, which are quantum programming platforms, are explored. Two experiments are conducted. First, the circuits are examined on quantum computer simulators from IBM and Rigetti. The sizes of the circuits are 4, 5, 6, and 7- qubits position state s. For each size, the circuit is constructed with 5 different types of shift operators. The results show that computing wall -time from IBM’s qua ntum computer simulator is significantly lower than Rigetti’s quantum computer simulator for the same size and circuit type. Moreover, on the same circuit type, a positive linear relationship between the computing wall -time and the circuit depth is presented. Second, the same circuit sizes and types from the first experiment were deployed on an actual IBM quantum computer, named ibmq_16_melbourne . However, the returned result is only for the 4- qubit position state. The other sizes of position state s return error due to the capacity of the quantum computer. Furthermore, the returned measurement results are disturbed by the noise from the operation of the actual quantum computer.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'System logs contain the complete information of service operations which is usually written by developers. In recent years, anomaly detection on log messages is popular. Several researchers applied the machine learning techniques to the system logs to detect the anomalous behaviors of the system. At the same time, CERN organization planned to establish a new data center in 2020 to improve the performance of the system. According to the new datacenter, ALICE intends to build a monitoring system for the new system. However, ALICE has been deploying the conventional monitoring system but requires the intervention of administrators. In this work, a novel anomaly detection framework is proposed by using Convolutional Neural Network (CNN) along with Transformer which is popular in machine translation tasks. The proposed framework automatically monitors the service operation logs and detects the anomalous events based on the log messages. The dataset used in the work is HDFS operation logs. To evaluate the model, the measurement metrics include Precision, Recall, F-measure, Miss detection rate, and False alarm rate. Finally, we design the experiments to compare the performance against the existing works which use LSTM (Long-Short Term Memory)-based and CNN-based approaches.',\n",
       "       \"This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakers’ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /ɛ:/, /ɯ:/, /ɤ:/, /o:/, and /ɔ:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the model’s estimation, using a vowel identification model. While this model was perfectly identifying ฃ the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.\",\n",
       "       'Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.',\n",
       "       'Loss estimation is considerably significant for network planning processes and plays a main role in bandwid th allocation optimization, network design, guaranteeing quality of service (QoS), etc. According to The European Organization for Nuclear Research (CERN), the ALICE O2 computing system has nodes, called First Level Processors (FLPs), which collect particl e interaction data from ALICE detectors and carry out local processing. Log data generated by tasks running on FLPs are sent over a network to the Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could lead to packet loss. In this research, we create FLPs in a testbed environment to characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series models and proba bility distributions assuming independent interarrival times. The fitted models are then used to study end -to-end packet loss with input traffic from a large number of FLPs in a network of switches. The simulation results can help predict the number of FLP s and traffic intensity that the network can sustain for different kinds of tasks running on FLPs. Lastly, in order to find the best represented model compared to the real trace result, we performed model verification and took into account the end-to- end packet loss and queue utilization of each task.',\n",
       "       'In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this research , we propose Deep Word -Topic Latent Dirichlet  Allocation  (DWT -LDA), a ne w process for training LDA with word embedding. DWT -LDA  augment s the samp ling process of an original LDA by incorporating  word embedding  technique  to allow the model to capture topics based embedding.  A neural network is applied  to the Collapsed Gibbs Sampling process as another choice for  word topic assignment.  A dataset crawled from Panti p.com  and Amazon Customer Review . To quantitatively evaluate our model, the  topic coherence framework , topic diversity , and topic quality  were  used to compare between  our approach and LDA.  The experimental result on both Thai and English dataset indicate  that DWT - LDA  performs better than LDA on both datasets.',\n",
       "       'The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.',\n",
       "       'The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.',\n",
       "       'Word segmentation is an important pre-processing step in natural language processing applications, particularly in languages with no demarcation indicators including such as Thai. Dictionary-based segmentation, while simple, does not consider the context of the sentence. This paper proposes an attention-based deep learning approach for Thai word segmentation. With an additional attention mechanism, the model can learn character correlations across the entire sentence without gradient vanishing or gradient explode problems and then tokenize them into word vectors. The goal of this research is to test two different types of attention mechanisms to determine the effectiveness of word tokenization. The visualization of attention for each attention mechanism is also included as an outcome.',\n",
       "       \"Understanding and interpretation of legislative corpus has problems related to unclear entity actions and coreference. Coreference is a linguistic expression in the corpus that refers to the same object, and entity's activities represent semantic meaning on what has been done to whom. This research addresses coreference resolution and meaning representation in legislative corpora using an integrated algorithm including Span Bidirectional Encoder Representation Transformer (SpanBERT) for coreference resolution and Abstract Meaning Representation (AMR) for meaning representation. Five-step frameworks are conducted: legal text preprocessing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores were applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR performance was evaluated by Smatch score with 12 experiments conducted on AMR nodes. Convention on the Right of the Child (CRC), Convention on the Rights of the Person with Disabilities (CRPD), and Convention and Protocol Relating to the Status of the Refugees (CPRSR) were the datasets. This experiment shows the SpanBERT algorithm's generalization for coreference resolution in the legal area with 72.08%, 78.03%, and 69.19% in CRC, CRPD, and CPRSR respectively. The ambiguity reduction shows the AMR semantic conversion, meaning preservation, and node fluctuation. The AMR nodes are fluctuated according to the complexity of coreference. When applying SpanBERT with AMR parsing, the legal text is simplified, and AMR is a potential tool to perform legal text meaning representation. The AMR graphs after complexity being reduced can be applied for further legal text processing tasks with Neural Network such as legal inferencing.\",\n",
       "       \"This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakers’ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /ɛ:/, /ɯ:/, /ɤ:/, /o:/, and /ɔ:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the model’s estimation, using a vowel identification model. While this model was perfectly identifying ฃ the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.\",\n",
       "       'Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.',\n",
       "       'Handling an imbalanced class problem is a challenging task in real-world applications. This problem affects various prediction models that predict only the majority classes and fail to identify the minority classes because of the skewed data. The oversampling technique is one of the exciting solutions that handles the imbalanced class problem. However, several existing oversampling methods do not consider the distribution of the target variable and cause an overlapping class problem. Therefore, this study introduces a new oversampling technique, namely Synthetic Minority Based on Probabilistic Distribution (SyMProD), to handle skewed datasets. Our method normalizes data using a Z-score and removes noisy data. Then, the proposed method selects minority samples based on the probability distribution of both classes. The synthetic instances are generated from the selected points and several minority nearest neighbors. Our technique aims to create synthetic instances that cover the minority class distribution, avoid the noise generation, and reduce the possibilities of overlapping classes and overgeneralization problems. Our proposed technique is validated using 17 benchmark datasets and three classifiers. Moreover, this study compares the performance of the method with other eight conventional oversampling algorithms. The empirical results show that our method achieves better performance than other oversampling techniques.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text.Tokenizer()\n",
    "t.fit_on_texts(x_train)\n",
    "x_indx = t.texts_to_sequences(x_train)\n",
    "x_indx_test = t.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_indx)\n",
    "len(x_indx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(x_indx, key=len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(x_indx):\n",
    "    if len(x) < maxlen:\n",
    "        x_indx[i] = np.hstack((x, np.zeros(maxlen-len(x))))\n",
    "x_train_index = np.array(x_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 278)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,  62.,   3.,  97.,   5.,   6.,  41., 544.,   8.,   1., 545.,\n",
       "         7., 546.,  91.,   1., 371.,   3.,  18., 291.,  68., 547.,  13.,\n",
       "        97.,   5.,  12.,  44., 222., 150., 548.,  34.,   2., 195.,  31.,\n",
       "         1.,  12., 549.,  14.,  44.,   3.,   1.,  97.,   5.,   6.,  41.,\n",
       "        28.,  50.,  95.,  18.,  74., 186.,   3.,  48.,  11., 111., 196.,\n",
       "         6., 292.,  30.,   8.,  97.,   9., 550.,   1.,  44., 551.,  45.,\n",
       "        39.,  97.,  31.,   6.,  41.,   1.,  12.,  19., 293.,  13.,  14.,\n",
       "        68., 160.,  24.,   3.,   1., 552., 553.,   3., 554., 555.,   1.,\n",
       "        30., 372.,  26., 556.,   2.,  17.,  44.,   4., 194., 557.,  13.,\n",
       "         1.,  82.,   1.,  44.,  42., 332., 373., 558.,  44., 559., 560.,\n",
       "         2., 561., 246.,   1.,  30., 171., 562.,  14., 563., 564., 294.,\n",
       "        24.,  13.,   1.,  17.,  48., 565.,  13.,   1.,  17.,  44.,   1.,\n",
       "        78., 223.,  14., 374.,   5.,   1.,  23., 247., 363.,   1.,  17.,\n",
       "        48.,   9., 291.,  48.,  11., 566.,   1., 161., 567.,   3.,  18.,\n",
       "        17., 568., 569.,   5.,   6., 108., 135.,  68., 294.,  24.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_index[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word = len(t.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'in': 5,\n",
       " 'a': 6,\n",
       " 'is': 7,\n",
       " 'on': 8,\n",
       " 'with': 9,\n",
       " 'for': 10,\n",
       " 'this': 11,\n",
       " 'data': 12,\n",
       " 'from': 13,\n",
       " 'an': 14,\n",
       " 'model': 15,\n",
       " 'that': 16,\n",
       " 'network': 17,\n",
       " 'using': 18,\n",
       " 'were': 19,\n",
       " 'are': 20,\n",
       " 'based': 21,\n",
       " 'amr': 22,\n",
       " 'performance': 23,\n",
       " 'system': 24,\n",
       " 'learning': 25,\n",
       " 'text': 26,\n",
       " 'was': 27,\n",
       " 'can': 28,\n",
       " 'by': 29,\n",
       " 'study': 30,\n",
       " 'as': 31,\n",
       " 'coreference': 32,\n",
       " 'meaning': 33,\n",
       " 'time': 34,\n",
       " 'applied': 35,\n",
       " 'proposed': 36,\n",
       " 'has': 37,\n",
       " 'results': 38,\n",
       " 'legal': 39,\n",
       " 'word': 40,\n",
       " 'dataset': 41,\n",
       " 'method': 42,\n",
       " 'models': 43,\n",
       " 'analysis': 44,\n",
       " 'thai': 45,\n",
       " 'research': 46,\n",
       " 'which': 47,\n",
       " 'features': 48,\n",
       " 'it': 49,\n",
       " 'be': 50,\n",
       " 'algorithm': 51,\n",
       " 'been': 52,\n",
       " 'processing': 53,\n",
       " 'different': 54,\n",
       " 'representation': 55,\n",
       " 'machine': 56,\n",
       " 'problem': 57,\n",
       " 'we': 58,\n",
       " 'our': 59,\n",
       " 'used': 60,\n",
       " 'new': 61,\n",
       " 'number': 62,\n",
       " 'problems': 63,\n",
       " 'process': 64,\n",
       " 'quantum': 65,\n",
       " 'many': 66,\n",
       " 'best': 67,\n",
       " 'information': 68,\n",
       " 'tuning': 69,\n",
       " 'lstm': 70,\n",
       " 'ga': 71,\n",
       " 'technique': 72,\n",
       " 'than': 73,\n",
       " 'other': 74,\n",
       " 'have': 75,\n",
       " 'better': 76,\n",
       " 'neural': 77,\n",
       " 'evaluation': 78,\n",
       " 'one': 79,\n",
       " 'datasets': 80,\n",
       " 'nodes': 81,\n",
       " 'corpus': 82,\n",
       " 'speech': 83,\n",
       " 'random': 84,\n",
       " 's': 85,\n",
       " 'flps': 86,\n",
       " 'resolution': 87,\n",
       " 'search': 88,\n",
       " 'techniques': 89,\n",
       " 'each': 90,\n",
       " 'however': 91,\n",
       " 'language': 92,\n",
       " 'tasks': 93,\n",
       " 'result': 94,\n",
       " 'conducted': 95,\n",
       " 'both': 96,\n",
       " 'documents': 97,\n",
       " 'same': 98,\n",
       " 'visual': 99,\n",
       " 'audio': 100,\n",
       " 'minority': 101,\n",
       " 'oversampling': 102,\n",
       " 'including': 103,\n",
       " 'two': 104,\n",
       " 'between': 105,\n",
       " 'optimization': 106,\n",
       " 'log': 107,\n",
       " 'real': 108,\n",
       " 'synthetic': 109,\n",
       " 'end': 110,\n",
       " 'thesis': 111,\n",
       " 'methods': 112,\n",
       " 'moreover': 113,\n",
       " 'circuit': 114,\n",
       " 'or': 115,\n",
       " 'computing': 116,\n",
       " 'large': 117,\n",
       " 'when': 118,\n",
       " 'most': 119,\n",
       " 'algorithms': 120,\n",
       " 'deep': 121,\n",
       " 'approach': 122,\n",
       " 'lda': 123,\n",
       " 'compared': 124,\n",
       " 'spanbert': 125,\n",
       " 'complexity': 126,\n",
       " 'convention': 127,\n",
       " 'there': 128,\n",
       " 'accuracy': 129,\n",
       " 'work': 130,\n",
       " 'computer': 131,\n",
       " 'such': 132,\n",
       " 'attention': 133,\n",
       " 'then': 134,\n",
       " 'world': 135,\n",
       " 'score': 136,\n",
       " 'facial': 137,\n",
       " 'class': 138,\n",
       " 'classes': 139,\n",
       " 'distribution': 140,\n",
       " 'summarization': 141,\n",
       " 'alice': 142,\n",
       " 'reduction': 143,\n",
       " '19': 144,\n",
       " 'short': 145,\n",
       " 'build': 146,\n",
       " 'them': 147,\n",
       " 'could': 148,\n",
       " 'according': 149,\n",
       " 'more': 150,\n",
       " 'acoustic': 151,\n",
       " 'various': 152,\n",
       " 'solutions': 153,\n",
       " 'long': 154,\n",
       " 'parameters': 155,\n",
       " 'articulatory': 156,\n",
       " 'reinforcement': 157,\n",
       " 'risk': 158,\n",
       " 'evaluate': 159,\n",
       " 'service': 160,\n",
       " 'potential': 161,\n",
       " 'tool': 162,\n",
       " 'at': 163,\n",
       " 'sets': 164,\n",
       " 'term': 165,\n",
       " 'genetic': 166,\n",
       " 'its': 167,\n",
       " 'while': 168,\n",
       " 'not': 169,\n",
       " 'without': 170,\n",
       " 'also': 171,\n",
       " 'loss': 172,\n",
       " 'estimation': 173,\n",
       " 'logstash': 174,\n",
       " 'detection': 175,\n",
       " 'traffic': 176,\n",
       " 'related': 177,\n",
       " 'sentences': 178,\n",
       " 'experiment': 179,\n",
       " '4': 180,\n",
       " 'hyperparameter': 181,\n",
       " 'hyperparameters': 182,\n",
       " 'logs': 183,\n",
       " 'news': 184,\n",
       " 'hyper': 185,\n",
       " 'types': 186,\n",
       " 'embedding': 187,\n",
       " 'topic': 188,\n",
       " 'optimal': 189,\n",
       " 'benchmark': 190,\n",
       " 'hissing': 191,\n",
       " 'vowel': 192,\n",
       " 'natural': 193,\n",
       " 'gain': 194,\n",
       " 'resources': 195,\n",
       " 'presented': 196,\n",
       " 'legislative': 197,\n",
       " 'semantic': 198,\n",
       " 'preservation': 199,\n",
       " 'smatch': 200,\n",
       " 'experiments': 201,\n",
       " 'crc': 202,\n",
       " 'crpd': 203,\n",
       " 'cprsr': 204,\n",
       " 'shows': 205,\n",
       " 'known': 206,\n",
       " 'behavior': 207,\n",
       " 'classification': 208,\n",
       " 'several': 209,\n",
       " 'memory': 210,\n",
       " 'specific': 211,\n",
       " 'cost': 212,\n",
       " 'languages': 213,\n",
       " 'consider': 214,\n",
       " 'sentence': 215,\n",
       " 'studies': 216,\n",
       " 'customer': 217,\n",
       " 'first': 218,\n",
       " 'level': 219,\n",
       " 'e': 220,\n",
       " 'generated': 221,\n",
       " 'requires': 222,\n",
       " 'revealed': 223,\n",
       " 'bidirectional': 224,\n",
       " 'relation': 225,\n",
       " 'show': 226,\n",
       " 'seven': 227,\n",
       " 'popular': 228,\n",
       " 'researchers': 229,\n",
       " 'monitoring': 230,\n",
       " 'use': 231,\n",
       " 'make': 232,\n",
       " 'operators': 233,\n",
       " 'segmentation': 234,\n",
       " 'important': 235,\n",
       " 'particularly': 236,\n",
       " 'gradient': 237,\n",
       " 'over': 238,\n",
       " 'anomaly': 239,\n",
       " 'packet': 240,\n",
       " 'create': 241,\n",
       " 'predict': 242,\n",
       " 'order': 243,\n",
       " 'find': 244,\n",
       " 'task': 245,\n",
       " 'graphs': 246,\n",
       " 'after': 247,\n",
       " 'expression': 248,\n",
       " 'activities': 249,\n",
       " 'status': 250,\n",
       " 'parameter': 251,\n",
       " 'grid': 252,\n",
       " 'only': 253,\n",
       " 'abc': 254,\n",
       " 'because': 255,\n",
       " 'improve': 256,\n",
       " 'convolutional': 257,\n",
       " 'since': 258,\n",
       " 'covid': 259,\n",
       " 'abstractive': 260,\n",
       " 'estimate': 261,\n",
       " 'gbm': 262,\n",
       " 'setting': 263,\n",
       " 'explored': 264,\n",
       " 'recorded': 265,\n",
       " 'trading': 266,\n",
       " 'policy': 267,\n",
       " 'step': 268,\n",
       " 'into': 269,\n",
       " 'framework': 270,\n",
       " 'organization': 271,\n",
       " 'cern': 272,\n",
       " 'understanding': 273,\n",
       " 'interpretation': 274,\n",
       " 'transformer': 275,\n",
       " 'understudy': 276,\n",
       " 'speakers': 277,\n",
       " 'these': 278,\n",
       " 'ability': 279,\n",
       " 'target': 280,\n",
       " 'noise': 281,\n",
       " 'operation': 282,\n",
       " 'metrics': 283,\n",
       " 'augmentation': 284,\n",
       " 'forest': 285,\n",
       " 'layer': 286,\n",
       " 'character': 287,\n",
       " 'applying': 288,\n",
       " 'original': 289,\n",
       " 'elasticsearch': 290,\n",
       " 'textual': 291,\n",
       " 'computational': 292,\n",
       " 'collected': 293,\n",
       " 'retrieval': 294,\n",
       " 'unclear': 295,\n",
       " 'five': 296,\n",
       " 'preprocessing': 297,\n",
       " '12': 298,\n",
       " 'further': 299,\n",
       " 'movement': 300,\n",
       " 'expressive': 301,\n",
       " 'stimuli': 302,\n",
       " 'total': 303,\n",
       " 'combinations': 304,\n",
       " 'tracked': 305,\n",
       " 'fundamental': 306,\n",
       " 'function': 307,\n",
       " 'defined': 308,\n",
       " 'well': 309,\n",
       " 'imbalanced': 310,\n",
       " 'skewed': 311,\n",
       " 'overlapping': 312,\n",
       " 'instances': 313,\n",
       " 'selected': 314,\n",
       " 'reduce': 315,\n",
       " 'logistic': 316,\n",
       " 'vehicle': 317,\n",
       " 'routing': 318,\n",
       " 'among': 319,\n",
       " 'windows': 320,\n",
       " 'vrptw': 321,\n",
       " 'vrp': 322,\n",
       " 'variants': 323,\n",
       " 'mutation': 324,\n",
       " 'tested': 325,\n",
       " 'solomon': 326,\n",
       " 'domain': 327,\n",
       " 'bees': 328,\n",
       " 'mel': 329,\n",
       " 'minimum': 330,\n",
       " 'applications': 331,\n",
       " 'included': 332,\n",
       " 'dwt': 333,\n",
       " 'collect': 334,\n",
       " 'performed': 335,\n",
       " 'ed': 336,\n",
       " 'through': 337,\n",
       " 'set': 338,\n",
       " 'traditional': 339,\n",
       " 'majority': 340,\n",
       " 'existing': 341,\n",
       " 'therefore': 342,\n",
       " 'namely': 343,\n",
       " 'handle': 344,\n",
       " 'conventional': 345,\n",
       " 'due': 346,\n",
       " 'modelling': 347,\n",
       " 'case': 348,\n",
       " 'error': 349,\n",
       " 'tree': 350,\n",
       " 'size': 351,\n",
       " 'walk': 352,\n",
       " 'ibm': 353,\n",
       " 'sizes': 354,\n",
       " 'position': 355,\n",
       " 'state': 356,\n",
       " 'sharpe': 357,\n",
       " 'ratio': 358,\n",
       " 'pre': 359,\n",
       " 'indicators': 360,\n",
       " 'paper': 361,\n",
       " 'mechanism': 362,\n",
       " 'incorporating': 363,\n",
       " 'main': 364,\n",
       " 'design': 365,\n",
       " 'called': 366,\n",
       " 'running': 367,\n",
       " 'sent': 368,\n",
       " 'lead': 369,\n",
       " 'help': 370,\n",
       " 'limitation': 371,\n",
       " 'utilized': 372,\n",
       " 'connected': 373,\n",
       " 'increase': 374,\n",
       " 'production': 375,\n",
       " 'thus': 376,\n",
       " '16': 377,\n",
       " '10': 378,\n",
       " 'native': 379,\n",
       " 'speaker': 380,\n",
       " 'o': 381,\n",
       " 'sequentially': 382,\n",
       " 'values': 383,\n",
       " 'amount': 384,\n",
       " 'adjust': 385,\n",
       " 'bee': 386,\n",
       " 'automatically': 387,\n",
       " 'f1': 388,\n",
       " 'against': 389,\n",
       " 'challenging': 390,\n",
       " 'empirical': 391,\n",
       " 'baseline': 392,\n",
       " 'messages': 393,\n",
       " 'anomalous': 394,\n",
       " 'cnn': 395,\n",
       " 'recall': 396,\n",
       " 'rate': 397,\n",
       " 'approaches': 398,\n",
       " 'about': 399,\n",
       " 'automatic': 400,\n",
       " 'us': 401,\n",
       " 'extractive': 402,\n",
       " 'summaries': 403,\n",
       " 'developing': 404,\n",
       " 'installed': 405,\n",
       " 'hard': 406,\n",
       " 'configurations': 407,\n",
       " 'signals': 408,\n",
       " 'their': 409,\n",
       " 'dimensional': 410,\n",
       " 'trained': 411,\n",
       " 'layers': 412,\n",
       " 'effectiveness': 413,\n",
       " 'recent': 414,\n",
       " 'allocation': 415,\n",
       " 'training': 416,\n",
       " 'com': 417,\n",
       " 'quality': 418,\n",
       " 'compare': 419,\n",
       " 'experimental': 420,\n",
       " 'european': 421,\n",
       " 'nuclear': 422,\n",
       " 'kibana': 423,\n",
       " 'future': 424,\n",
       " 'entity': 425,\n",
       " 'actions': 426,\n",
       " 'linguistic': 427,\n",
       " 'refers': 428,\n",
       " 'object': 429,\n",
       " \"entity's\": 430,\n",
       " 'represent': 431,\n",
       " 'what': 432,\n",
       " 'done': 433,\n",
       " 'whom': 434,\n",
       " 'addresses': 435,\n",
       " 'corpora': 436,\n",
       " 'integrated': 437,\n",
       " 'span': 438,\n",
       " 'encoder': 439,\n",
       " 'abstract': 440,\n",
       " 'frameworks': 441,\n",
       " 'bilingual': 442,\n",
       " 'bleu': 443,\n",
       " 'scores': 444,\n",
       " 'overlapped': 445,\n",
       " 'resolved': 446,\n",
       " 'unresolved': 447,\n",
       " 'evaluated': 448,\n",
       " 'right': 449,\n",
       " 'child': 450,\n",
       " 'rights': 451,\n",
       " 'person': 452,\n",
       " 'disabilities': 453,\n",
       " 'protocol': 454,\n",
       " 'relating': 455,\n",
       " 'refugees': 456,\n",
       " \"algorithm's\": 457,\n",
       " 'generalization': 458,\n",
       " 'area': 459,\n",
       " '72': 460,\n",
       " '08': 461,\n",
       " '78': 462,\n",
       " '03': 463,\n",
       " '69': 464,\n",
       " 'respectively': 465,\n",
       " 'ambiguity': 466,\n",
       " 'conversion': 467,\n",
       " 'node': 468,\n",
       " 'fluctuation': 469,\n",
       " 'fluctuated': 470,\n",
       " 'parsing': 471,\n",
       " 'simplified': 472,\n",
       " 'perform': 473,\n",
       " 'being': 474,\n",
       " 'reduced': 475,\n",
       " 'inferencing': 476,\n",
       " 'difficult': 477,\n",
       " 'furthermore': 478,\n",
       " 'predefined': 479,\n",
       " 'adopted': 480,\n",
       " 'variation': 481,\n",
       " 'novel': 482,\n",
       " 'measurement': 483,\n",
       " 'works': 484,\n",
       " 'faster': 485,\n",
       " 'currently': 486,\n",
       " 'identifying': 487,\n",
       " 'working': 488,\n",
       " 'perfect': 489,\n",
       " 'analyzed': 490,\n",
       " 'widely': 491,\n",
       " 'complex': 492,\n",
       " 'implemented': 493,\n",
       " 'possible': 494,\n",
       " 'outperformed': 495,\n",
       " 'another': 496,\n",
       " 'review': 497,\n",
       " 'english': 498,\n",
       " 'neuron': 499,\n",
       " 'especially': 500,\n",
       " 'researches': 501,\n",
       " 'significantly': 502,\n",
       " 'designed': 503,\n",
       " 'showed': 504,\n",
       " 'dropout': 505,\n",
       " 'lower': 506,\n",
       " '21': 507,\n",
       " 'resource': 508,\n",
       " 'examine': 509,\n",
       " 'how': 510,\n",
       " 'systems': 511,\n",
       " 'historical': 512,\n",
       " 'regression': 513,\n",
       " 'linear': 514,\n",
       " 'mean': 515,\n",
       " 'classical': 516,\n",
       " 'solve': 517,\n",
       " 'rigetti': 518,\n",
       " 'circuits': 519,\n",
       " '5': 520,\n",
       " '6': 521,\n",
       " 'wall': 522,\n",
       " 'simulator': 523,\n",
       " 'type': 524,\n",
       " 'actual': 525,\n",
       " 'returned': 526,\n",
       " 'strategy': 527,\n",
       " 'targets': 528,\n",
       " 'g': 529,\n",
       " 'recurrent': 530,\n",
       " 'having': 531,\n",
       " 'utterances': 532,\n",
       " 'estimated': 533,\n",
       " 'inversion': 534,\n",
       " 'aai': 535,\n",
       " 'financial': 536,\n",
       " 'variance': 537,\n",
       " 'market': 538,\n",
       " 'averse': 539,\n",
       " 'average': 540,\n",
       " 'reward': 541,\n",
       " 'sensitive': 542,\n",
       " 'categorical': 543,\n",
       " 'available': 544,\n",
       " 'internet': 545,\n",
       " 'increasing': 546,\n",
       " 'derived': 547,\n",
       " 'computation': 548,\n",
       " 'grow': 549,\n",
       " 'references': 550,\n",
       " 'chose': 551,\n",
       " 'supreme': 552,\n",
       " 'court': 553,\n",
       " 'thailand’s': 554,\n",
       " 'website': 555,\n",
       " 'mining': 556,\n",
       " 'insight': 557,\n",
       " 'component': 558,\n",
       " 'graph': 559,\n",
       " 'clustering': 560,\n",
       " 'induced': 561,\n",
       " 'built': 562,\n",
       " 'informatio': 563,\n",
       " 'n': 564,\n",
       " 'obtained': 565,\n",
       " 'indicated': 566,\n",
       " 'benefit': 567,\n",
       " 'fe': 568,\n",
       " 'atures': 569,\n",
       " 'presents': 570,\n",
       " 'relevant': 571,\n",
       " 'consist': 572,\n",
       " 'fac': 573,\n",
       " 'ial': 574,\n",
       " 'video': 575,\n",
       " 'captured': 576,\n",
       " 'pronounce': 577,\n",
       " 'd': 578,\n",
       " 'all': 579,\n",
       " '160': 580,\n",
       " 'audi': 581,\n",
       " 'track': 582,\n",
       " 'extracted': 583,\n",
       " 'markers': 584,\n",
       " 'pronunciation': 585,\n",
       " 'frequency': 586,\n",
       " 'f0': 587,\n",
       " 'synchronized': 588,\n",
       " 'computati': 589,\n",
       " 'onal': 590,\n",
       " 'landmark': 591,\n",
       " 'dynamic': 592,\n",
       " 'provides': 593,\n",
       " 'together': 594,\n",
       " 'adjustments': 595,\n",
       " 'handling': 596,\n",
       " 'affects': 597,\n",
       " 'prediction': 598,\n",
       " 'fail': 599,\n",
       " 'identify': 600,\n",
       " 'exciting': 601,\n",
       " 'handles': 602,\n",
       " 'do': 603,\n",
       " 'variable': 604,\n",
       " 'cause': 605,\n",
       " 'introduces': 606,\n",
       " 'probabilistic': 607,\n",
       " 'symprod': 608,\n",
       " 'normalizes': 609,\n",
       " 'z': 610,\n",
       " 'removes': 611,\n",
       " 'noisy': 612,\n",
       " 'selects': 613,\n",
       " 'samples': 614,\n",
       " 'probability': 615,\n",
       " 'points': 616,\n",
       " 'nearest': 617,\n",
       " 'neighbors': 618,\n",
       " 'aims': 619,\n",
       " 'cover': 620,\n",
       " 'avoid': 621,\n",
       " 'generation': 622,\n",
       " 'possibilities': 623,\n",
       " 'overgeneralization': 624,\n",
       " 'validated': 625,\n",
       " '17': 626,\n",
       " 'three': 627,\n",
       " 'classifiers': 628,\n",
       " 'compares': 629,\n",
       " 'eight': 630,\n",
       " 'achieves': 631,\n",
       " 'management': 632,\n",
       " 'delivering': 633,\n",
       " 'goods': 634,\n",
       " 'crucial': 635,\n",
       " 'save': 636,\n",
       " 'budget': 637,\n",
       " 'company': 638,\n",
       " 'decades': 639,\n",
       " 'ago': 640,\n",
       " 'vrps': 641,\n",
       " 'emerged': 642,\n",
       " 'enormously': 643,\n",
       " 'productivity': 644,\n",
       " 'industry': 645,\n",
       " 'applicable': 646,\n",
       " 'solved': 647,\n",
       " 'special': 648,\n",
       " 'composed': 649,\n",
       " 'crossover': 650,\n",
       " 'operator': 651,\n",
       " 'heuristic': 652,\n",
       " 'exploring': 653,\n",
       " 'space': 654,\n",
       " '100': 655,\n",
       " 'quite': 656,\n",
       " 'comparable': 657,\n",
       " 'c': 658,\n",
       " 'even': 659,\n",
       " 'r': 660,\n",
       " 'rc': 661,\n",
       " 'motivation': 662,\n",
       " 'behind': 663,\n",
       " 'tw': 664,\n",
       " 'desirable': 665,\n",
       " 'outputs': 666,\n",
       " 'cases': 667,\n",
       " 'honeybee': 668,\n",
       " 'social': 669,\n",
       " 'insect': 670,\n",
       " 'communicates': 671,\n",
       " 'nestmates': 672,\n",
       " 'engaging': 673,\n",
       " 'honeybees': 674,\n",
       " 'normally': 675,\n",
       " 'emit': 676,\n",
       " 'signaling': 677,\n",
       " 'sounds': 678,\n",
       " 'communicate': 679,\n",
       " 'flight': 680,\n",
       " 'orientation': 681,\n",
       " 'dance': 682,\n",
       " 'recruited': 683,\n",
       " 'response': 684,\n",
       " 'disturbances': 685,\n",
       " 'beehives': 686,\n",
       " 'monitored': 687,\n",
       " 'numerous': 688,\n",
       " 'gather': 689,\n",
       " 'classify': 690,\n",
       " 'beehive': 691,\n",
       " 'demonstrates': 692,\n",
       " 'asian': 693,\n",
       " 'cavity': 694,\n",
       " 'nesting': 695,\n",
       " 'apis': 696,\n",
       " 'cerana': 697,\n",
       " 'under': 698,\n",
       " 'circumstances': 699,\n",
       " 'devices': 700,\n",
       " 'defensive': 701,\n",
       " 'feature': 702,\n",
       " 'extraction': 703,\n",
       " 'energy': 704,\n",
       " 'spectral': 705,\n",
       " 'transformation': 706,\n",
       " 'filter': 707,\n",
       " 'banks': 708,\n",
       " 'spectrograms': 709,\n",
       " 'support': 710,\n",
       " 'vector': 711,\n",
       " 'decision': 712,\n",
       " 'networks': 713,\n",
       " 'competing': 714,\n",
       " 'objectives': 715,\n",
       " '95': 716,\n",
       " 'temporal': 717,\n",
       " 'spectrogram': 718,\n",
       " 'consists': 719,\n",
       " '2': 720,\n",
       " 'hidden': 721,\n",
       " '32': 722,\n",
       " '3': 723,\n",
       " '737': 724,\n",
       " 'trainable': 725,\n",
       " 'provide': 726,\n",
       " 'no': 727,\n",
       " 'demarcation': 728,\n",
       " 'dictionary': 729,\n",
       " 'simple': 730,\n",
       " 'does': 731,\n",
       " 'context': 732,\n",
       " 'proposes': 733,\n",
       " 'additional': 734,\n",
       " 'learn': 735,\n",
       " 'correlations': 736,\n",
       " 'across': 737,\n",
       " 'entire': 738,\n",
       " 'vanishing': 739,\n",
       " 'explode': 740,\n",
       " 'tokenize': 741,\n",
       " 'vectors': 742,\n",
       " 'goal': 743,\n",
       " 'test': 744,\n",
       " 'mechanisms': 745,\n",
       " 'determine': 746,\n",
       " 'tokenization': 747,\n",
       " 'visualization': 748,\n",
       " 'outcome': 749,\n",
       " 'considerably': 750,\n",
       " 'significant': 751,\n",
       " 'planning': 752,\n",
       " 'processes': 753,\n",
       " 'plays': 754,\n",
       " 'role': 755,\n",
       " 'bandwid': 756,\n",
       " 'th': 757,\n",
       " 'guaranteeing': 758,\n",
       " 'qos': 759,\n",
       " 'etc': 760,\n",
       " 'o2': 761,\n",
       " 'processors': 762,\n",
       " 'particl': 763,\n",
       " 'interaction': 764,\n",
       " 'detectors': 765,\n",
       " 'carry': 766,\n",
       " 'out': 767,\n",
       " 'local': 768,\n",
       " 'filtered': 769,\n",
       " 'lar': 770,\n",
       " 'ge': 771,\n",
       " 'amounts': 772,\n",
       " 'testbed': 773,\n",
       " 'environment': 774,\n",
       " 'characterize': 775,\n",
       " 'fit': 776,\n",
       " 'series': 777,\n",
       " 'proba': 778,\n",
       " 'bility': 779,\n",
       " 'distributions': 780,\n",
       " 'assuming': 781,\n",
       " 'independent': 782,\n",
       " 'interarrival': 783,\n",
       " 'times': 784,\n",
       " 'fitted': 785,\n",
       " 'input': 786,\n",
       " 'switches': 787,\n",
       " 'simulation': 788,\n",
       " 'flp': 789,\n",
       " 'intensity': 790,\n",
       " 'sustain': 791,\n",
       " 'kinds': 792,\n",
       " 'lastly': 793,\n",
       " 'represented': 794,\n",
       " 'trace': 795,\n",
       " 'verification': 796,\n",
       " 'took': 797,\n",
       " 'account': 798,\n",
       " 'queue': 799,\n",
       " 'utilization': 800,\n",
       " 'chooses': 801,\n",
       " 'once': 802,\n",
       " 'would': 803,\n",
       " 'define': 804,\n",
       " 'select': 805,\n",
       " 'consuming': 806,\n",
       " 'automated': 807,\n",
       " 'suggested': 808,\n",
       " 'expedite': 809,\n",
       " 'computationally': 810,\n",
       " 'expensive': 811,\n",
       " 'hyperparamters': 812,\n",
       " 'analyze': 813,\n",
       " 'reactively': 814,\n",
       " 'toward': 815,\n",
       " 'solution': 816,\n",
       " 'artificial': 817,\n",
       " 'colony': 818,\n",
       " 'metaheuristic': 819,\n",
       " 'foraging': 820,\n",
       " 'execution': 821,\n",
       " 'plus': 822,\n",
       " 'chosen': 823,\n",
       " 'comparison': 824,\n",
       " 'those': 825,\n",
       " 'contain': 826,\n",
       " 'complete': 827,\n",
       " 'operations': 828,\n",
       " 'usually': 829,\n",
       " 'written': 830,\n",
       " 'developers': 831,\n",
       " 'years': 832,\n",
       " 'detect': 833,\n",
       " 'behaviors': 834,\n",
       " 'planned': 835,\n",
       " 'establish': 836,\n",
       " 'center': 837,\n",
       " '2020': 838,\n",
       " 'datacenter': 839,\n",
       " 'intends': 840,\n",
       " 'deploying': 841,\n",
       " 'but': 842,\n",
       " 'intervention': 843,\n",
       " 'administrators': 844,\n",
       " 'along': 845,\n",
       " 'translation': 846,\n",
       " 'monitors': 847,\n",
       " 'detects': 848,\n",
       " 'events': 849,\n",
       " 'hdfs': 850,\n",
       " 'include': 851,\n",
       " 'precision': 852,\n",
       " 'f': 853,\n",
       " 'measure': 854,\n",
       " 'miss': 855,\n",
       " 'false': 856,\n",
       " 'alarm': 857,\n",
       " 'finally': 858,\n",
       " 'spread': 859,\n",
       " 'coronavirus': 860,\n",
       " 'disease': 861,\n",
       " '2019': 862,\n",
       " 'extensive': 863,\n",
       " 'takes': 864,\n",
       " 'humans': 865,\n",
       " 'read': 866,\n",
       " 'retrieve': 867,\n",
       " 'necessary': 868,\n",
       " 'matter': 869,\n",
       " 'decisions': 870,\n",
       " 'parts': 871,\n",
       " 'extracting': 872,\n",
       " 'subset': 873,\n",
       " 'closer': 874,\n",
       " 'human': 875,\n",
       " 'reproduction': 876,\n",
       " 'rephrasing': 877,\n",
       " 'canadian': 878,\n",
       " 'broadcasting': 879,\n",
       " 'corporation': 880,\n",
       " 'cbc': 881,\n",
       " 'exploited': 882,\n",
       " 'part': 883,\n",
       " 'example': 884,\n",
       " 'diverse': 885,\n",
       " 'enough': 886,\n",
       " 'generate': 887,\n",
       " 'resulting': 888,\n",
       " 'oriented': 889,\n",
       " 'gisting': 890,\n",
       " 'rouge': 891,\n",
       " 'predictive': 892,\n",
       " 'modeling': 893,\n",
       " 'become': 894,\n",
       " 'fields': 895,\n",
       " 'utilizing': 896,\n",
       " 'inefficient': 897,\n",
       " 'boosting': 898,\n",
       " 'differ': 899,\n",
       " 'greatly': 900,\n",
       " 'depending': 901,\n",
       " 'background': 902,\n",
       " 'knowledge': 903,\n",
       " 'need': 904,\n",
       " 'kept': 905,\n",
       " 'consideration': 906,\n",
       " 'often': 907,\n",
       " 'pro': 908,\n",
       " 'blem': 909,\n",
       " 'tune': 910,\n",
       " 'addition': 911,\n",
       " 'bayesian': 912,\n",
       " 'four': 913,\n",
       " 'had': 914,\n",
       " 'competitive': 915,\n",
       " 'high': 916,\n",
       " 'dimension': 917,\n",
       " 'requiring': 918,\n",
       " 'smaller': 919,\n",
       " 'putation': 920,\n",
       " 'optimum': 921,\n",
       " 'point': 922,\n",
       " 'nlp': 923,\n",
       " 'words': 924,\n",
       " 'propose': 925,\n",
       " 'latent': 926,\n",
       " 'dirichlet': 927,\n",
       " 'ne': 928,\n",
       " 'w': 929,\n",
       " 'augment': 930,\n",
       " 'samp': 931,\n",
       " 'ling': 932,\n",
       " 'allow': 933,\n",
       " 'capture': 934,\n",
       " 'topics': 935,\n",
       " 'collapsed': 936,\n",
       " 'gibbs': 937,\n",
       " 'sampling': 938,\n",
       " 'choice': 939,\n",
       " 'assignment': 940,\n",
       " 'crawled': 941,\n",
       " 'panti': 942,\n",
       " 'p': 943,\n",
       " 'amazon': 944,\n",
       " 'quantitatively': 945,\n",
       " 'coherence': 946,\n",
       " 'diversity': 947,\n",
       " 'indicate': 948,\n",
       " 'performs': 949,\n",
       " 'emerging': 950,\n",
       " 'proven': 951,\n",
       " 'essential': 952,\n",
       " 'improvement': 953,\n",
       " 'rarely': 954,\n",
       " 'unfortunately': 955,\n",
       " 'characteristic': 956,\n",
       " 'latin': 957,\n",
       " 'next': 958,\n",
       " 'sequence': 959,\n",
       " 'nlm': 960,\n",
       " 'value': 961,\n",
       " '0': 962,\n",
       " '75': 963,\n",
       " 'batch': 964,\n",
       " 'normalization': 965,\n",
       " 'perplexity': 966,\n",
       " 'up': 967,\n",
       " 'uses': 968,\n",
       " 'efficient': 969,\n",
       " 'logging': 970,\n",
       " 'ion': 971,\n",
       " 'collider': 972,\n",
       " 'detector': 973,\n",
       " 'elk': 974,\n",
       " 'software': 975,\n",
       " 'stack': 976,\n",
       " 'beats': 977,\n",
       " 'shipper': 978,\n",
       " 'processor': 979,\n",
       " 'will': 980,\n",
       " 'receive': 981,\n",
       " 'logged': 982,\n",
       " 'transfer': 983,\n",
       " 'pipeline': 984,\n",
       " 'ingests': 985,\n",
       " 'sends': 986,\n",
       " 'ingested': 987,\n",
       " 'analytics': 988,\n",
       " 'faces': 989,\n",
       " 'clusters': 990,\n",
       " 'services': 991,\n",
       " 'decrease': 992,\n",
       " 'plan': 993,\n",
       " 'metricbeat': 994,\n",
       " 'get': 995,\n",
       " 'machines': 996,\n",
       " 'reliable': 997,\n",
       " 'adaptable': 998,\n",
       " 'changes': 999,\n",
       " 'multiple': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(total_word, 100, input_length=maxlen, name='Embedding'))\n",
    "model.add(keras.layers.LSTM(512, kernel_initializer='he_normal', dropout=0.3, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(256, kernel_initializer='he_normal', dropout=0.3))\n",
    "model.add(keras.layers.Dense(70, activation='softmax'))\n",
    "# model.add(keras.layers.Dense(70))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam' ,metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding (Embedding)       (None, 278, 100)          113700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 278, 512)          1255424   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               787456    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 70)                17990     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,174,570\n",
      "Trainable params: 2,174,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 28s 7s/step - loss: 4.3130 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 30s 9s/step - loss: 4.2094 - acc: 0.0541\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 36s 9s/step - loss: 4.1565 - acc: 0.0541\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 29s 9s/step - loss: 4.1110 - acc: 0.0811\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 29s 9s/step - loss: 4.1086 - acc: 0.0946\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 29s 9s/step - loss: 4.0407 - acc: 0.0811\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 32s 9s/step - loss: 4.0171 - acc: 0.0541\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 29s 8s/step - loss: 4.1478 - acc: 0.0676\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 28s 9s/step - loss: 3.9877 - acc: 0.0541\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 28s 8s/step - loss: 3.9358 - acc: 0.0541\n"
     ]
    }
   ],
   "source": [
    "his = model.fit(x_train_index, y_train, epochs=10, batch_size= 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
