Effects of Facial Movements to Expressive Speech Productions: A Computational Study
This thesis presents a computational study on the relation between the movement of visual facial and acoustic features. 
Audio-visual corpus on expressive speech production was collected for the study. 
For the corpus, the relevant stimuli consisted of 4 facial expression s and 4 Thai sentences, thus in total, there are 16 combinations. 
Video and audio data were captured from 10 native Thai speakers. 
Each speaker pronounced the sentence of all combinations in the stimuli. 
In total, there were 160 audio-visual tracks for the analysis. 
Facial features were extracted and tracked by using visual markers through the pronunciation. 
At the same time, acoustic data, particularly the fundamental frequency (F0) was tracked and synchronized with the facial data. 
Computational analysis on the landmark and dynamic features of both visual and audio data were performed. 
This result provides the relation of the expressive facial movement together with the acoustic adjustments.  