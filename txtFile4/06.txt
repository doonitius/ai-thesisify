  
  ACOUSTIC-TO-ARTICULATORY INVERSION USING  DEEP LEARNING APPROACH                   MR. THANAT LAPTHAWAN                         A THESIS SUBMITTED IN PARTIAL FULFILLMENT  OF THE REQUIREMENTS FOR  THE DEGREE OF MASTER OF ENGINEERING  (COMPUTER ENGINEERING)   FACULTY OF ENGINEERING  KING MONGKUTâ€™S UNIVERSITY OF TECHNOLOGY THONBURI  2019 
2  Acoustic-to-Articulatory Inversion Using Deep Learning Approach    Mr. Thanat Lapthawan B.Eng. (Computer Engineering)   A Thesis Submitted in Partial Fulfillment  of the Requirements for  the Degree of Master of Engineering (Computer Engineering)   Faculty of Engineering  King Mongkutâ€™s University of Technology Thonburi  2019  Thesis Committee    â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.................................  Chairman of Thesis Committee (Assoc. Prof. Boonserm Kaewkamnerdpong, Ph.D.)  â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.................................  Member and Thesis Advisor           (Asst. Prof. Santitham Prom-on, Ph.D.)    â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.................................  Member        (Asst. Prof. Phond Phunchongharn, Ph.D.)  â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.................................  Member                (Asst. Prof. Suthathip Maneewongvatana, Ph.D.)    â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.................................  Member                     (Assoc. Prof. Anan Banharnsakun, Ph.D.)     Copyright reserved
  Thesis Title Acoustic-to-Articulatory Inversion Using Deep Learning Approach  Thesis Credits     12 Candidate            Mr. Thanat Lapthawan Thesis Advisors  Asst. Prof. Dr. Santitham Prom-on Program           Master of Engineering   Field of Study     Computer Engineering Department         Computer Engineering Faculty                Engineering Academic Year   2019  Abstract  This thesis proposed a speech production acquisition model using a self-learning strategy. The model presented the underlying articulatory target of vowels from speech features, using a deep learning model. The model was trained with a synthetic dataset interpolating from the predefined speakers and vowel configurations provided in a VocalTractLab - a 3D articulatory synthesizer. Proposed data generating methods interpolated articulatory targets and simulated speakersâ€™ vocal tract models. This method was designed to maximize possible speech variations. Post-processing data augmentation, e.g., noise injection and pitch shifting, was applied to the synthetic dataset. The study compared different model architectures, e.g., fully connected model, convolutional model, and recurrent model. The bidirectional long short-term memory recurrent neural network outperformed other models, having five bidirectional layers with 128 units and 50% dropout in each layer. Recorded speech from 12 native Thai speakers was used to numerically evaluate the model's effectiveness and generalizability. Each recorded utterance contained Thai disyllabic vowel sequences, which was a combination of /a:/, /i:/, /u:/, /e:/, /É›:/, /É¯:/, /É¤:/, /o:/, and /É”:/. Thus, there were 81 utterances per speaker. The proposed model accurately estimated articulatory targets from the recorded Thai vowel utterances. The study further analyzed the modelâ€™s estimation, using a vowel identification model. While this model was perfectly identifying the vowel from a given speech, the estimation result from the acoustic-to-articulatory inversion (AAI) model was not perfect. The result showed that the inversion speech from the estimated target articulatory was accurate, having 82.6% matching between the original recorded speech and the synthetic speech from the AAI model.  Keywords: Acoustic-to-Articulatory Inversion/ Deep Learning/  Speech Production Acquisition Model     à¸«à¸±à¸§à¸‚à¹‰à¸­à¸§à¸´à¸—à¸¢à¸²à¸™à¸´à¸à¸™à¸˜à¹Œ à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¸ªà¸±à¸à¸à¸²à¸“à¹€à¸ªà¸µà¸¢à¸‡à¹€à¸›à¹‡à¸™à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸¥à¸±à¸à¸©à¸“à¸°à¸à¸²à¸£à¸­à¸­à¸à¹€à¸ªà¸µà¸¢à¸‡à¸”à¹‰à¸§à¸¢à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸Šà¸´à¸‡à¸¥à¸¶à¸à¸‚à¸­à¸‡à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ  à¸«à¸™à¹ˆà¸§à¸¢à¸à¸´à¸•  12 à¸œà¸¹à¹‰à¹€à¸‚à¸µà¸¢à¸™   à¸™à¸²à¸¢à¸˜à¸™à¸±à¸— à¸¥à¸±à¸à¸˜à¸§à¸£à¸£à¸“à¹Œ à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œà¸—à¸µIà¸›à¸£à¸¶à¸à¸©à¸²  à¸œà¸¨. à¸”à¸£.à¸ªà¸±à¸™à¸•à¸´à¸˜à¸£à¸£à¸¡ à¸à¸£à¸«à¸¡à¸­à¹ˆà¸­à¸™ à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£  à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¸¡à¸«à¸²à¸šà¸±à¸“à¸‘à¸´à¸• à¸ªà¸²à¸‚à¸²à¸§à¸´à¸Šà¸²  à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ à¸ à¸²à¸„à¸§à¸´à¸Šà¸²   à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ à¸„à¸“à¸°   à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œ à¸›à¸µ à¸ à¸² à¸£ à¸¨ à¸¶ à¸ à¸© à¸²  2562 à¸šà¸—à¸„à¸±à¸”à¸¢à¹ˆà¸­  à¸‡à¸²à¸™à¸§à¸´à¸ˆà¸±à¸¢à¸™à¸µMà¸™à¹à¸²à¹€à¸ªà¸™à¸­à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸à¸²à¸£à¹„à¸”à¹‰à¸¡à¸²à¸‹à¸¶Ià¸‡à¸à¸²à¸£à¸œà¸¥à¸´à¸•à¹€à¸ªà¸µà¸¢à¸‡à¸‚à¸­à¸‡à¸¡à¸™à¸¸à¸©à¸¢à¹Œà¸”à¹‰à¸§à¸¢à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸”à¹‰à¸§à¸¢à¸•à¸™à¹€à¸­à¸‡ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸«à¸¥à¸±à¸à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸Šà¸´à¸‡à¸¥à¸¶à¸à¸‚à¸­à¸‡à¹€à¸„à¸£à¸·Ià¸­à¸‡à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ à¸•à¸±à¸§à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸¥à¸±à¸à¸©à¸“à¸°à¸à¸·Mà¸™à¸à¸²à¸™à¸‚à¸­à¸‡à¸„à¸¥à¸·Ià¸™à¹€à¸ªà¸µà¸¢à¸‡à¹à¸¥à¸°à¸›à¸£à¸°à¸¡à¸§à¸“à¸œà¸¥à¹€à¸›à¹‡ à¸™à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡ à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µIà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸‚à¸¶Mà¸™à¸¡à¸² à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸ˆà¸²à¸à¹‚à¸›à¸£à¹à¸à¸£à¸¡ VocalTractLab à¸‹à¸¶I à¸‡à¹€à¸›à¹‡ à¸™à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸ªà¸µà¸¢à¸‡à¸ˆà¸²à¸à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸Šà¹ˆà¸­à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸¡à¸™à¸¸à¸©à¸¢à¹Œà¹à¸šà¸šà¸ªà¸²à¸¡à¸¡à¸´à¸•à¸´ à¸›à¸£à¸°à¸à¸­à¸šà¹„à¸›à¸”à¹‰à¸§à¸¢à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡à¸‚à¸­à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸ªà¸£à¸°à¸”à¹‰à¸§à¸¢à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸„à¸²à¸”à¸à¸²à¸£ à¸à¸²à¸£à¸ˆà¹à¸²à¸¥à¸­à¸‡à¸Šà¹ˆà¸­à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸‚à¸­à¸‡à¸¡à¸™à¸¸à¸©à¸¢à¹Œ à¹à¸¥à¸°à¸à¸²à¸£à¸à¹à¸²à¸«à¸™à¸”à¸„à¹ˆà¸²à¸à¸²à¸£à¸à¸¹à¸”à¸‚à¸­à¸‡à¹€à¸ªà¸µà¸¢à¸‡ à¹ƒà¸«à¹‰à¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢à¸¡à¸²à¸à¸—à¸µIà¸ªà¸¸à¸” à¸£à¸§à¸¡à¹„à¸›à¸–à¸¶à¸‡à¸à¸²à¸£à¹€à¸ªà¸£à¸´à¸¡à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¸£à¸šà¸à¸§à¸™ à¹à¸¥à¸°à¸à¸²à¸£à¹€à¸›à¸¥à¸µIà¸¢à¸™à¹à¸›à¸¥à¸‡à¸£à¸°à¸”à¸±à¸šà¹€à¸ªà¸µà¸¢à¸‡ à¸—à¸±Mà¸‡à¸™à¸µMà¸‡à¸²à¸™à¸§à¸´à¸ˆà¸±à¸¢à¹„à¸”à¹‰à¸—à¸”à¸ªà¸­à¸šà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸­à¸‡à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸Šà¸´à¸‡à¸¥à¸¶à¸à¸‚à¸­à¸‡à¹€à¸„à¸£à¸·Ià¸­à¸‡à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ à¹„à¸”à¹‰à¹à¸à¹ˆà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸à¸²à¸£à¹€à¸Šà¸·Ià¸­à¸¡à¸•à¹ˆà¸­à¹€à¸•à¹‡à¸¡à¸£à¸¹à¸›à¹à¸šà¸š à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸ªà¸±à¸‡à¸§à¸±à¸•à¸™à¸²à¸à¸²à¸£ à¹à¸¥à¸°à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹à¸šà¸šà¸¢à¹‰à¸­à¸™à¸à¸¥à¸±à¸š à¸à¸±à¸šà¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸µà¸¢à¸‡à¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸¥à¸°à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸µà¸¢à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸ˆà¸²à¸à¸œà¸¹à¹‰à¸à¸¹à¸”à¸ªà¸£à¸°à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸—à¸±Mà¸‡à¸«à¸¡à¸” 12 à¸„à¸™ à¹ƒà¸™à¹€à¸£à¸·Ià¸­à¸‡à¸‚à¸­à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸œà¸¥à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸£à¸œà¸¥à¸ªà¸¹à¹ˆà¸à¸¥à¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¸¢à¸‡à¸­à¸·Ià¸™ à¹† à¹‚à¸”à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸•à¹ˆà¸¥à¸°à¸Šà¸¸à¸”à¹€à¸›à¹‡à¸™à¸ªà¸£à¸°à¸ªà¸­à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸›à¸£à¸°à¸à¸­à¸šà¸ˆà¸²à¸à¸ªà¸£à¸°à¸—à¸±Mà¸‡à¸«à¸¡à¸”à¹€à¸à¹‰à¸²à¹€à¸ªà¸µà¸¢à¸‡ à¹„à¸”à¹‰à¹à¸à¹ˆ à¸­à¸² à¸­à¸µ à¸­à¸·à¸­ à¹€à¸­ à¹à¸­ à¹€à¸­à¸­ à¹‚à¸­ à¸­à¸­ à¸­à¸¹ à¸£à¸§à¸¡à¸—à¸±Mà¸‡à¸«à¸¡à¸” 81 à¹€à¸ªà¸µà¸¢à¸‡à¸•à¹ˆà¸­à¸œà¸¹à¹‰à¸à¸¹à¸”à¸«à¸™à¸¶Ià¸‡à¸„à¸™ à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸à¸šà¸§à¹ˆà¸²à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸—à¸µIà¹ƒà¸«à¹‰à¸œà¸¥à¸”à¸µà¸—à¸µIà¸ªà¸¸à¸” à¸„à¸·à¸­à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹à¸šà¸šà¸›à¸£à¸°à¸¡à¸§à¸“à¸œà¸¥à¸¢à¹‰à¸­à¸™à¸à¸¥à¸±à¸šà¸ªà¸­à¸‡à¸—à¸´à¸¨à¸—à¸²à¸‡à¸”à¹‰à¸§à¸¢à¸«à¸™à¹ˆà¸§à¸¢à¸„à¸§à¸²à¸¡à¸ˆà¹à¸²à¸£à¸°à¸¢à¸°à¸ªà¸±Mà¸™à¹à¸¥à¸°à¸¢à¸²à¸§ à¸‡à¸²à¸™à¸§à¸´à¸ˆà¸±à¸¢à¸™à¸µMà¹„à¸”à¹‰à¸à¸±à¸’à¸™à¸²à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¹€à¸à¸·Ià¸­à¸£à¸°à¸šà¸¸à¸•à¸±à¸§à¸ªà¸£à¸°à¸ˆà¸²à¸à¹€à¸ªà¸µà¸¢à¸‡ à¹€à¸à¸·Ià¸­à¸™à¹à¸²à¸¡à¸²à¸§à¸±à¸”à¸œà¸¥à¸ˆà¸²à¸à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸„à¸²à¸”à¸à¸²à¸£à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸ˆà¸²à¸à¹€à¸ªà¸µà¸¢à¸‡ à¹€à¸à¸·Ià¸­à¸—à¸”à¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¹à¸²à¹ƒà¸™à¸à¸²à¸£à¸„à¸²à¸”à¸à¸²à¸£à¹€à¸ªà¸µà¸¢à¸‡à¸ªà¸£à¸°à¹ƒà¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¸Šà¸µMà¸§à¹ˆà¸²à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸à¸²à¸£à¸„à¸²à¸”à¸à¸²à¸£à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œ à¸ªà¸²à¸¡à¸²à¸£à¸–à¸ˆà¹à¸²à¸¥à¸­à¸‡ à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸ˆà¸²à¸à¸Šà¸¸à¸”à¹€à¸ªà¸µà¸¢à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸ªà¸£à¸°à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸‹à¸¶Ià¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸ªà¸£à¸°à¸—à¸µIà¹€à¸à¸´à¸”à¸ˆà¸²à¸à¸à¸²à¸£à¸„à¸²à¸”à¸à¸²à¸£à¸ˆà¸²à¸à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸¡à¸µà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸–à¸¶à¸‡ 82.6 à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™ à¹ƒà¸™à¸à¸²à¸£à¸„à¸²à¸”à¸à¸²à¸£à¹€à¸ªà¸µà¸¢à¸‡à¸ªà¸£à¸°à¸—à¸µIà¸•à¸£à¸‡à¸à¸±à¸šà¹€à¸ªà¸µà¸¢à¸‡à¸ªà¸£à¸°à¸ˆà¸²à¸à¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸ iv   à¸„à¹à¸²à¸ªà¹à¸²à¸„à¸±à¸: à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸Šà¸´à¸‡à¸¥à¸¶à¸à¸‚à¸­à¸‡à¹€à¸„à¸£à¸·Ià¸­à¸‡à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ/ à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¸à¸²à¸£à¹„à¸”à¹‰à¸¡à¸²à¸‹à¸¶Ià¸‡à¸à¸²à¸£à¸œà¸¥à¸´à¸•à¹€à¸ªà¸µà¸¢à¸‡à¸‚à¸­à¸‡à¸¡à¸™à¸¸à¸©à¸¢à¹Œ  / à¹à¸šà¸šà¸ˆà¹à¸²à¸¥à¸­à¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸¥à¸±à¸à¸©à¸“à¸°à¸à¸·Mà¸™à¸à¸²à¸™à¸‚à¸­à¸‡à¸„à¸¥à¸·Ià¸™à¹€à¸ªà¸µà¸¢à¸‡à¹à¸¥à¸°à¸›à¸£à¸°à¸¡à¸§à¸“à¸œà¸¥à¹€à¸›à¹‡à¸™à¸ªà¸£à¸µà¸£à¸ªà¸±à¸—à¸¨à¸²à¸ªà¸•à¸£à¹Œ  v  ACKNOWLEDGEMENTS   I thank my advisor, Asst. Prof. Santitham Prom-on, a lecturer at the Computer Engineering Department, King Mongkut's University of Technology Thonburi, who gives me advice through this research. I thank all participants, my friends, colleagues, and my family, contributing to this research. I thank King Mongkutâ€™s University of Technology Thonburi for the Petchra Prajom Klao scholarship program for a masterâ€™s degree and support from professors and officers at the computer engineering department. I appreciated the support from my family throughout my studies.     CONTENTS   PAGE   ENGLISH ABSTRACT ii THAI ABSTRACT iii ACKNOWLEDGEMENTS v CONTENTS  vi LIST OF TABLES viii LIST OF FIGURES ix  CHAPTER 1. INTRODUCTION 1 1.1     Statement of Problem 1 1.2     Objectives 2 1.3     Scopes 2 1.4     Expected Benefits 2  2. LITERATURE REVIEW AND THEORY 3 2.1     Speech Synthesis 3 2.2     Physical Simulation of Speech Production Model 4 2.3     VocalTractLab 2.2 Application Program Interface 6 2.4     Speech Features 7 2.5     Deep Neural Network Architecture 8 2.6     Acoustic-to-Articulatory Inversion Model 16 2.7     Computational Model of Speech Acquisition 17  3. METHODOLOGY 21 3.1     Speech Production Acquisition Model 21 3.2     Data Generating Method 23 3.3     Recorded Data Collecting Method 27 3.4     Data Preprocessing 27 vii  CONTENTS (Cont'd)   PAGE  3.5     Data Augmentation 29 3.6     Recorded Data Preprocessing 29 3.7     Post-Processing Model 30 3.8     Model 30 3.9     Model Training and Experimental Setup 32 3.10   Design of Experiment 37  4. RESULTS AND DISCUSSION 40 4.1     Model Comparison 40 4.2     Model Performance on The Predefined Dataset 43 4.3     Model Performance on The Recorded Dataset 48 4.4     Evaluation using Vowel Identification Model 53 4.5     General Discussion 58  5. CONCLUSION 59  REFERENCES 61  CURRICULUM VITAE 70      LIST OF TABLES   TABLE              PAGE  2.1     The articulatory parameters of the vocal tract 7 4.1     The performance of models trained with a monosyllabic vowel utterance 40 4.2     The model performance on predefined monosyllabic vowel data  41 4.3     The formant MAPE of a model on predefined monosyllabic vowel dataset 41 4.4     The performance models trained with the disyllabic vowel utterance  42 4.5     The model performance on predefined disyllabic vowel data   42 4.6     The formant MAPE of a model on  predefined disyllabic vowel data  42 4.7     The RMSE and R2 of the estimated articulatory parameter from  44                BiLSTM-RNN on a predefined monosyllabic vowel data  4.8     Relative formant error of predefined monosyllabic vowel utterance  44 4.9     Statistical comparison between the original synthetic and    45            the inversion speech formant of a monosyllabic vowel utterance  4.10   The RMSE and R2 of the estimated articulatory parameter from   46                BiLSTM-RNN on a predefined disyllabic vowel data  4.11   Relative formant error of predefined disyllabic vowel utterance  47 4.12   Statistical comparison between the original synthetic and    48            the inversion speech formant of a disyllabic vowel utterance  4.13   Mean absolute percentage formant error of the first and    49            second vowel   between actual speech and inversion speech  4.14   Statistical comparison between the actual and the inversion speech   49            formant of a disyllabic vowel utterance  4.15   The average precision and recall of the evaluator model   54 4.16   The identification rate of the inversion speech result from different models 55 4.17   The identification rate of the inversion speech result from different datasets 55 4.18   Compare model trained only on monosyllable data with a model with  56            disyllable data  4.19   Compare model prediction performance on female and male speaker  56           disyllabic vowel speech data  4.20   Compare model prediction performance on normal and    57            loud disyllabic vowel speech data       LIST OF FIGURES   FIGURE              PAGE   2.1     The rendering of the 3D vocal tract model  6 2.2     Birkholzâ€™s three-dimensional theoretical vocal tract  model parameters 6 2.3     An MFCC feature extraction diagram 8 2.4     Neural unit 9 2.5     MLP 10 2.6     LSTM block 15 2.7     Bidirectional recurrent layer 16 2.8     Long-term recurrent convolutional neural network architecture 17 2.9     The articulatory target estimation 19 3.1     Three stages of a speech production acquisition 21 3.2     The proposed speech production acquisition model 22 3.3     Experiment overview 23 3.4     Data generation process 24 3.5     The synthetic data pre-processing overview 28 3.6     The recorded data pre-processing overview 30 3.7     Simple multiple regression model 31 3.8     FCNN model 31 3.9     LSTM-RNN model 31 3.10   BiLSTM-RNN model 32 3.11   Conv-BiLSTM model 32 3.12   Vowel classification model 32 3.13   Distribution of the initial interpolated phonetic description 33 3.14   The interpolated target articulatory plot using UMAP 33 3.15   The midsagittal area of speaker vocal tract model 34 3.16   The sample disyllable speech signal and its spectrogram (left),  34           and a The MFCC (top right) and its normalization MFCC (bottom right)            of a generated disyllable speech  3.17   The distribution of each articulatory parameter 35 3.18   Recorded MFCCs feature plot using UMAP dimensional reduction 36 3.19   ROC-AUC 38 3.20   The AAI model evaluation schema. (top) is when the AAI model is  39           correctly inverted speech. (bottom) is when the AAI model            is incorrectly inverted speech  4.1     Formant plot comparing original synthetic and inversion speech signal  45           of a predefined monosyllabic vowel utterance   x  LIST OF FIGURES (Cont'd)   FIGURE              PAGE   4.2     Spectrogram comparing original synthetic (top) and inversion speech  45           signal (bottom) of a monosyllabic vowel utterance, /u:/ and /i:/.  4.3     Speaker articulation of /a:/ (left), /i:/ (center), and /u:/ (right) vowel 46 4.4     Formant plot comparing original synthetic and inversion speech signal  47           of a disyllabic vowel utterance  4.5     Spectrogram comparing original synthetic and resynthesized speech  48           signal of a disyllabic vowel utterance  4.6     Average speaker articulation of each vowel according to the IPA chart  49           of a disyllabic vowel utterance  4.7     Average formant plot comparing actual and inversion speech signal  50            of a disyllabic vowel utterance  4.8     F1 and F2 Comparison between inversion speech and the empirical  51           formant range  4.9     Spectrogram comparing actual and inversion disyllabic vowel speech  52 4.10   UMAP plot of the estimated articulation from actual speech 52 4.11   Average speaker articulation of each vowel in a first syllable (left)  53           and second syllable (right) of a disyllabic vowel utterance  4.12   Speaker articulation of /a:/, /i:/, and /u:/ 53 4.13   Confusion matrix of an identification result from the AAI model  57           of normal speech data and loud speech data  4.14   Confusion matrix of a vowel identification result by the evaluator model 58    CHAPTER 1 INTRODUCTION    A human speech is generated from an ensemble of articulators, e.g., tongue, teeth, lip, soft palate, and jaw, coordinating together to shape the vocal tract into a configuration, acting as filters modulating glottal signals and generating voice as we hear. Thus, the speech signal encodes articulatory information. Estimating articulatory configurations from an acoustic signal is known as the acoustic-to-articulatory inversion (AAI). This research proposes the AAI model as a part of the speech production acquisition model using a deep artificial neural network and a three-dimensional theoretical speaker vocal tract model to study speech production acquisition.   1.1   Statement of Problem The AAI is a complicated and ill-posed problem [1], which means a unique or correct solution does not exist. This is a result of nonlinearity characteristics and a one-to-many nature, where more than one articulatory position produces the same acoustic signal [2]. Factors affecting the AAI model present in both articulatory and acoustic signals. In the acoustic signal, the co-articulatory trajectory effect [3], a smooth transition between sounds, in multi-syllable utterance creates a speech variety for each articulatory target, i.e., many-to-one relationship. The speaker or environment noise and a speaker speaking rate [4] affects an articulatory estimation, which increases a variation to the system. Besides, an inter-speaker variation such as the speakerâ€™s vocal tract shape adds a variation to both acoustic signal and articulatory position [5]. Moreover, a current articulatory position correlates with the acoustic signal both before and after the current timeframe. The choice of an articulatory space impacts the design of the AAI system, e.g., an actual articulatory space [6, 7], an articulatory of a particular speaker, and a theoretical articulatory space [8, 9], a preprocessing of multiple articulatory data. Therefore, the AAI problem is challenging in the research in speech signals.  Current AAI models are researched to invert an utterance into articulatory features in an actual articulatory space, e.g., magnetic resonance imaging (MRI) [10] and electromagnetic articulography (EMA) [11-12]. A few studies proposed the AAI model on a theoretical articulatory space [13-14], i.e., a predefined articulatory space by transforming actual articulatory data. This is because a few explicit data are provided, and it is suitable for studying a speech production mechanism model [16-18]. Besides, the theoretical articulatory space is difficult to use without prior knowledge of speech production. A generalization of the AAI model trained on the theoretical articulatory space is not fully explored. Thus, this study desired to improve the AAI model performance and generalization on a three-dimensional theoretical vocal tract space.      2   1.2     Objectives 1. To design the speech production acquisition computational model. 2. To design and develop an AAI model using deep learning framework and the Birkholzâ€™s three-dimensional theoretical vocal tract model [8]. 3. To evaluate the performance and generalizability of the model by comparing synthesized speech and recording speech of a Thai vowel utterance from native Thai speakers.   1.3   Scopes 1. This study applied the VocalTractLab (VTL) application [15], a speech synthesizer based on a Birkholzâ€™s three-dimensional theoretical vocal tract model because the model produced an accurate and natural synthetic speech. 2. This study experimented with a monosyllabic and a disyllabic vowel utterance. 3. The AAI model estimates target articulatory because its trajectory is handled by the VTL synthesizer. 4. The AAI model is trained on a self-generated data using interpolation from a predefined articulatory data of the three-dimensional theoretical vocal tract model because a few data are explicitly provided. 5. The speech alignments and segmentation are manually performed in this study.   1.4   Expected Benefits The process of gathering of an actual articulatory data is complex and expensive, thus being able to encode an acoustic signal to an articulatory feature benefits many applications. In speech recognition, articulatory information has been used as a speech feature to train the speech-to-text model [20]. In speech synthesis, the inversion of an acoustic signal has been used to study human speech production improving voice quality and modify the synthesis speaker voice [21]. The articulatory information, which is decoded from speech data by the AAI model, has also been used in many applications. These applications are: 1) the study of a speakersâ€™ accent [22]; 2) pronunciation training [23]; 3) speakersâ€™ identification [24]; and 4) facial feature modeling [25]. This study demonstrated that synthesized data from the vocal tract model can be used as a speech corpus to enrich other datasets, which benefited low-resource speech and articulatory systems.  CHAPTER 2  LITERATURE REVIEW AND THEORY    A neural model of human articulatory learning explains how humans acquired speech production knowledge, i.e., learning to articulate [26]. The learning process involves both speech perception and speech production. The signal is being received from sensory neural. Then, the signal is being processed into sensory information. Last, the sensory information is delivered to the motor control. Besides, the neural model provides a solution to examine a speech production hypothesis in both normal and disorder systems [27]. The computational neural model of speech production acquisition is associated with both speech recognition and perception by encoding the perceived speech and speech synthesis and translating the encoded representation and produced a speech signal.   An artificial neural network has been applied to neuroscience [28] to study human perception, cognition, and motor control system. The artificial neural unit inspired by a biological neuron is a simplified design, but its ensemble creates a complex and powerful network model. The artificial neural network can capture a complex function, resembling a cortical function in our brain [29]. This powerful computational model is used to study the behavioral detail, i.e., the study on the response of a neuron component, while a more interpretable model is used to study the biological detail, i.e., the study on how the biological neuron function. Thus, in the context of this thesis, the artificial neural network is suitable to represent the process, where the articulatory data are estimated from acoustics.  The advent of a gradient-based method with backpropagation has revolutionized the neural network research. Moreover, the study suggested that the human brain may initiate an objective and activate a learning step [30]. This is represented as an abstract representation of an error-feedback mechanism, rearranging a neuron connection according to the error that simulates a transferring process of episodic memory to long-term memory. The episodic memory iteratively replayed the memory until it successfully transferred to long-term memory.   2.1   Speech Synthesis Speech synthesis is an artificial speech production, generating a speech sound from a given input, e.g., text and grapheme. The main goal of speech synthesis is to accurately generate acoustic speech resembling a speech produced by humans. The accuracy of synthesized speech is evaluated by measuring the completeness of the speech signal from a given input. The naturalness of synthesized speech is evaluated by human perception.   Previously proposed speech synthesis models are a formant speech synthesis [31], a concatenative speech synthesis [32], statistical parametric speech synthesis [33], data-driven model speech synthesis [34], codebook method [35], and physical model speech synthesis [36]. A formant synthesis utilizes a source-filter model to synthesize the speech signal, producing various speech-like signal characteristics. However, the 4  synthesized speech signal is unnatural and robotic-like. The concatenative synthesis accomplished in both the accuracy and naturalness of a synthesized signal. Nevertheless, the dataset constraints the synthesized speech variation, requiring a large dataset to generate a high speech quality and variation which increases the cost of acquiring and maintaining the speech corpus. Similar to the concatenative synthesis, the codebook method creates a dictionary model from a large dataset, impacting the cost of corpus acquiring and maintenance. The statistical parametric method is a promising speech synthesis solution, specifically a deep learning method. Both the Hidden Markov model (HMM) and deep learning generate a variety of speaker identity, emotion, and speaker speaking styles [33]. A large dataset is required to train a model, but not during model inferencing. However, the non-linguistic sound, e.g., a laughing sound, a coughing sound, a sighing sound, is challenging to be synthesized by the model. The physical model or articulatory model solves these problems [37]. Despite having a huge performance gap compared to the data-driven model, the physical model is a common solution to study human speech production because of its interpretability and ability to synthesize any possible speech produced by humans.   2.2   Physical Simulation of Speech Production Model A physical model for a speech synthesis imitates a human speech production system, where the glottal source model simulated the human vocal fold and the vocal tract model simulated a human vocal tract. The vocal tract model consists of articulatory motors, i.e., organs corresponding to human speaking. These articulatory motors are: 1) a tongue; 2) a jaw; 3) a soft palate; 4) a lip. The model synthesizes a speech signal by generating a harmonic wave from the glottal source model and rearranging an articulator in the vocal tract model to transform harmonic wave into a formant. The physical model can synthesize a speech with a diverse emotion and intonation.   An actual human speech production data is required to develop a physical model. A human speech production data acquisition is classified into a static method and a dynamic method [36]. The static method acquires static data from human speech production, for example, a computing tomography (CT), ultrasound, and MRI. The dynamic method acquires the movement of the articulatory, for example, cineradiography, X-ray microbeam, EMA, Electropalatography, Optopalatography, and Fast MRI.   Two types of physical models classified by articulatory space are a statistical articulatory space and a theoretical articulatory space. The statistical articulatory space is a vocal tract model developed from acquired data. The theoretical articulatory space is a vocal tract model developed by preprocessing multiple acquired data into a two-dimensional or three-dimensional space.  The statistical articulatory space consists of speaker articulatory information, e.g., the speaker's vocal tract shape and ratio. The theoretical articulatory space is defined by calibrating and interpolating an acquiring data, thus its shape and ratio are not related to a particular speaker. The theoretical articulatory space can synthesize a variety of high-quality speech signals other than the speech signal from the acquiring data.  A two-dimensional vocal tract model represents a midsagittal plane by the contour line of a human vocal tract. The model consists of a parameter control the shape of the 5  model. The most common two-dimensional vocal tract model is the Maeda model [8]. Estimating the area of a vocal tract only from a midsagittal distance, the two-dimensional vocal tract model produces less realistic speech in both accuracy and naturalness than a three-dimensional vocal tract model.    A three-dimensional theoretical vocal tract model represents a complex three-dimensional geometrical articulatory space. Although the construction of the three-dimensional theoretical vocal tract model is more complicated than the two-dimension and has more degrees of freedom, the model has better accuracy and naturalness than the two-dimensional model because the three-dimensional model is directly computed a vocal tract area function. Birkholz, et al. [9] has proposed a three-dimensional geometrical model and a gestural dominance-controlled model. The model has realistically simulated the coarticulation movement [38], which is an articulatory movement of each phoneme that depended on the context. The model is considered to be one of the advanced three-dimensional theoretical vocal tract models.   2.2.1 Birkholzâ€™s Three-dimensional Theoretical Vocal Tract Model Birkholzâ€™s three-dimensional theoretical vocal tract model is a geometric vocal tract model with a posteriori control parameter, illustrated in Figure 2.1. The model is developed from volumetric MRI data and CT data of a German native speaker by tracing, normalizing, and then calibrating the acquired data [39]. The model consists of seven wireframe meshes, where each represents a geometrical shape of the vocal tract model [9]. These meshes are: an upper lip, lower lip, upper teeth, lower teeth, upper cover, lower cover, and tongue.   The 24 articulatory parameters are defined to control the model, as illustrated in Figure 2.2. The jaw is controlled by a jaw angle (JA) and a displacement of its movement (JX). The velum shape is controlled by a shape of the velopharyngeal port (JS) and a velic opening parameter (VO). Both VS and VO parameters correspond to a nasal sound and an unvoiced consonant sound. The position of hyoid and shape of the larynx is controlled by parameter HX and HY. The lip is controlled by a lip protrusion (LP) and a distance between the lower lip and upper lip (LD). The other 12 parameters are TCX, TCY, TRX, TRY, TBX, TBY, TTX, TTY, TS1, TS2, TS3, and TS4. These parameters control the tongue movement. The MS1, MS2, and MS3 parameters constrain the vocal tract area function, which reduces the noise produced from a fricative sound.   The range of each articulatory parameter prevents the abnormally anatomic shape by restraining a model's degree of freedom, as shown in Table 2.1. These constraints are a soft constraint, i.e., the occurrence of some unnatural vocal tract position is possible. Moreover, some combinations of articulatory parameters are correlated and redundant.   The gesture score, constituted from gestures representing an utterance, is used to manually regulate the unrealistic vocal tract shape. The gesture of this model is a coordinated movement of one or more articulators at a specific interval. These gestures include vocalic, consonantal, glottal, pulmonary, F0-phase, and accent command. The vocalic and consonant gesture are defined by a predefined articulatory parameter from the MRI data. When synthesizing speech signals from the gestural score, the 6  articulatory parameter is initialized at the neutral position and then moved to the target articulatory defined in a gesture score. The glottal model controls the pressure and the fundamental frequency (F0) from a subglottal system. The realization of the coarticulation effect is calculated from a gestural score using the target approximation model [40]. The model is developed to effectively synthesize the effect of the coarticulation sound of both vowel-vowel and consonant-vowel syllables.   
  Figure 2.1 The rendering of the 3D vocal tract model [40]   
  Figure 2.2 Birkholzâ€™s three-dimensional theoretical vocal tract  model parameters [40]   2.3   VocalTractLab 2.2 Application Program Interface The VocalTractLab 2.2 application [19] is a speech synthesizer using Birkholzâ€™s three-dimensional theoretical vocal tract model. The application provides an application program interface (API), which enables the access of an application function using programming language, e.g., Python and Matlab. The VTL application uses a gesture file consisting of a defined gestural score and a geometrical vocal tract model to synthesize the speech signal. Both gesture and speaker files are defined in an XML 
7  format. The output audio file is defined in a WAV format. The graphic user interface of the VTL application provides a visualization of the articulation in a vocal tract model. The application provides an analysis tool to study a relationship between articulation and speech signals produces by the vocal tract model.   Table 2.1 The articulatory parameters of the vocal tract  Descriptions Min. Max. Unit Horizontal hyoid position 0.0 1.0  Vertical hyoid position -6.0 -3.5 cm Horizontal jaw displacement -0.5 0.0 cm Jaw angle -0.7 0.0 deg Lip protrusion -1.0 1.0  Vertical lip distance -2.0 4.0 cm Velum shape 0.0 1.0  Velic opening -0.1 1.0  Tongue body center X -3.0 4.0 cm Tongue body center Y -3.0 1.0 cm Tongue tip X 1.5 5.5 cm Tongue tip Y -3.0 2.5 cm Tongue blade X -3.0 4.0 cm Tongue blade Y -3.0 5.0 cm Tongue root X -4.0 2.0 cm Tongue root Y -6.0 0.0 cm Tongue side elevation 1 -1.4 1.4 cm Tongue side elevation 2 -1.4 1.4 cm Tongue side elevation 3 -1.4 1.4 cm Tongue side elevation 4 -1.4 1.4 cm Min area tongue back region 0.0 0.3 cm2 Min area tongue tip region 0.0 0.3 cm2 Min area lip region 0.0 0.3 cm2   2.4   Speech Features Various methods [41] are proposed to extract useful features from speech signals. These methods reduce a dimension from a stationary to non-stationary transformation, for example, line spectral frequency (LSF), filter banks (FBANK), perceptual linear prediction (PLP), and Mel-frequency cepstral coefficient (MFCC). The MFCC is the most commonly used speech feature in both the speech recognition and synthesis because of a decorrelated characteristic of the feature which improved model efficiency.  The comparison of speech features in the AAI problem has been previously studied [42]. The study compared speech features. The study reported that a higher smoothness of the speech feature and the short window length positively affected the 8  accuracy of the model. Recently, the MFCC is used because its performance is outperformed others [43].    2.4.1 Mel-frequency Cepstral Coefficient Mel-frequency cepstral coefficient (MFCC) [44] is a decorrelated coefficient of a speech feature representing the overall shape of the spectral envelope. The first few coefficients of MFCCs are approximately a pitch-invariant representation of a speech signal. This representation aims to separate a source and a filter from a speech signal. The cepstral in an MFCC representation contains a rate of change in each spectral band. The Mel-scale approximates a human auditory scale system, where a low frequency is more sensitive than a higher frequency. The transformation of a speech signal to an MFCC representation is shown in Figure 2.3.      Figure 2.3 An MFCC feature extraction diagram   First, the pre-emphasis filter amplifies the high-frequency spectrum of the speech signal, where its magnitude is usually lower than a low-frequency spectrum. Second, the short-time Fourier transform (STFT) using a Hamming window transforms a time-domain signal into a time-frequency domain signal by computing Fourier transform over a short period signal. The result from STFT on a local segment is a spectrum consisted of a frequency range and magnitude over a local speech signal section. The changed in the spectrum over time is called a spectrogram. The short time window reduces a stationary characteristic of the speech signal to have more a nonstationary-like characteristic. Third, the filter bank is applied on a Mel-scale of the spectrogram to extract frequency energies, which is called the Mel-filter bank coefficient. The filter bank is a triangle response having a magnitude of zero. The Mel-scale is a non-linear transformation of a frequency scale, where lower frequency energies are more discriminated than higher frequency energies. The Mel-filter bank can be applied as a speech feature. Last, the discrete cosine transforms (DCT) are applied to decorrelate a Mel-filter bank coefficient. The decorrelation provides a benefit in a data-driven model training, where the decorrelated input is preferred. The first 13 cepstral coefficients are commonly used because it contains information of the filter function of the speech signal, while a higher cepstral coefficient represented the source function of the speech signal.    2.5   Deep Neural Network Architecture Deep learning is a sub-field of machine learning primarily related to the integration of artificial neural network (ANN) architecture and learning theory. The simplest unit of the ANN is a neural unit, which consists of the input weight and activation function, as illustrated in Figure 2.4. The multiple weighted inputs are summed together to form a linear function and then passed to an activation function. The activation function 
9  transforms linearity into non-linearity. Given a vector of an input ğ‘¥, weight ğ‘¤, bias ğ‘, and approximation of output ğ‘¦%, the computation of the single neural unit is defined in Equation 2.1.   ğ‘¦%=ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¤ğ‘¥+ğ‘)	(2.1)   
  Figure 2.4 Neural unit   The multiple neural units form a fully-connected layer. The stack of the fully-connected layer forms a deep neural network (DNN), as shown in Figure 2.5. The first layer is an input layer, which is a placeholder for input data. An intermediate layer is a layer between the input and the output layers, which performs a feature extraction. The feature from the intermediate layer is called a latent feature. The DNN is a universal approximator [29] approximated any complex non-linearity regression from given information.   2.5.1 Learning Algorithm: Gradient-based learning method The DNN uses a gradient-based learning method. It is an optimization task whose objective is to minimize the cost function ğ½(ğœƒ). A ğœƒ is a model parameter vector, which is a set of weights in each neural unit, and ğœƒ	âˆˆ	â„. The common cost function for the regression task is a mean square error (MSE), where the Euclidean distance between an estimated prediction ğ‘¦% and an actual target ğ‘¦ is being computed. Thus, the objective is to find ğœƒâˆ—, which is a model parameter vector that results in minimum cost as follows:  ğœƒâˆ—=argğ‘šğ‘–ğ‘›	ğ½(ğœƒ)(2.2)  The gradient-based learning uses a gradient to minimize the cost function by taking the first-order partial derivative to a cost function denoted as âˆ‡"ğ½(ğœƒ), where a slope at point ğœƒ is obtained. Thus, if the model parameter vector is changed to ğœƒ+ğœ–, the value of the cost function is changed as follows:  ğ½(ğœƒ+ğœ–)â‰ˆğ½(ğœƒ)+	ğœ–âˆ‡"ğ½(ğœƒ)(2.3)   
10  
  Figure 2.5 MLP   The cost function ğ½(ğœƒ) at point ğœƒ is decreased fast when it moves in the direction of the negative gradient as follows:  ğ½(ğœƒâˆ’ğœ–)â‰ˆğ½(ğœƒ)âˆ’	ğœ–âˆ‡"ğ½(ğœƒ)(2.4)  The algorithm minimizes the cost function and updates the parameter as follows:   ğœƒ#=	ğœƒ#$%âˆ’ğœ–âˆ‡"ğ½(ğœƒ#$%),ğœ–âˆˆâ„(2.5)  The term ğœ– is a learning rate, which is a step size of a negative gradient of a parameter ğœƒ#$%. If a new point is iteratively updated using Equation 2.5, the cost will reach the minimum point. However, the cost function of the DNN model is usually non-convex. Thus, the cost function is converged to a local minimum instead of the global minimum.   Mini-batch gradient descent is a common learning algorithm of a DNN model, which is computed an average of a gradient over a small batch of training data ğ‘¥(':#)={ğ‘¥',ğ‘¥'*%,ğ‘¥'*+,â€¦ğ‘¥#}, as follows:     ğœƒ#=	ğœƒ#$%âˆ’ğœ–âˆ‡"ğ½Bğœƒ#$%,ğ‘¥(':#),ğ‘¦(':#)C,ğœ–âˆˆâ„(2.6)  The common practice is to use the largest possible mini-batch size that fitted into the available computational memory. The term of mini-batch stochastic gradient descent is used interchangeably with a stochastic method.   2.5.2 Adam Optimization An adaptive moment estimation optimization, called Adam optimization [16], is an adaptive learning rate stochastic gradient optimization, adjusting the learning rate during the DNN training. The Adam applies both the momentum and the accumulated sum of a square gradient, defined as follows:  ğ‘£,	=	Î²%ğ‘£,	+(1âˆ’Î²%)g(2.7) 
11  ğ‘ ,=	Î²+ğ‘ ,+(1âˆ’Î²+)gâ¨€g(2.8)  The term ğ‘£, and ğ‘ , are the first momentum and the second momentum of the gradient respectively. These values are initialized at zero which tends to bias toward zero later during the learning process. Thus, the correct bias terms are defined as follows:   v-I=	ğ‘£,	1âˆ’Î²%.(2.9)  s,I=	ğ‘ ,1âˆ’Î²+.(2.10)  The learning rate is updated as follow:  Ïµ=	Ïµv-ILs,I+ğ›¿(2.11)  Practically, the hyperparameter Î²% and Î²+ is equal to 0.9 and 0.999 sequentially. This learning algorithm is proved to be practical. Thus, it becomes a common design choice for training a deep learning model.    2.5.3 Backpropagation Backpropagation [17] is an efficient gradient computational method in a deep learning model using a chain rule of differential calculus. The simple multilayer neural network is defined as in Equation 2.1. This simple multilayer neural network consists of a neural unit weight vector ğ‘¤, input vector or a post-activation vector from a previous layer ğ‘¥, and a pre-activation output vector ğ‘. ğ¿ is a total number of the neural network layer. The forward propagation is predicted an output ğ‘¦% as follows:  ğ‘¦%=	ğ‘¥/=	ğœ(ğ‘¤/ğ‘¥/$%+ğ‘/)(2.13)  The function ğœ  is a differentiable activation function. The error term is computed by comparing a prediction ğ‘¦% with an actual target ğ‘¦ using a cost function defined as follows:  ğ¶=	ğ¿(ğ‘¦%,ğ‘¦)(2.14)  Then, the gradient is computed from an error term using a chain rule of differential calculus from the output layer ğ¿ to the first layer, called backpropagation, defined as follows:  âˆ‡,!ğ¶=	ğœâ€²(ğ‘0)â¨€âˆ‡1!ğ¶(2.15)  âˆ‡2!ğ¶=	ğ‘¥0$%â¨€âˆ‡,!ğ¶(2.16)  âˆ‡1!"#ğ¶=	(ğ‘¤0)3â¨€âˆ‡,!ğ¶(2.17)  12  The â¨€ is an element-wise matrix multiplication operation. The stochastic method minimizes the error term by applying a negative gradient from an Equation 2.17 to adjust the modelâ€™s weight, defined in Equation 2.5. The gradient of the bias term is adjusted as follows:  b0=	ğ‘0âˆ’ğœ–âˆ‡,!ğ¶(2.18)  The deep learning model is trained by iteratively computing a forward-propagation and a backpropagation until reaching a certain number of iterations. Each iteration is called an epoch. One epoch is equal to training a model over a dataset once.    2.5.4 Activation function Without the activation function, the deep neural network is just another linear model. The tanh activation function is one of the common activation functions, defined as follows: 	tanh(ğ‘)=	ğ‘’,+ğ‘’$,ğ‘’,+ğ‘’$,(2.19)  The downside of the tanh activation function is the saturation of a function resulting in gradient vanishing. The rectified activation unit (ReLU) activation solves a gradient vanishing during backpropagation. The function is computed faster than the tanh activation function [18]. The ReLU activation function is defined as follows:    ReLU(ğ‘)=	max	(0,ğ‘)(2.20)  The ReLU activation function improves model convergence rate and creates a sparsity of the networkâ€™s activation by deactivating neurons when pre-activation less than zero.    2.5.5 Parameter initialization The parameter initialization is designed to break asymmetry between each neural unit in a DNN. The unit symmetrizing to each other is updated with the same gradient when applied to a gradient-based learning algorithm, which affects the neural modelâ€™s efficiency. The He initialization [19] is used together with ReLU activation, defined in Equation 2.21. The ğ‘ is a pre-activation, and ğ‘š' is a number of outputs in a layer ğ‘–.  Var[ğ‘]=	2ğ‘š'(2.21)  2.5.6 Model Regularization Methods The regularization method is a strategy to constrain the deep learning model capacity. This method applies during the model learning process to reduce the generalization error and prevent early overfitting.     13  2.5.6.1 Dropout The dropout regularization [20] approximates an ensemble bagging model by randomly creating a subnetwork from a defined model in each iteration. The subnetwork is created by masking the output of the modelâ€™s neural unit to zero. It prevents a co-adaptation of the neural unit by training different neural units set in each training iteration and forces each neural unit to be independent. During inferencing, all neural units are active but scaling down by the defined retained probability.    2.5.6.2 Data Augmentation Data augmentation is a transformation of input data ğ‘¥ without changing its label ğ‘¦. To design an augmentation function, the density estimation of the population or the empirical distribution must be known.   In speech processing, the vocal tract length perturbation (VTLP) [21] mimics a voice from different speakers while maintaining the same speech content using a filter bank frequency warping before applied DCT. The speech perturbation [22] shifting the frequency component by applying the time-warping factor ğ›¼ to the time domain signal, in which affect the Fourier transformation, as follows:   ğ‘¥(ğ‘¡)=ğ‘¥(ğ›¼ğ‘¡)(2.22)  The stochastic feature mapping (SFM) [23] is a method to map an acoustic feature from one speaker to another, where these two speakers have the same utterance and target. The acoustic model of each speaker mimics the acoustic feature of another speaker by minimizing the distance error. The noise injection and its variant increase the robustness of the model by applying a noise signal to speech feature, for example, a speech waveform, a spectrogram, and a Mel-spectrogram. The noise injection adds a Gaussian noise to the speech feature, defined as follows:  ğ‘¥(ğ‘¡)=ğ‘¥(ğ‘¡)+ğ‘›(ğ‘¡)(2.23)  A ğ‘›(ğ‘¡) is a Gaussian noise, or uniform arbitrary noise to simulate white noise. The random censoring is a noise injection variant by adding a random sequence of zero and one ğ‘(ğ‘¡), defined as follows:   ğ‘¥(ğ‘¡)=ğ‘¥(ğ‘¡)âˆ—ğ‘(ğ‘¡)(2.24)  Volume modification is a method to randomly multiply noise to a signalâ€™s amplitude with arbitrary factor ğ›¼, defined as follows:  ğ‘¥(ğ‘¡)=ğ›¼ğ‘¥(ğ‘¡)(2.25)  Other speech data augmentation methods are a pitch-shifting [24] and a speech rate modification. The pitch-shifting method transforms speech by shifting a frequency of speech signal and applying a PSOLA to correct the distortion. The speech rate modification method transforms speech by randomly adding factors to the speech rate without affected its pitch and its formant using WSOLA [25]. Another speech augmentation is the used of synthesizing methods [26, 27] to generate more data. 14  2.5.6.3 Early Stop Mechanism The early stop [28] is an implicit regularized technique by monitoring and halting the optimization process. This technique prevents the model to perfectly fit the training data. Typically, the halting is done by monitoring the minimum validating error and halting the process either manually or automatically.  Not only a model capacity correlated to a generalization error but also the model architecture had a soft regularization effect on the generalization error [29].   2.5.7 Convolutional Neural Network The convolutional neural network (CNN) [30] is one of the neural unit architectures specialized in pattern recognition of an image and sequential data. Instead of matrix multiplication between weight and input feature, the convolutional layer applies convolution operation. The convolution operation in the neural unit architecture is a cross-correlation operation, i.e., a convolution operation without a flipped kernel. The discretized equation is defined as follows:  ğ‘=eğ‘¥(ğœ+ğ‘¡)ğ‘¤(ğœ)(2.26)  The displacement parameter ğ‘¡ is placed at an input feature because the feature has less variation in range than the kernel. The term ğ‘ is a pre-activation output or a feature mapping in a term of a convolution operation. The localized connectivity property in a convolutional neural layer is a localized spatial feature extraction, where the local region is defined by a hyperparameter kernel size ğœ. The weight sharing is another property of the convolutional neural layer, which applies a trainable kernel ğ‘¤'(ğœ) to the entire input feature, where ğ‘–âˆˆğ¼ and ğ¼ are a number of kernel filters. This property reduces a modelâ€™s parameter compared to a fully connected model and provides the invariant translation property, in which the representation can be recognized regardless of time. Other hyperparameters are a skip operation called stride and padding, which specifies the edgeâ€™s padding of the input feature during convolution.   2.5.8 Recurrent Neural Network A fully-connected neural network is unable to capture the sequential feature because the network considers each frame independently. The recurrent neural network (RNN) is a sequence-based neural network specialized in a sequential representation extraction. The recurrent neural unit is resembled a general neural unit but consists of a recurrent state or hidden state â„ which fed an activation to itself from time ğ‘¡ to time ğ‘¡+1, defined as follows:  ğ‘(.)=ğ‘Šâˆ—jâ„(.$%),ğ‘¥(.)k+ğ‘(2.27)  â„(.)=ğœ(ğ‘(.))(2.28)  The ğ‘Š weight matrix composes of the weight of the input feature and the weight of the recurrent state. ğ‘(.) is the activation of the timestep ğ‘¡ from an activation function ğœ. The RNN uses a tanh activation function. The simple recurrent unit causes a 15  gradient vanishing problem when applied with a long sequence feature. The long short-term memory (LSTM) unit [31] reduces the occurrence of this problem by adding memory state ğ‘(.). The diagram of the LSTM is illustrated in Figure 2.7.   
  Figure 2.6 LSTM block  A forgot gate ğ‘“(.) controls a flow of memory state by adjusting how much the information is carried on to this state. The input gate ğ‘–(.) controls the information concentration of the LSTM unit activation adding to the memory state. The output gate ğ‘œ(.) controls the information concentration of the recurrent state â„(.) and the output activation ğ‘¦(.) of the LSTM. These gates are controlled by hard-sigmoid activation function to constrain its output to a discrete value of zero and one. The computation of the LSTM unit is defined as follows:  ğ‘“(.)=â„ğ‘ğ‘Ÿğ‘‘_ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘Š4jâ„(.$%),ğ‘¥(.)k+ğ‘4)(2.29)  ğ‘–(.)=â„ğ‘ğ‘Ÿğ‘‘_ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘Š'jâ„(.$%),ğ‘¥(.)k+ğ‘')(2.30)  ğ‘œ(.)=â„ğ‘ğ‘Ÿğ‘‘_ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘Š5jâ„(.$%),ğ‘¥(.)k+ğ‘5)(2.31)  ğ‘(.)=ğ‘¡ğ‘ğ‘›â„(ğ‘Š,jâ„(.$%),ğ‘¥(.)k+ğ‘,)(2.32)  ğ‘(.)=ğ‘“(.)ğ‘(.$%)+ğ‘–(.)ğ‘(.)(2.33)  ğ‘¦(.)=	â„(.)=ğ‘œ(.)tanh	(ğ‘(.))(2.34)  The general recurrent neural unit is a one direction recurrent state whether a forward direction {1,2,â€¦,ğ‘‡} or a backward direction {ğ‘‡,ğ‘‡âˆ’1,â€¦,1}. The bidirectional recurrent unit [32] computes in both directions. Then, the result is concatenated, as illustrated in Figure 2.8. The computation of bidirectional recurrent unit using a simple recurrent unit is defined as follow:  â„452,67(.)=ğœ(ğ‘Š452,67âˆ—jâ„452,67(.$%),ğ‘¥(.)k+ğ‘452,67)(2.35)  â„8,9:2,67(.)=ğœ(ğ‘Š8,9:2,67âˆ—jâ„8,9:2,67(.$%),ğ‘¥(.)k+ğ‘8,9:2,67)(2.36)  
16  ğ‘¦(.)=[â„8,9:2,67(.),â„452,67(.)](2.37)  For the LSTM unit, the output ğ‘¦(.) is a concatenation of the result from the output gate from both forward and backward directions.    
  Figure 2.7 Bidirectional recurrent layer   2.6   Acoustic-to-Articulatory Inversion Model  2.6.1 Codebook Lookup Approach The codebook lookup approach is a vector pair of an acoustic feature and an articulatory feature using a given solution and an optimization method. The input acoustic feature is used to search for a related articulatory. This approach treats an acoustic-to-articulatory inversion as a linear problem while the problem is nonlinear. To handle the nonlinearity, a hierarchy of hypercube is used [62] to represent a region of an articulatory space, where the acoustic feature and the articulatory feature have a linear relationship. Nevertheless, this approach depends on a given solution in which the quality is not reliable.   2.6.2 Generative Model Approach The generative model, e.g., HMM [63] and Gaussian mixture model (GMM) [64], maps the relationship of an acoustic and an articulatory using linear transform function. In [65], the study used a mixture of probabilistic canonical correlation analysis to solve an overfitted caused by its complexity. This method achieved an RMSE of 1.56mm and 1.48mm of a female and male speaker sequentially. While a generative model requires fewer data and interpretable, the discriminative model, e.g., machine learning and deep learning model, is more accurate and more generalized.   2.6.3 Deep Learning Approach Most of the data-driven approach studied on open-source data from a MOCHA-TIMIT [6] and MNGU0 dataset [7]. An EMA articulatory data consists of 7 parameters: an upper incisor, a lower incisor, an upper lip, a lower lip, a tongue tip, a tongue blade, a tongue a dorsum, and a velum. In [66], a feed-forward neural network with 3 hidden 
17  layers was reported to perform better than a trajectory mixture density networks, support vector regression, autoregressive artificial neural network, and distal supervised learning. In [67], a promising result was reported using the regression deep neural network with a mixture density network as an output. A restrict Boltzmann machine (RBM) was used to train hidden layers. Similarly, the general regression neural network (GRNN) architecture [68] had been compared with the deep belief network (DBN). The acoustic data was represented as an MFCCs with its velocity and acceleration. This study reported that the GRNN was performed better than the DBN model. In [69], the study applied a deep neural architecture to estimate the dynamic movement of the articulatory from the series of MRIs. The acoustic feature was represented in a form of LSF. The hidden layer of the deep neural network was pre-trained using RBM. The output layer was a two-layer stacking linear regression. The study reported that the deep neural networks performed better than the GMM comparing the RMSE from the model. In [70] the study has reported that the bidirectional LSTM network (BiLSTM) lowered the RMSE to 0.816mm. The acoustic data was represented in the form of an LSF and fed to a model with an additional 15ms delayed from a current EMA articulatory frame. The model consisted of four hidden layers consisted of two layers fully-connect layer and two bi-directional LSTM layers to learn the dynamic feature of the acoustic data. In [71], the study applied the MFCCs feature instead of LSF together with a word embedding and a three hidden layer BiLSTM. A convolutional neural network and skip-connection were used to a BiLSTM, as illustrated in Figure 2.8 [72]. This model achieved state-of-the-art RMSE on the MNGU0 dataset of 0.690 mm and the correlation coefficient of 0.949. The MFCC with 25 coefficients and its delta feature together were used with a current ultrasound image to construct the ultrasound image of the next articulatory trajectory using a fully-connected with five hidden layers, having 500 neural units each [73]. Therefore, the deep learning model had influenced the modeling choice in the AAI problem because of its capability to extract complicated speech features and its performance outperformed other methods.    2.7   Computational Model of Speech Acquisition The AAI model is a part of a speech production model, as discussed in the following section. The speech production computational model had included a theoretical articulatory model because of its interpretability.     Figure 2.8 Long-term recurrent convolutional neural network architecture [72]    
18  2.7.1 Distal Learning and Analysis by Synthesis  The analysis by synthesis method using a distal learning model [16] was proposed to study a childâ€™s speech imitation without articulatory instruction, as shown in Figure 2.10. The study applied Birkholzâ€™s three-dimensional theoretical vocal tract model as a forward model to synthesize a speech. The distal learning model resembles supervised learning when the articulation is not measurable, thus the produced speech is measured instead.  The model inverses original speech into an articulation. The forward model produces speech from this articulatory command. This synthesized MFCCs feature is compared with the original speech using a sum of square error. The gradient descent optimization with embodiment constraints improves the model by producing an accurate synthesized speech.  The quality of mimicked speech was analyzed by computing the RMSE of the formant frequency (F1-F3) of a first and a second position of a disyllabic vowel utterance between the synthetic speech and the original speech. The study has reported that the F1 and F2 of the first and the second positions were not a significant difference. A perception test was conducted to evaluate a synthesis quality by having listeners identified the vowel and rated the naturalness of the sound. The identification rate of both synthetic and original sound was not significantly different. In contrast, the naturalness rate of the original sound was better than the synthesis sound.  The study claimed that the analysis by synthesis using a distal learning model was effectively represented the speech production acquisition model. However, the stochastic gradient descent optimizer consumed a huge amount of time for large data and the result was not assured. The study [17] applied a particle swarm optimization, i.e., a metaheuristic optimization method. The study claimed that this method consumed less computational time than the previous method while having a similar result. Using a similar model, the study [18] had an experiment on a consonant-vowel. A genetic algorithm has been applied instead of gradient descent, where each gene in a chromosome was a gesture parameter in a VTL. Gesture parameters were a target articulatory, its duration, and a time to reach its target articulatory. The first population was initialized using a rule-based model based on a phonetic transcription of the original speech. This population is refined to increase its speech quality using a crossover, mutation, and regularization. The next parent was selected based on the cosine distance between the original speech and synthesized speech. The study claimed that this model performed better than coordinate descent. A perception test score, in terms of similarity between reference speech and synthetic speech, showed that the reference and resynthesize speech resemble. However, the result from this model did not work well to all the phonemes and the result was not guaranteed. Moreover, the model could not generalize with a new speech and required a recomputing of the optimization process.   19  
  Figure 2.9 The articulatory target estimation [16]   2.7.2 Distal Learning with Multilayer Perceptron Model The distal learning with a multilayer perceptron has been proposed [74] using the Maedaâ€™s vocal tract model, which is a two-dimensional vocal tract synthesizer, to model a speech acquisition. The babbling model randomly generates speech data for training an imitation model. Comparing with the distal learning model, supervised learning is trained by comparing an articulatory parameter, while the distal learning is trained by comparing the synthesized speech and target speech. The study reported that both models performed well on a speech from the same distribution, while the performance significantly drops when imitated the external speaker.    2.7.3 Reinforcement Learning with Neural Model A speech acquisition model has been proposed to model a babbling stage in an infant using reinforcement learning with an echo state network (ESN) [75]. The study considered the speech acquisition into two learning stages: 1) the sensory learning; 2) the sensory-motor learning stage. The sensory learning stage memorizes the speech representation learned from the auditory system. The sensory-motor learning stage maps these representations to an articulatory control. The Birkholzâ€™s theoretical vocal tract model simulates a speech from motor control. The synthesized speech was compared to the original speech, i.e., a speech from a mentor or parent. The evaluation result was used to update the neural model. The original speech from a parent are /a:/, /i:/, and /u:/. The motor command with the highest evaluation reward was memorized during the training and used to imitate the target speech during inferencing. The parent vocal tract model was a predefined speaker of an adult male native German speaker, and the infant vocal tract model was a scaling version of a parent model. Without the information of lip and jaw, the /u:/ sound was hard to learn than the model learning with this information. The study concluded that some speech was challenged to imitate only auditory information. The limitation of this study was that just three vowels were used in this experiment.    
20  2.7.4 State Feedback Model  The internal feedback model has been proposed to improve speech production control by realizing an explicit representation of motor command [76]. The speech was synthesized from the motor command using a Maedaâ€™s vocal tract model to simulate a babbling stage. The acoustic-to-motor inversion model simulates an imitating stage. The babbling model was designed with an unknown a priori to aid the inversion model. The babbling model used an HMM to search for the significant phonetic region of the input space for a trajectory articulatory using cosine interpolation. The imitation model is an inversion model using a supervised learning training strategy by giving a speech feature as an input and an articulatory parameter as an output. The MLP model with stochastic method and backpropagation was applied. The study reported that the inversion of the speech from the same distribution was accomplished. However, the inversion from a different distribution was not achieved. The mismatch in the speaker characteristic problem produced an error. Moreover, the inversion model performed worse when applied to the actual human speech.   In [77], the study improved the previous model by applying a state feedback model. The disturbance from an articulation was controlled by state feedback, which stabilized the Birkholzâ€™s vocal tract speech synthesizer. The error represented in the articulatory domain was computed and then feedback to the AAI model. The babbling model generated a repeated sequence of 16 vowel utterances from a male speaker by using the cosine interpolation to interpolate trajectory articulatory. The speech represented as frequency filter bank features was generated using the synthesizer. The MLP model was trained using a supervised learning strategy with backpropagation and conjugate gradient descent. The study reported that the model can estimate an articulatory parameter and resynthesize a resemble speech comparing with a target speech from the same distribution. However, the evaluation using actual recorded speech had not been reported.    Thus, while the computational of speech production acquisition has been studied, the generalization of the model with actual human speech does not fully explore. The DNN model shows a better performance comparing to the optimization method in terms of consistency of inferencing and inferencing time. Therefore, this study proposes the novel speech production acquisition using deep learning on a Birkholzâ€™s three-dimensional theoretical vocal tract model, which is capable of performing AAI and provides generalizability with the actual human speech. Besides, this study shows the use of speaker simulation by interpolating to increase variation in the data along with other data augmentation methods. These method aims to improve the model inferencing of unseen speech data from a different distribution.  CHAPTER 3 METHODOLOGY    3.1   Speech Production Acquisition Model  The conceptualization of the speech production acquisition model is divided into three parts: babbling, speech imitation, and self-learning. The babbling initializes the control of the articulatory motor by searching all combinations of a motor command and producing a speech. Speech imitation occurs when the learners learned by mimicking the parentsâ€™ speech which enhances both speech production and speech memorization. Self-learning improves speech production using memory, imagination, and self-evaluation. The imagination is constructed from an auditory perceptual simulation mechanism using a semantic and episodic memory. It is activated by an intention [78,79]. This constructive mechanism is known as replay experience. An evaluation of a speech production acquisition in self-learning uses internal feedback and an internal validator to evaluate and correct the produced speech. The internal validator evaluates a task by mimicking an exact inner voice. The speech imitation and self-learning work simultaneously, as illustrated in Figure 3.1. The speech imitation mimics speech from an external speaker. The self-learning mimics the speech from a constructive internal voice. The diagram of the proposed speech production acquisition model was illustrated in Figure 3.2. The gray area highlights the computational model developed in this study.     Figure 3.1 Three stages of a speech production acquisition   In Figure 3.2, the memory simulation module simulates the constructed speech signals and sends this speech signal to the auditory-motor mapping model to estimate an articulatory motor command. The internal feedback and internal evaluation mechanism work together to adjust the auditory-motor mapping model by improving the accuracy in estimating the articulatory motor command. The intention to produce the speech activates a speech production mechanism that transforms articulatory command into an actual speech. Following the distal learning model in the study [16], the self-learning uses external feedback by listening to the produced speech and internally evaluated the accuracy between target speech and the mimicked speech. The learners adjust articulation based on the difference measured by the internal evaluator.   
22  
  Figure 3.2 The proposed speech production acquisition model   This thesis aims to develop the computational model of speech production acquisition using a deep learning framework. The neural networkâ€™s weight initialization simulated the babbling as an initial mapping module. The model primarily focused on a self-learning. Components of a self-learning consist of a data generator module to simulate the memory simulation module, and a deep learning framework to simulate an internal feedback learning mechanism. The speech imitation was not developed in this study. The following experiment, as shown in Figure 3.3, was developed to prove this concept of speech production acquisition.  The overview of the experiment was illustrated in Figure 3.3. The deep learning AAI model was trained from synthetic data which was generated from a data generator module. The model was trained using a supervised learning method, where a speech feature was used as an input and its articulatory parameter was used as a label. The speakersâ€™ vocal tract model was simulated and used in synthetic speech production. The gesture score was generated. The generation method aimed to maximize variation in the synthetic speech signal. The synthetic speech and its articulation were preprocessed by transforming speech data into an MFCCs representation and normalization. The model was applied to estimate the target articulatory from actual recorded data from 12 Thai speakers. The inverted speech was synthesized using VTL. This inverted speech could not be directly compared to the recorded speech because the target articulatory of the actual speaker was unavailable, and each speaker produced different speech formant frequencies for the same vowel. Thus, this study developed another neural network model that acted as an evaluator to identify the vowel of the inverted speech. Simulating the perception test, the evaluator model identified an inverted speechâ€™s phonetic from a given inverted speech feature. If the AAI model was correctly estimated target articulatory, the evaluator would identify the same phonetic description of the actual recorded speech. The evaluator model was 
23  trained by using the same recorded data applying to the AAI model. The reason to use a model to perform a simulation of a perception test was an ability to provide instance experimental results while performing actual perception required significantly more time and resources.    
  Figure 3.3 Experiment overview   3.2   Data Generating Method The data generator module was designed to generate a realistic articulation. The method produced a monosyllabic and a disyllabic vowel utterance with a wide variation. The VTL speech synthesizer was used to synthesize speech from a target articulatory parameter. The predefined adult speaker on the Birkholzâ€™s three-dimensional theoretical vocal tract space named JD2.speaker, predefined articulations 
24  of a vowel, and its phonetic description were provided. The predefined adult speaker and a child speaker that transform using the deterministic method [80] were used in a speaker interpolation. The data generator diagram was illustrated in Figure 3.4.   
  Figure 3.4 Data generation process   Predefined articulations of a vowel were used as a data point to interpolate the articulation space of a speaker model. These predefined articulations were /a:/, /i:/, /u:/, /e:/, /É›:/, /É¤:/, /o:/, /Å“:/, /É™:/, /Ã¸:/, /ÊŠ:/, /É‘:/, /É’:/, /y:/, and /É”:/. One articulation consisted of 24 articulatory parameters. The used a predefined articulatory in the interpolation was to ensure a realistic articulation, which was impossible in a completed randomization method.  The articulation space from one vowel to others was interpolated using a linear interpolation function, a time-warping, on both parameter in the cartesian coordinate and the radian coordinate, defined as follows:  ğ‘…=ğ‘¢ğ‘ƒ+(1âˆ’ğ‘¢)ğ‘„,ğ‘¢âˆˆ(0,1)(3.1)  ğ‘ƒ and ğ‘„ were a data point represented the selected predefined articulation vector consisted of 24 articulatory parameters. ğ‘… was an interpolated articulation vector. ğ‘¢ was a warping factor ranging from zero to one. To prevent the overrepresentation of a middle articulation /É¤:/, the hard-warping constraint factor ğ›¾ was added to the interpolation function which was controlled the range of warping allowance from ğ‘ƒ, defined as follows:    ğ‘¢=wğ‘¢,					ğ‘–ğ‘“	ğ‘¢<	ğ›¾0,			ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’,ğ›¾âˆˆ(0,1)(3.2)  This study assigned ğ›¾ to 40% allowance. The ğ‘ƒ and ğ‘„ were randomly selected using a simple random uniform sampling schema. The probability distribution of the random 
25  sampling without replacement and having a fixed sample size of 2 was defined as follows:  	ğ‘(ğ‘ )=z{ğ‘!2(ğ‘âˆ’2)!~$%												,âˆ€ğ‘ âˆˆğ‘†																0																								,ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(3.3)  ğ‘ was a total number of articulations in a predefined articulation pool ğ¾. ğ‘† was a subset of the articulation pool ğ¾ and ğ‘†={ğ‘ âŠ‚	ğ¾|#ğ‘ =2}. ğ‘  was a selected articulation sample. The inclusion probability of the selected pair of samples ğ‘;< was defined as follows:  ğ‘;<=2ğ‘(ğ‘âˆ’1)(3.4)  The warping factor ğ‘¢ was randomized using a uniform sampling schema without replacement to prevent duplication with a fixed sample size of 1. The probability of random sampling ğ‘¢ was defined as follows:   ğ‘ƒ(0<ğ‘¢<1)=Â…ğ‘“(ğ‘¢)ğ‘‘ğ‘¢%=(3.5)  The ğ‘“(ğ‘¢) was a probability density function of a uniform distribution defined as follows:  ğ‘“(ğ‘¢)=w												1,ğ‘–ğ‘“	0<ğ‘¢<1								0,ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(3.6)  The data used to train the model were disyllabic vowel utterance and monosyllabic vowel utterance. For the disyllabic vowel utterance, the interpolation was performed twice by having the first and the second interpolated articulation as the first and the second syllable in a disyllabic vowel utterance. Duplicated samples were filtered out.   The speaker vocal tract models were simulated to increase the speaker variation in a speech signal. The predefined speaker articulation was calibrated from an actual MRI data, thus performing a perturbation could not ensure a realistic and natural speech signal produced from VTL. However, from the study [81], the formant was interpolated linearly with the growth from child to adult. Therefore, the linear interpolation between the predefined adult speaker vocal tract model and computed child speaker vocal tract model was used to simulate a speaker, as defined in equation 3.7.  ğ‘‰'#.>6;50,.>7=ğ‘—ğ‘‰,7?0.+(1âˆ’ğ‘—)ğ‘‰9@'07,ğ‘¢âˆˆ(0,1)(3.7)  The ğ‘‰'#.>6;50,.>7,	ğ‘‰,7?0., and ğ‘‰9@'07 were a vector of the three-dimensional point of the interpolated, adult, and child speaker respectively. The ğ‘— was a warping factor where ğ‘—âˆˆğ½ and ğ½={âˆ’0.3,âˆ’0.2,âˆ’0.1,0.0,0.1,0.2,0.3}. The choice of set ğ½ was empirically selected based on the realism and naturalness of generated data. With a 26  speaker warping factor ğ‘—>0.3 or ğ‘—<âˆ’0.3, some vowel in the boundary of vowel space was obstructed as a result of an abnormal articulatory parameter. The probability of uniform sampling ğ‘— was defined as follows:  ğ‘(ğ‘—)=ÂŠ							17ÂŒ																					,âˆ€ğ‘—âˆˆğ½																	0																						,ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(3.8)  Generated articulations were randomly assigned to each speaker as a one-to-one relationship, i.e., one articulation was related to one speaker. In contrast, each simulated speaker had a many-to-one relationship with the generated articulation, i.e., each speaker was related to many generated articulations. The articulatory parameter that was assigned to the simulated speaker was adjusted, according to the scale of the assigning simulated speaker, defined as follows:  ğ‘¦%B:=ğ‘¦:âˆ’min	(YÂB:)	maxBYÂB:Câˆ’min	(YÂB:)(3.9)  The term ğ‘¦: was an articulatory parameter	ğ‘˜, where  ğ‘˜âˆˆ{1,2,â€¦,24}. YÂB: was a simulated articulatory parameter ğ‘˜ of the simulated speaker ğ‘—. ğ‘¦%B: was a scaled articulatory parameter ğ‘˜ of the simulated speaker ğ‘—.  The speech variation was defined in a gestural score of the assigning generated articulation. Gestural attributes used in this study were: an acoustic duration, a time constant to reach the target articulatory, an F0 gesture, an F0 duration, and lung pressure. The parameter lip, tongue, and velic gesture were remained idle because these parameters corresponded to the consonant. The glottal shape was assigned as modal. The duration of a disyllabic vowel utterance was uniformly sampling from the range of ğ·7'âˆˆ[0.6,1.4] second, and ğ·D5#5âˆˆ[0.4,0.7] second for monosyllabic vowel utterance. The perturbation of a transition between the first and the second syllable of the disyllabic vowel utterance was added, where the transition time factor was uniformly sampling from the range of  ğ‘‡âˆˆ[0.45,0.55], defined as follows:  ğ‘¡ğ‘–ğ‘šğ‘’	ğ‘¡ğ‘œ	ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘‘ğ‘¡,ğ‘‘âˆˆğ·7'	ğ‘ğ‘›ğ‘‘	ğ‘¡âˆˆğ‘‡(3.10)  The duration was empirically chosen in which the duration was not too long or too short for both monosyllabic and disyllabic vowel utterance. The time constant was uniformly sampling from a range of ğ¶âˆˆ[0.015,0.020] to ensure the naturalness of synthesized speech. The glottal pressure was uniformly sampling from a range of ğºâˆˆ[9000,12000].  The realism and naturalness of the speech signal were handled by VTL 2.2. The application took the generated articulation and the gestural score to synthesized speech waveform. Some of the combinations between generated articulation and a simulated speaker obstructed a speech signal. This obstructed speech was filtered out if more than 10% of the speech was silent.    27   3.3   Recorded Data Collecting Method This study collected actual recorded data of a disyllabic vowel utterance from 12 native Thai speakers including six male and six female speakers without a report of a speech and hearing disorder. The author asked speakers to record speech using their available microphone in a room without a noticeable environment sound. The sample rate of a recorded speech was 44.1kHz with a 16-bits resolution. The speech data consisted of nine vowels which were: /a:/, /i:/, /u:/, /e:/, /É›:/, /É¯:/, /É¤:/, /o:/, and /É”:/. These vowels composed a disyllabic vowel utterance resulting in 81 disyllabic vowel utterances per recorded set. The author of this study recorded eight recorded sets and other speakers recorded one set per speaker. Thus, a total of 19 recorded sets with 1,539 disyllabic vowel utterances were recorded.    3.4   Data Preprocessing In this step, the data preprocessing overview of the speech and its articulatory was illustrated in Figure 3.5. The highlight operation indicated that it applied to a speech together with its articulatory parameters. The model was designed to take a speech feature representing a single syllable and its corresponding target articulatory. To transform speech signal into requiring speech feature, first, the dataset was partitioned into a training set, validating set, and testing set. These three sets were preprocessed similarly. Second, the training set was augmented and added to the original training set. Third, each speech and its articulation of a disyllabic vowel utterance dataset was separated by half into the first and the second monosyllabic vowel, which treated as individual data. Fourth, a speech signal was transformed into an MFCCs representation with 13 cepstral coefficients along with its velocity and its acceleration resulting in a total of 39 features per timeframe. The MFCC was computed using a sample rate of 16 kHz, Hanning window with a window length of 32ms, and frame step of 10ms. The discrete cosine transformation (DCT) type-II was used to decorrelate a filter bank frequency. Fifth, the MFCCs were normalized using a feature compensation method called cepstral mean and variance normalization (CMVN) [81]. The mean and variance were inferred from its distribution. The CMVN was done by z-scoring and scaling per coefficient was computed as follows:  ğ‘‹'B[ğ‘]=ğ‘‹'B[ğ‘]âˆ’ğ‘‹[ğ‘]Â˜Â˜Â˜Â˜Â˜Â˜ğ‘†ğ·(ğ‘‹[ğ‘])(3.11)  The mean ğ‘‹[ğ‘]Â˜Â˜Â˜Â˜Â˜Â˜	and standard deviation ğ‘†ğ·(ğ‘‹[ğ‘]) of ğ‘.@ coefficients were computed as follows:    ğ‘‹[ğ‘]Â˜Â˜Â˜Â˜Â˜Â˜=1ğ‘eeğ‘‹:D[ğ‘]DEFDE=:EG:E=(3.12)  ğ‘†ğ·(ğ‘‹[ğ‘])=Â™âˆ‘âˆ‘(ğ‘‹:B[ğ‘]âˆ’ğ‘‹[ğ‘]Â˜Â˜Â˜Â˜Â˜Â˜)DEFDE=:EG:E=ğ‘âˆ’1(3.13)  28  The ğ‘‹'B[ğ‘] was a ğ‘–.@ input feature of ğ‘.@ coefficient at a frame ğ‘—.@. ğ‘ was the total number of samples. ğ½ was the total number of timeframes. The mean and variance of a training set were computed from a distribution excluding a data augmentation distribution. Sixth, the speech representation was padded to equal a length using pre-padding with zero value benefitting a recurrent neural network [33].    
  Figure 3.5 The synthetic data pre-processing overview   The articulatory parameter was preprocessed to use as a target in supervised learning. The speakersâ€™ normalization was done by rescaling the articulation back to the predefined adult speakerâ€™s scale. Then, the min-max scaling was applied to the articulatory parameter, defined as follows:   ğ‘¦_ğ‘ ğ‘ğ‘ğ‘™ğ‘’B=ğ‘¦Bâˆ’min	(ğ‘¦B)	maxBğ‘¦BCâˆ’min	(ğ‘¦B)(3.14)  From the 24 articulatory parameters, the JX, VO, WC, TRX, TRY, MS1, MS2, and MS3 were removed and not being estimated by the AAI model. The articulatory parameters JX, MS1, MS2, and MS3 had small variance. The parameter VO has not affected the vowel and related only to the consonant. The parameter WC was newly introduced to the model and was not related to vowel speech production. The TRX and TRY parameters were dependent on the tongue body, TCX, and TCY. Thus, the post-computation using a deterministic method was used to ensure the naturalness of the tongue root position. Last, the data for model training was shuffled.   The preprocessed module learned and memorized the state of the speaker normalization. These memorized states were applied to other datasets during model inferencing, e.g., recorded data.  
29  3.5   Data Augmentation The data augmentation was applied during the model training stage. This experiment implemented a pre-computed data augmentation method by randomly selecting and augmenting 25% of the data in the training set for each augmentation method. The data augmentation methods were: 1) random noise injection; 2) volume perturbation; and 3) pitch shift. The random noise injection method randomly sampled a sequence of noise ğ´(ğ‘¡), in which each noise was randomly selected from a continuous range of  ğ‘âˆˆ(0.001,0.01) representing the ratio of the original value used as a noise. Given ğ‘‹(ğ‘¡) is an original speech signal at time ğ‘¡, defined as follows:  ğ‘‹,?H(ğ‘¡)=ğ‘‹(ğ‘¡)+	ğ´(ğ‘¡)ğ‘‹(ğ‘¡)(3.15)  The volume perturbation random a perturbation factor ğ›¼ from a continuous range where ğ›¼âˆˆ(1.5,3) representing the multiplying factor of its original volume. Although MFCCs representation is a pitch invariant, difference pitch still added small variation to the feature. The shift factor defined as ğ›½âˆˆ(âˆ’1.0,4) representing the semitone step to shift the pitch of the speech. The shift factor was randomly selected for each speech data.    3.6   Recorded Data Preprocessing The recorded data were preprocessed as illustrated in Figure 3.6. The highlight diagram indicates that the operation applied to speech together with its phonetic description. The recorded data from different speakers consisted of wide variations. The author was manually performed following transformation: 1) scaled its amplitude; 2) trimmed the silence section; 3) marked a syllabic transition. Other preprocessing operations were resembled the training data preprocessing. The speech was separated into the first and the second syllable using a transitional mark. Each separated speech was treated as individual data. The speech was transformed into an MFCC, its velocity, and its acceleration. The cepstral normalization was applied using a mean and variance of its dataset. This method eliminated the channel effected presented in the recorded speech from a different environment and a different microphone. The cepstral normalization was computed resembled Equation 3.11, 3.12, and 3.13. The length of a speech feature was post-padding or trimming based on a maximum length of a training dataset. This speech feature was used in both model inferencing and vowel classification model training. The phonetic description of recorded data was used as a label in a vowel classification model training. After the disyllabic separation, the one-hot encoding was applied. The encoding dictionary was used later to translate the prediction from a vowel classification model.  30  
  Figure 3.6 The recorded data pre-processing overview   3.7   Post-Processing Model The articulation from an AAI model was post-processed by rescaling and inferring a min-max parameter from a data preprocessing module, defined as follows:  ğ‘¦B=ğ‘¦IIâˆ—BmaxBğ‘¦BCâˆ’minBğ‘¦BCC+min	(ğ‘¦B)	(3.16)  ğ‘¦II was a predicting articulatory parameter ğ‘—. The maxBğ‘¦BC and minBğ‘¦BC parameters were from the training set. The parameter JX, VO, WC, TRX, TRY, MS1, MS2, and MS3 were added. JX and WC were filled with 0.0. MS1, MS2, and MS3 were filled with  -0.05. TRX and TRY were computed using the following equation:  ğ‘‡ğ‘…ğ‘‹=0.938âˆ—ğ‘‡ğ¶ğ‘Œâˆ’	5.1100	(3.17)  ğ‘‡ğ‘…ğ‘Œ=0.831âˆ—ğ‘‡ğ¶ğ‘‹âˆ’	3.0300	(3.18)  The equations were taken from a predefined speaker model, a JD2.speaker.    3.8   Model The study compared six different model architectures. Each had a different capability to learn a given problem, as discussed in Section 2.5. The model architectures were selected based on the existing studies with some adjustments. The first model was a simple multiple linear regression model, as a baseline model, as shown in Figure 3.7. The diagram was captured from the Keras model summary function. This model was used as a baseline model to compare the performance from other models. The second model was a fully-connected layer (FCNN) consisted of four hidden layers with 1,024 neural units and a fully-connected output layer, as shown in Figure 3.8. The third model was a recurrent neural network using an LSTM architecture consisted of five 
31  LSTM layers with 128 hidden units each, as shown in Figure 3.9. The fourth model was a bi-directional LSTM recurrent neural network (BiLSTM) having similar setup to LSTM architecture, as shown in Figure 3.10. The fifth model was an LTRCNN proposed in the study [72]. The last model was a convolutional layer connected to the three bidirectional LSTM recurrent layer (Conv-BiLSTM), as shown in Figure 3.11. The convolutional layer was a 1-dimensional convolutional layer consisted of 128 filters per layer, a kernel size of 1 by 3, and a zero-padding for a featureâ€™s size consistency. The dropout with a 50% drop rate was applied in all models except for a simple multiple regression model. The batch normalization was applied in a convolutional layer. The ReLU activation function was applied to the fully-connected layer and convolutional layer. No activation function was applied to an output layer.      Figure 3.7 Simple multiple regression model    Figure 3.8 FCNN model    Figure 3.9 LSTM-RNN model  
32  
  Figure 3.10 BiLSTM-RNN model    Figure 3.11 Conv-BiLSTM model   The evaluator model was a shallow LSTM recurrent network consisted of two LSTM layers and 64 hidden units per layer. The dropout with a 50% drop rate was applied. The output layer was a fully-connected layer with nine units representing the phonetic target class. The model was illustrated in Figure 3.12. The hyperparameter of all models was selected using an empirical study.     Figure 3.12 Vowel classification model   3.9   Model Training and Experimental Setup  3.9.1 Synthetic AAI Training Dataset Exploration The generated dataset consisted of a speech and its target articulation. This experiment generated a dataset, having a size of 41,000 samples. The audio sample rate was specified to 16 kHz with 16 bits resolution. The predefined articulatory were uniformly sampling for interpolation, as illustrated in Figure 3.13.   Figure 3.14 illustrates an interpolate target articulatory parameter by uniform manifold approximation and projection (UMAP) [83] for dimensional reduction. The phonetic 
33  label was based on the starting interpolated point. The target articulatory had a clear separation because of the defined constraint. Thus, each vowel had a clearly defined articulatory space. The bottom left clusters an articulation that had a lip wide open, e.g., /a:/ and /É›:/. The top right clusters an articulation that had a lip close, e.g., /u:/ and /É”:/. The articulation toward the top right had a lip round and a tongue towards the back, and vice versa.  
  Figure 3.13 Distribution of the initial interpolated phonetic description   
  Figure 3.14 The interpolated target articulatory plot using UMAP   The speaker model was simulated with an interpolated ratio of 30%, 20%, and 10% increased and decreased, as shown in Figure 3.15. Highlight speaker models were the model using for generating speech signals in this experiment.   The disyllable speech signal in the time domain and a spectrogram was shown in Figure 3.16. The transition from /y:/ to /É”:/ was shown in the spectrogram, where the effect of perseverative coarticulation as the speech from /y:/ was carried over to the 
34  speech /É”:/. The /y:/ had an anticipatory coarticulation where the articulatory was in place before producing a speech.  
  Figure 3.15 The midsagittal area of speaker vocal tract model   
  Figure 3.16  The sample disyllable speech signal and its spectrogram (left), and a The MFCC (top right) and its normalization MFCC (bottom right) of a generated disyllable speech   Figure 3.17 shows the histogram of each articulatory parameter. From the figure, JX, VO, WC, and MS1 to MS3 had a small variance. This caused by an irrelevant of an articulatory in a vowel speech production, which parameter in most of the predefined articulatory parameters were the same.  3.9.2 AAI Model Training The AAI model was trained by supervised learning using the gradient-based optimization with backpropagation to minimize the model's parameter corresponding to a mean square error (MSE) predicted and target articulatory parameter, defined as follows:   
35  ğ‘€ğ‘†ğ¸=	âˆ‘âˆ‘Bğ‘¦:Bâˆ’ğ‘¦%	:BC+DBE%#:E%ğ‘›âˆ—ğ‘š(3.18)  The ğ‘¦:B and ğ‘¦%	:B were a target and an estimation of articulatory parameter position j at data-point k sequentially. ğ‘š was the total number of a parameter. ğ‘› was the total amount of data.    
  Figure 3.17 The distribution of each articulatory parameter   This study evaluated six different neural network architectures, defined in section 3.8. The learning hyperparameter was a controlling factor, where a learning rate was 0.001, the batch size was 64. The Adam optimizer was used with a ğ›½% and ğ›½+ of 0.9 and 0.999 respectively. The model was trained with 150 epochs.   The training dataset was divided into 70% of training data as a training set, 15% as a validating set, and another 15% as a testing set. The validating set was used for an early stop mechanism to monitor the model performance. The testing set was used to evaluate model performance.   3.9.3 Evaluator Model Dataset The dataset to train the evaluator model was the recorded dataset as described in Section 3.3. Figure 3.18 illustrates the recorded speech feature space using UMAP to reduce the dimension of the MFCC and its velocity and acceleration. The /i:/, /e:/, /É›:/, /a:/, and /É”:/ were clearly separated from others. Both /u://o:/ and /É¤://É¯:/ were 
36  clustered together but still separable. /É”:/ and /u://o:/ were form one big cluster, and /i:/, /e:/. /É¤://É¯:/ were formed another big cluster. The cluster /É”://u://o:/ was a speech produced from the round lip, and /i://e:/ /É¤://É¯:/ was a speech produced from wide to middle mouth open without a round lip. The mixture occurred because of speaker variations presenting in speech features. These variations were: 1) an individual speaker; 2) gender; 3) the surrounding environment; and 4) recorded channel.   
  Figure 3.18 Recorded MFCCs feature plot using UMAP dimensional reduction   3.9.4 Evaluator Model Training The evaluator model was trained with a supervised classification learning task by predicting a vowel of a given speech feature. The training dataset was the recorded data, as described in section 3.3. The cross-entropy loss was used as an objective function to train this model, defined as follows:  ğ¿(ğ‘§Ì‚,ğ‘§	)=	âˆ’eğ‘§ğ‘™ğ‘œğ‘”(ğ‘§Ì‚)J9E%(3.19)  ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘)=	ğ‘’,$âˆ‘ğ‘’,%J7E%(3.20)  ğ‘§Ì‚ was a predicted probability producing from a ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘) function. ğ¶ was the total number of classes. ğ‘ was an activation from the previous layer. ğ‘§ was a ground truth of a predicted class.   The dataset had a small size. Thus, the permutation test and a bootstrapping subsampling method were applied. The permutation test was used to test the model performance whether the evaluated performance was not by chance. The training data was split into 80% as a training set and another 20% as a testing set. These two sets were permuted and used to train the model. The bootstrapping method was used to compute the confidence interval of the modelâ€™s performance.  
37  The learning hyperparameter of the model was specified as follows: the batch size is 128 with 200 epochs, the Adam optimizer was used with a hyperparameter resembling the AAI model, and the early stop mechanism was applied by monitoring the testing set. The subsampling was performed 100 times for both the permutation test and the bootstrapping method.   The evaluator modelâ€™s performance was evaluated using an area under the curve of receiver operating characteristics (ROC-AUC). The ROC was a probability curve between the true positive rate and the false-positive rate of the classification model, as illustrated in Figure 3.19. The true positive rate was a rate of a sample that the model correctly classified as a positive from all the target positive samples. The false-positive rate was a rate of sample incorrectly classified as positive by the model over all the target negative samples.   3.10   Design of Experiment Six deep learning model architectures were used to invert the speech from the actual recorded data. The inversion result was evaluated using the evaluator model in terms of average precision, average recall, and ROC-AUC. The objective of this experiment was to compare the generalization performance of each model architecture.   An effect of a data augmentation was studied. The objective of this experiment is to compare the effect of the proposed data augmentation method, e.g., speaker simulation and data augmentation during the data preprocessing. The best-performed model selected from 3.10.1 was used to train on four different datasets. These datasets were: 1) the dataset as previously described; 2) the dataset without speaker simulation; 3) the dataset without data augmentation; 4) the dataset without both speaker simulation and data augmentation. The evaluation was similar to 3.10.1 where the evaluator evaluated each model using average precision and ROC-AUC as metrics.  An effect of a coarticulation was studied. The objective of this experiment is to test the effect of a coarticulation during target articulatory inversion. The same model architecture in 3.10.2 was used to train on the monosyllabic vowel utterance and disyllabic vowel utterance. The model trained with a monosyllabic vowel was assumed to not consider the coarticulation during inferencing because the coarticulation feature was not learned by the model. Similar to 3.10.1 and 3.10.2, the output was evaluated using the evaluator model by comparing average precision and ROC-AUC.  The effect of gender and loudness in speech features were studied separately. The AAI model used in this experiment was the same model, which was the best-performed model from 3.10.1. The gender effect was an experiment by performing AAI inversion with the recorded speech of Thai vowels, which separated into male speaker speech and female speaker speech. The loudness effect was an experiment by performing AAI inversion with the recorded speech of Thai vowels from one speaker where the speaker spoke normally, and another was noticeably loud. The output was evaluated using the evaluator model by comparing average precision and ROC-AUC.  38  
  Figure 3.19 ROC-AUC  During the AAI model training, the root means square error (RMSE) was used to evaluate the distance error in Euclidean space between target and estimated articulatory parameters. The R-squared (R2) was used to measure how well the model explained the given data comparing to the used of average value, defined as follow:  ğ‘…ğ‘€ğ‘†ğ¸=	Â™1ğ‘›(ğ‘¦'âˆ’ğ‘¦%	')+(3.21)  ğ‘…+=1âˆ’âˆ‘(ğ‘¦B:âˆ’ğ‘¦%	B:)+#:E%âˆ‘(ğ‘¦B:âˆ’ğ‘¦Â˜	B:)+#:E%		(3.22)  The variable  ğ‘¦' was an actual label, the ğ‘¦%' is a predicted label. ğ‘¦B: was an actual articulatory parameter position j over the data point k. ğ‘¦Â˜	B: was a mean of an actual articulatory parameter position j over the data point k.  The interesting outcome of the model was an inversion speech produced from estimated articulation. The mean absolute percentage formant error (MAPE) between the inversion model and the synthesized speech from a predefined dataset was used as an evaluation metric, as shown in Equation 3.23. The formant of each syllable was computed by trimming 25% upper range and lower range to exclude the effected from a transition which caused an inaccurate formant measurement. This study compared F1, F2, and F3 formants. The F1 and F2 were a primary focus because it directly related to a vowel, where the F3 was related to a consonant. The formant was extracted using a Praat script [84].   ğ‘€ğ´ğ‘ƒğ¸=âˆ‘Â¤Â¥ğ¹'âˆ’ğ¹,ğ¹,	Â¥ğ‘¥100Â§ğ‘	(3.23)  The ğ¹' was a formant of an inversion speech. ğ¹, was a formant of an actual speech. ğ‘ was a total amount of a formant sample in the speech data. The model produced the least error on a predefined dataset was selected to further evaluated the performance numerically, visually, and perceptually using both predefined datasets and recorded datasets.  
39   The evaluator model was used to evaluate the inverted speech from the AAI model. That was because formants from different speakers were different. Thus, it could not be directly compared. The precision and ROC-AUC metrics were used as a numerical score. The precision metric showed an accuracy of the vowel identification model, defined as follows:  ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=ğ‘‡ğ‘Ÿğ‘¢ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘‡ğ‘Ÿğ‘¢ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’+ğ¹ğ‘ğ‘™ğ‘ ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’	(3.24)  The ğ‘‡ğ‘Ÿğ‘¢ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ was a number of modelâ€™s prediction that correctly identified. The  ğ¹ğ‘ğ‘™ğ‘ ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ was a number of modelâ€™s prediction that incorrectly identified. In this experiment, the precision metric indicates that the inversion speech from the AAI model was recognizable or not, as shown in Figure 3.20. First, the AAI model estimated target articulatory from the recorded data used to synthesize speech by the VTL. These synthetic speeches were fed to the evaluator model to predict its vowel. The study assumed the evaluator was perfectly-identified in the Thai vowel from a given speech. Thus, if the AAI model estimated an inaccurate inversion speech from a recorded speech, the evaluator would identify the vowel phonetic description in which mismatch the actual vowel phonetic description of that recorded speech. This will be resulting in reducing the AAI model performance score.    
  Figure 3.20  The AAI model evaluation schema  (a) is when the AAI model is correctly inverted speech  (b) is when the AAI model is incorrectly inverted speech 
  CHAPTER 4 RESULTS AND DISCUSSION   This chapter demonstrates and discusses the results of the experiment proposed in Chapter 3. The first section shows and discusses the results from the model comparison on both monosyllabic and disyllabic vowel utterance using a synthetic dataset. The following section shows and discusses the model evaluation results from the best-performed model, which is selected from the previous section. The model is used on both the predefined dataset and the recorded dataset. For the last section, the result from evaluator model training is shown, and the evaluator model is applied to evaluate the result from the proposed experiments described in Section 3.10.   4.1   Model Comparison Table 4.1 shows the result from six AAI model architectures trained with a synthetic monosyllabic vowel dataset. The result showed that the BiLSTM-RNN model achieved the best performance on both the training set and testing set. The training set achieved 0.117 RMSE and 0.94 R2. The testing set achieved 0.139 RMSE and 0.92 R2. Its R2 performance was 1.722 times better than the baseline and 1.15 times better than FC-NN, even though the FC-NN had 5,135,377 parameters which were 2.93 times more than BiLSTM-RNN. Thus, the ability to extract the sequential characteristic in BiLSTM helped to boost up the performance. The BiLSTM-RNN performed slightly better in terms of RMSE and R2 than the LSTM, LTRCNN, and Conv-BiLSTM. However, the performance of Conv-BiLSTM was slightly better than BiLSTM when applying with the predefined dataset, as shown in Table 4.2. The performance of an LTRCNN was dropped when applying with the predefined dataset.   Table 4.1 The performance of models trained with a monosyllabic vowel utterance  Model Train RMSE Train R2 Test RMSE Test R2 Baseline 0.3441 0.5117 0.3348 0.5344 FC-NN 0.2308 0.7844 0.2224 0.8005 LSTM-RNN 0.1356 0.9248 0.1450 0.9130 BiLSTM-RNN 0.1172 0.9440 0.1385 0.9202 LTRCNN 0.1469 0.9109 0.1468 0.9106 Conv-BiLSTM 0.1330 0.9282 0.1405 0.9191   The different articulations resulted in a similar speech. Thus, the F1, F2, and F3 formants were extracted and compared between inverted speech from the AAI model and an original synthetic speech from a monosyllabic predefined dataset. These formants could be compared because both data were synthesized from the same speaker vocal tract model. Table 4.3 shows the result of an absolute percentage formant error with a 95% confidence interval. The BiLSTM-RNN achieved the lowest error in all the formants, following by the Conv-BiLSTM. The LSTM-RNN and 41  LTRCNN produced a similar error followed by FC-NN. The baseline produced the worse error.    Table 4.2 The model performance on predefined monosyllabic vowel data  Model RMSE R2 Baseline 0.3857 0.5923 FC-NN 0.2993 0.7268 LSTM-RNN 0.2020 0.8500 BiLSTM-RNN 0.1968 0.8564 LTRCNN 0.2411 0.7906 Conv-BiLSTM 0.1942 0.8581   Table 4.3 The formant MAPE of a model on predefined monosyllabic vowel dataset    F1 Error (%) F2 Error (%) F3 Error (%)  Baseline 77.38 Â± 16.03 151.50 Â± 27.09 161.87 Â±31.03 FC-NN 13.14 Â± 1.08 29.27 Â± 2.76 48.76 Â± 7.04 LSTM-RNN 9.57 Â± 1.04 15.03 Â± 1.95 24.98 Â± 4.35 BiLSTM-RNN 7.90 Â± 0.62 7.60 Â± 0.55 14.21 Â± 1.90 LTRCNN 9.73 Â± 1.05 14.01 Â± 1.81 18.72 Â± 2.68 Conv-BiLSTM 8.45 Â± 0.83 15.13 Â± 1.70 19.84 Â± 2.30   Table 4.4 shows the result of a model trained with the disyllabic vowel. The result showed that adding the coarticulation effect reduced the overall performance of all the models. That was because the coarticulation effect increased the variation in a dataset, which affected the model to learn more complicated speech patterns. The BiLSTM-RNN achieved the best performance on the training set with 0.12 RMSE and 0.94 R2, while both BiLSTM-RNN and Conv-BiLSTM achieved the best performance on the testing set with around 0.14 RMSE and 0.91 R2. Both R2 performances were around 1.82 times better than baseline and around 1.14 times better than FC-NN. Numerically, the Conv-BiLSTM slightly performed better than BiLSTM-RNN, but this was likely due to the chance from the early stop and initialization. Resembling the model on a monosyllabic vowel data result, the LSTM-RNN and LTRCNN achieved similar performance, following by FC-NN.  Table 4.5 shows that LTRCNN achieved the best performance in terms of both RMSE and average R2, following closely by Conv-BiLSTM. However, Table 4.6 shows that BiLSTM-RNN achieved the best performance in terms of an outcome by having the lowest MAPE for an F1 error which related directly to vowel speech production. This result showed that while LTRCNN more fitted the data, the F1 error in /i:/ and /o:/ produced by LTRCNN is significantly higher than /i:/ and /o:/ produced by BiLSTM-RNN with F1 MAPE of 10.36% and 4.89% for /i:/ and /o:/ sequentially. BiLSTM-RNN produced 3.31% and 2.79% F1 MAPE for /i:/ and /o:/ sequentially. The different 42  articulatory could produce a similar sound, while the estimated target articulatory from BiLSTM-RNN did not align with the original synthetic target articulatory, it produced the resembled speech. Comparing to the error by using the t-statistical test, the study found that the error produced by LTRCNN and BiLSTM was not statistically significant indicated by p-values of 0.444 with 0.005 alpha.   Table 4.4 The performance models trained with the disyllabic vowel utterance Model Train RMSE Train R2 Test RMSE Test R2 Baseline 0.3575 0.4795 0.3462 0.5048 FC-NN 0.2312 0.7835 0.2228 0.7980 LSTM-RNN 0.1404 0.9196 0.1505 0.9056 BiLSTM-RNN 0.1220 0.9395 0.1428 0.9147 LTRCNN 0.1441 0.9143 0.1464 0.9098 Conv-BiLSTM 0.1410 0.9181 0.1420 0.9157   Table 4.5 The model performance on predefined disyllabic vowel data  Model RMSE R2 Baseline 0.4043 0.5381 FC-NN 0.2568 0.8096 LSTM-RNN 0.1616 0.9013 BiLSTM-RNN 0.1434 0.9258 LTRCNN 0.1197 0.9594 Conv-BiLSTM 0.1229 0.9556   Table 4.6 The formant MAPE of a model on  predefined disyllabic vowel data    F1 Error (%) F2 Error (%) F3 Error (%)  Baseline 119.23 Â± 4.55 171.34 Â± 5.99 226.90 Â± 8.22 FC-NN 8.77 Â± 0.20 38.70 Â± 0.84 51.71 Â± 1.46 LSTM-RNN 3.02 Â± 0.06 12.49 Â± 0.26 12.19 Â± 0.34 BiLSTM-RNN 2.73 Â± 0.04 7.74 Â± 0.19 8.38 Â± 0.15 LTRCNN 3.28 Â± 0.06 5.53 Â± 0.11 15.68 Â± 0.52 Conv-BiLSTM 3.33 Â± 0.08 8.43 Â± 0.21 12.23 Â± 0.26   From the result, the BiLSTM-RNN achieved the best performance on both monosyllabic vowel and disyllabic vowel datasets in terms of inversion speech. The backward direction in BiLSTM-RNN boosted the model performance comparing to the LSTM-RNN which has only forward direction. This indicated the influence of the future speech feature affected the articulatory estimation. By comparing LTRCNN and 43  Conv-BiLSTM, the study found that the skip connection in LTRCNN did not significantly increase the performance. The convolutional layer helped the model performance when more variation was presented in the dataset. However, the BiLSTM-RNN produced more resembling inverted speech than the model with a convolutional layer. The result from the BiLSTM-RNN was further analyzed by applying to the predefined dataset and the recorded dataset in the following section.   4.2   Model Performance on The Predefined Dataset This section shows the inversion result from the BiLSTM-RNN with monosyllabic vowel utterances and disyllabic vowel utterances in a predefined dataset.    4.2.1 Model performance on a monosyllabic vowel utterance Table 4.7 shows the RMSE and R2 of normalized articulatory parameters estimated by BiLSTM-RNN. The result showed that the BiLSTM-RNN was noticeably weak in estimating TTX and TS1. The error in estimating TTX caused the error in an inversion speech as the tongue tip was the main articulator related to speech production. Table 4.8 shows the absolute percentage formant error of each monosyllabic vowel utterance.   From Table 4.8, the /i:/, /u:/, /o:/, and /Ã¸:/ produced from BiLSTM-RNN had a high F1 error comparing to other vowels. The F1-F2 formant plot compared the original synthetic and inverted speech signal, as shown in Figure 4.1. Similar to the numerical result, /i:/, /u:/, /o:/, and /Ã¸:/ vowels of the inverted speech were slightly distanced from the original synthetic speech.  However, these errors were not significantly different using the t-statistical test comparing to inverted speechâ€™s formant and original synthetic speechâ€™s formant, as shown in Table 4.9. The t-statistical test indicated that F1, F2, and F3 formants of both speech data were not significantly different. Figure 4.2 shows the resembled in spectrogram between the original synthetic speech signal and the inversion speech signal.    44  Table 4.7 The RMSE and R2 of the estimated articulatory parameter from     BiLSTM-RNN on a predefined monosyllabic vowel data  Articulatory Parameter RMSE R2 Articulatory Parameter RMSE R2 HX 0.153 0.950 TTX 0.385 0.645 HY 0.260 0.767 TTY 0.281 0.775 JA 0.124 0.931 TBX 0.144 0.923 LP 0.236 0.853 TBY 0.292 0.756 LD 0.193 0.936 TS1 0.473 0.514 VS 0.171 0.945 TS2 0.173 0.882 VO 0.243 0.913 TS3 0.180 0.905 TCX 0.138 0.958 TS4 0.094 0.949 TCY 0.141 0.957      Table 4.8 Relative formant error of predefined monosyllabic vowel utterance Phonetic F1 Error (%) F2 Error (%) F3 Error (%)  /a:/ 1.41 1.64 4.38 /i:/ 15.47 1.79 5.72 /u:/ 17.38 4.19 1.09 /e:/ 5.93 0.06 1.24 /É›:/ 8.20 5.18 1.32 /o:/ 12.62 2.82 2.67 /É™:/ 0.67 3.17 0.41 /Å“:/ 8.81 1.32 0.64 /É”:/ 2.16 5.72 1.77 /É‘:/ 0.54 0.17 0.50 /Ã¸:/ 14.70 2.80 2.81 /ÊŠ:/ 6.86 2.80 0.67   45  
  Figure 4.1 Formant plot comparing original synthetic and inversion speech signal of   a predefined monosyllabic vowel utterance   Figure 4.3 shows the speaker articulation visualizing from VTL. The result showed that the articulation was in the position according to the international phonetic alphabet (IPA) chart. The /a:/ vowel had tongue towards front and month open. The /i:/ vowel had tongue towards the front and mouth slightly close. The /u:/ had tongue towards back and mouth slightly close.   Table 4.9 Statistical comparison between the original synthetic and the inversion speech formant of a monosyllabic vowel utterance   F1 F2 F3 t-test 0.151 -0.059 -0.088 p-value 0.882 0.954 0.930   
  Figure 4.2 Spectrogram comparing original synthetic (top) and inversion speech     signal (bottom) of a monosyllabic vowel utterance, /u:/ (right) and /i:/     (left)   
46  
  Figure 4.3 Speaker articulation of /a:/ (left), /i:/ (center), and /u:/ (right) vowel   4.2.2 Model performance on a disyllabic vowel utterance Table 4.10 shows the RMSE and R2 of an average estimated articulatory parameter from a BiLSTM-RNN. Similar to the result from the previous section, the TS1 and TTX error were higher than others. Table 4.11 shows average F1, F2, and F3 formant error separating the first and the second syllables in a disyllabic vowel utterance. The /u:/ vowel produced from the model had the highest F2 error on both the first and the second syllables, which visually shown in Figure 4.4 that the inversion speech was distanced from the original synthetic speech. The t-statistical test in Table 4.12 shows that the inversion speech and original synthetic speech were not significantly different by having p-value more than the alpha threshold of 0.05 on all F1, F2, and F3.  Table 4.10 The RMSE and R2 of the estimated articulatory parameter from      BiLSTM-RNN on a predefined disyllabic vowel data  Articulatory  Parameter RMSE R2 Articulatory  Parameter RMSE R2 HX 0.152 0.950 TTX 0.259 0.840 HY 0.158 0.914 TTY 0.192 0.895 JA 0.109 0.946 TBX 0.136 0.931 LP 0.178 0.916 TBY 0.166 0.922 LD 0.139 0.966 TS1 0.290 0.817 VS 0.165 0.948 TS2 0.156 0.904 VO 0.176 0.954 TS3 0.148 0.936 TCX 0.083 0.985 TS4 0.109 0.931 TCY 0.091 0.982      
47  Table 4.11 Relative formant error of predefined disyllabic vowel utterance Label First syllable Second syllable F1 Error (%) F2 Error (%) F3 Error (%) F1 Error (%) F2 Error (%) F3 Error (%) /a:/ 1.79 2.11 2.45 1.66 1.94 2.10 /i:/ 3.99 0.69 1.40 2.63 0.41 1.41 /u:/ 2.95 15.46 1.15 4.27 11.10 1.58 /e:/ 2.30 0.15 0.32 1.75 0.36 1.51 /É›:/ 3.10 1.47 1.84 3.91 3.05 1.81 /o:/ 3.85 2.31 0.75 4.28 2.38 1.13 /É™:/ 0.55 0.49 0.37 0.92 1.06 0.17 /Å“:/ 2.34 6.49 1.74 3.24 5.62 2.21 /É”:/ 1.67 4.87 0.40 1.22 1.38 0.75 /É‘:/ 0.37 1.10 0.91 0.98 2.40 0.44 /Ã¸:/ 5.59 0.47 2.08 5.37 0.81 1.44 /ÊŠ:/ 1.49 4.70 1.39 5.40 7.98 1.35   
  Figure 4.4 Formant plot comparing original synthetic and inversion speech signal of    a disyllabic vowel utterance 
48  Table 4.12 Statistical comparison between the original synthetic and the inversion speech formant of a disyllabic vowel utterance    First syllable Second syllable F1 F2 F3 F1 F2 F3 t-test -0.027 0.029 -0.169 0.023 0.080 -0.248 p-value 0.978 0.977 0.867 0.981 0.937 0.806   Figure 4.5 visually compares the spectrogram of original synthetic speech and inverted speech from BiLSTM-RNN, which showed that both speeches resembled together. Figure 4.5 visualizes the average estimated articulation from each vowel from the BiLSTM-RNN. Similar to the result reported in 4.2.1, the average articulation of each vowel was according to the IPA chart.  From the overall result, the model could estimate the articulation from an original synthetic speech which the same speaker used in model training. The next section applied the BiLSTM-RNN trained on the synthetic disyllabic vowel utterance dataset to test the generalization of the model on a different distribution by applying to the recorded dataset of a Thai vowel.     Figure 4.5 Spectrogram comparing original synthetic and resynthesized speech signal of a disyllabic vowel utterance   4.3   Model Performance on The Recorded Dataset Table 4.13 shows the mean absolute percentage formant error between the actual speech and the inversion speech. The F1, F2, and F3 formant of original speech were computed by averaging the formant from a speech produced by a different speaker. The formant from the different speakers could not be compared because the formant was affected by vocal tract anatomy. However, the result showed that the F1, F2, and F3 formants between the actual speech and the inversion speech were not far from each other, as illustrated in Figure 4.7. The formant of the actual speech and the inverted speech were arranged in a similar formation. Table 4.14 shows that the F1 and F2 were not significantly different, while F3 was significantly different indicated by having a p-value less than 0.05.   
49  
  Figure 4.6 Average speaker articulation of each vowel according to the IPA chart of    a disyllabic vowel utterance   Table 4.13 Mean absolute percentage formant error of the first and second vowel   between actual speech and inversion speech  Label First syllable Second syllable F1 Error (%) F2 Error (%) F3 Error (%) F1 Error (%) F2 Error (%) F3 Error (%) /a:/ 25.45 15.20 17.05 26.86 17.35 17.06 /i:/ 18.78 27.56 14.71 31.11 32.97 14.57 /u:/ 16.82 32.54 10.96 21.79 36.91 13.96 /e:/ 19.56 21.36 15.99 22.53 21.47 15.76 /É›:/ 18.48 28.97 16.13 24.56 26.75 17.15 /É¯:/ 12.79 12.73 16.06 17.55 15.72 16.28 /É¤:/ 14.05 11.58 15.98 21.42 13.20 16.23 /o:/ 21.36 25.64 12.25 23.65 22.90 12.16 /É”:/ 15.54 20.82 15.09 23.32 21.51 15.69   Table 4.14 Statistical comparison between the actual and the inversion speech formant of a disyllabic vowel utterance    First syllable Second syllable F1 F2 F3 F1 F2 F3 t-test -0.718 -0.506 -5.253 -0.847 -0.708 -3.765 p-value 0.483 0.620 0.000 0.410 0.489 0.002  
50  
  Figure 4.7 Average formant plot comparing actual and inversion speech signal of a disyllabic vowel utterance   Figure 4.8 shows the formant comparison between the inverted speech and the empirical formant range [85]. The formant of the inverted speech was computed by averaging formant of the vowel in the disyllabic vowel separated by the first and the second syllable position. The plot shows that most syllables were comparable with the empirical range indicated the effectiveness of the model to estimate articulatory from recorded speech features.   
51  
   Figure 4.8 F1 and F2 Comparison between inversion speech and the empirical formant   range [85]   Figure 4.9 shows the spectrograms sample from the corpus by comparing the actual speech and inversion speech. The red contour shows the speech formant. Visually, the F1 and F2 formant in both speeches were resembled, while F3 formant in some samples was noticeably different, e.g., /É›:É”:/ and /É¯:o:/. Figure 4.10 shows the dimensional reduction plot of an estimated target articulation using UMAP. The plot shows an articulation group labeling by its phonetic description. While errors were presented, the articulation of the same vowel was clustered together. The /É”:/ and /É¤:/ had multiple clusters by having their cluster and grouping with others. The /É”:/ was grouped with /a:/ but not mixed which indicated that the speech produced by /É”:/ were analogous to /É”:/. Similarly, some of the /É¤:/ was grouped with /É¯:/ but distinctions were presented which indicated that the speech produced by these /É¤:/ were analogous to /É¯:/. Lastly, /u:/ and /o:/ were grouped and mixed in the middle, while the other end showed the clear distinction between vowels.  Figure 4.11 shows the average articulation of the first and the second syllables of an estimated target articulatory inverted from a recorded dataset by BiLSTM-RNN. The position of a tongue and a mouth of each vowel were according to the Thai phonetic chart [86]. The top right shows the articulation with tongue towards back and mouth close, and the bottom left shows the articulation with the tongue forward and mouth open. Figure 4.12 shows a lipâ€™s shape of the /a:/, /i:/, and /u:/ vowel to visualize the roundedness. The lipâ€™s shape of /u:/ was rounded, while /a:/ and /i:/ are unrounded.  
52  
  Figure 4.9 Spectrogram comparing actual and inversion disyllabic vowel speech. The red line is a formant contour, and the blue line is a pitch contour   
  Figure 4.10 UMAP plot of the estimated articulation from actual speech    
53  
  Figure 4.11 Average speaker articulation of each vowel in a first syllable (left) and second syllable (right) of a disyllabic vowel utterance     Figure 4.12 Speaker articulation of /a:/, /i:/, and /u:/   Visual and numerical results indicated that the model could invert the actual recorded speech from a Thai speaker. This study performed a further analysis by simulated the perception test using the evaluator model to identify a vowel from an inversion speech from the AAI model.   4.4   Evaluation using Vowel Identification Model In this section, the evaluator modelâ€™s performance was reported. Next, the evaluator model was used to evaluate the output from the design experiment proposed in 3.10.   4.4.1 Evaluator Modelâ€™s Performance The model was trained by using the bootstrapping with 100 subsampling. The average precision, recall, and AUC from the out-of-bag set were 0.963, 0.964, and 0.998, sequentially. The lower limited and upper limit of a confidence interval of the ROC-AUC were 0.995 and 0.987 sequentially. From the permutation test with 100 subsampling resulting in p-value closed to zero which indicated that the model 
54  performed better than chance with the alpha threshold of 0.05. Table 4.15 shows an average precision and recall from a bootstrapping. The result showed that the model could identify the vowel from a given speech feature accurately. Thus, the assumption that the model can perfectly identify the Thai vowel in this experiment was valid. Therefore, the evaluator model was used to evaluate the inversion result from the AAI model.      Table 4.15 The average precision and recall of the evaluator model  Targets Precision a: 0.99 i: 0.93 u: 0.99 e: 0.91 É›: 0.95 É¯: 0.98 É¤: 0.99 o: 0.96 É”: 0.97   4.4.2 AAI Model Comparison Table 4.16 shows the identification result of an inversion speech produced by different AAI models using the evaluator model. The BiLSTM-RNN produced an inversion speech that had the most correctly identified by the evaluator. The model achieved 0.826 precision and 0.822 recall. Ranking by the ROC-AUC, the second-best performed models were LTRCNN and Conv-BiLSTM. The inversion result from a BiLSTM-RNN was outperformed LTRCNN and Conv-BiLSTM by having an 8% higher average precision than inversion result from LTRCNN and Conv-BiLSTM. Thus, the property of a convolutional layer did not improve inversion performance. This was because the MFCCs feature already summarized speech signals using a sliding window. Thus, the convolutional layer could further summarise the speech feature resulting in information loss. The result comparing between LSTM-RNN and BiLSTM-RNN also confirmed that the future speech future helped to estimate the target articulatory, thus the BiLSTM-RNN performed better inversion than LSTM. Considering that the baseline model was a simple multiple regression model, the performance was higher than expected. Assuming that the AAI model could produce only neutral vowel /É¤:/, the expected precision would be 11%.    4.4.3 Data Augmentation Effect  Table 4.17 shows the identification result of an inverted speech produced by the BiLSTM-RNN model trained with different datasets. These datasets were the training dataset as described in Chapter 3. The data-augmentation was referred to processes in section 3.5. The result showed that both speaker simulation and data-augmentation improved inversion performance by having 23.9% average precision more than the 55  model trained without both. The AAI model trained with only data-augmentation performed better than the AAI model with only speaker simulation by having 3.1% average precision more. Thus, the proposed speaker simulation and data-augmentation method did improve inversion performance.   Table 4.16 The identification rate of the inversion speech result from different models  Inversion Speech from The Model Avg. Precision Baseline 0.466 FCNN 0.617 LSTM-RNN 0.749 BiLSTM-RNN 0.826 LTRCNN 0.7429 Conv-BiLSTM 0.747   Table 4.17 The identification rate of the inversion speech result from different datasets  Inversion Speech from The Model Avg. Precision BiLSTM-RNN 0.826      BiLSTM without speaker simulation 0.714 BiLSTM without data augmentation 0.683 BiLSTM without both speaker simulation and data augmentation 0.587   4.4.4 Coarticulation Effect  Table 4.18 shows the inversion result from the AAI BiLSTM-RNN model trained with a presented of coarticulation effect from a disyllabic vowel dataset, and without a presented of coarticulation effect from a monosyllabic vowel dataset. The result showed that the AAI model trained with coarticulation effected performed better than the AAI model trained without coarticulation effected by 5.3% average precision. Thus, the AAI model did consider the coarticulation effected to estimate the target articulatory of a given speech.     56  Table 4.18 Compare model trained only on a monosyllable data with a model with disyllable data  Inversion Speech from The Model Precision Disyllabic Vowel 0.826 Monosyllabic Vowel 0.773   4.4.5 Speakerâ€™s Gender Effect Table 4.19 shows the identification rate of an inversion speech of the female and male speaker recorded speech. The inversion speech from a male speaker speech was more recognizable than the inversion speech from female speaker speech. That was because the model was trained on male speakers, which the speech characteristic from male speakers had been learned by the AAI model. However, the AAI model could correctly inverted speech from a female speaker showing that the proposed system could generalized regardless of gender.    Table 4.19 Compare model prediction performance on female and male speaker disyllabic vowel speech data  Inversion Speech from The Model Precision Female Speaker 0.763 Male Speaker 0.893   4.4.6 Effect of Loudness Table 4.20 shows the identification rate of an inversion speech of the noticeably loud speech and normal speech. The result showed that the AAI performance of the model was dropped significantly when applying with loud speech signals. This caused by the distortion presenting in loud speech signals which affected a produced speech to have a similar sound to its neighbor vowel, as shown in Figure 4.13. The figure shows the matrix comparing with the actual vowel description and identification of a vowel description from a model. The diagonal shows the correctly identify vowel description from an invert speech which indicated that the AAI model was correctly inverse articulation from recorded speech. The highest incorrect identification vowel was /É¯:/ and /u:/. The use of MFCC representation as a speech feature filtered some of the loudness effect but not all. While synthetic speech had a variation of F0 pressure, the synthesized process was linear. In contrast, the actual speech production might have an effect that distorts articulation to produce loud speech signals.        57  Table 4.20 Compare model prediction performance on normal and loud disyllabic    vowel speech data  Inversion Speech from The Model Precision Normal Speech 0.821 Loud Speech 0.608   
  Figure 4.13 Confusion matrix of an identification result from the AAI model of  normal speech data and loud speech data  4.4.7 Model Evaluation Figure 4.14 shows the confusion matrix of a vowel identification result. The blue color indicates the corrected identification which is a result of a correct inverted speech from the AAI model. The orange color indicates the incorrect identification, which is a result of the incorrect inversion speech from the AAI model. Most of the inversion speeches were correctly identified. Similar to the result in Figure 4.10, most of the error were the inversion speech which was resembling the neighbor vowel, which were /a:/ and /É”:/, /u:/ and /É¯:/, /É¯:/ and /i:/, /É¯:/ and /e:/. The most misidentification was between /o:/ and /u:/. This error was due to the articulation of both /o:/ and /u:/ are similar, but the /u:/ need the mouth to be nearly closed. Thus, the mouth ranged from nearly close to mid close produced sound resembling to /o:/ more than /u:/. However, these errors could be identified prior inversion, as previously shown in Figure 3.19 where the MFCCs dimensionality reduced feature of /u:/ and /o:/ were cluster and mixed which indicated that the similarity between these features. Therefore, despite having some error, the AAI BiLSTM model could inverse most of an actual Thai vowel speech with decent accuracy.    
58  
  Figure 4.14 Confusion matrix of a vowel identification result by the evaluator model   4.5   General Discussion From experiments, the model could generalize to the actual recorded speech. With speaker simulation and data augmentation, the model accurately inverted speech data from a different distribution. Besides, the characteristic of the AAI problem, where similar speeches were produced from multiple articulations, improved model generalization by creating soft-regularization. This soft-regularization prevented the model from overfitting the synthetic data and aided the model to inference unseen distribution. The coarticulation effect simulated from the vocal tract model improved the model performance which indicated that this effect resembled the actual human speech.  Thus, the simulated data aided the AAI model when inferencing actual human speech. The proposed self-learning in the speech production acquisition model could learn the underlying speech representation and estimated articulation from the memory of the experience. The result indicated that more experiences could improve speech production, i.e., more variation of speaker and speech. This learning framework helped the model to recognize vowels from different languages. This was summarized from the result where the model learned from the synthetic data which originally were from native German speakers and successfully used to inference on the Thai vowel utterance. Therefore, the proposed model could accurately inference articulation from a given speech, and could generalize with an actual human speech from other languages.     
  CHAPTER 5 CONCLUSION   This study proposed the speech production acquisition learned by a self-learning strategy, developing from a deep learning framework. This model consisted of the data-generating model, which simulated the episodic and constructive memory. The data from this model were used to train the acoustic-to-articulatory model. The AAI model was trained on purely synthetic data and applied to actual recorded data of disyllabic Thai vowels. The VTL application was used to generate speech in both the data generation process and the speech inversion process. The vowel identification model was developed by training on the actual record speech of a disyllabic Thai vowel. This model was used to evaluate the AAI model. Speech features used in both models were preprocessed. These speech features were MFCCs, its velocity, and its acceleration.  The proposed AAI model accurately inverted, both simulated speech and a recorded Thai vowel speech, into an articulation configuration. From the model comparison, a five bidirectional long short-term memory recurrent neural network with 128 hidden units in each layer outperformed other models in a term of inversion speech accuracy given the original speech. The proposed data generating method, e.g., speaker vocal tract simulation and data-augmentation, improved the inversion performance. The proposed vowel identification model has accurately identified vowel from a given speech feature by having AOC with a 95% confidence interval of 0.995 and 0.987. The study findings were that the model trained with the coarticulation effect improved target articulatory estimation. The gender effect affected the AAI model, i.e., the model performed better on male speaker speech than female speaker speech. The loudness of speech affected the AAI model by decreasing AAI performance when applied with loud speech data. The AAI model produced a small inversion error between an inversion speech and original recorded Thai vowel speech. Most of the occurring mismatches were between the neighbor vowel, e.g., /o://u:/ and /a://É”:/. However, these errors were recognizable before an inversion, causing from data preprocessing method.  While the proposed AAI model could learn and mimic the speech from the actual recorded speech data, the few caveats were presented. First, the proposed AAI model was not fully completed. The proposed model required a manual data pre-processing method, e.g., syllable segmentation in long speech sequence and speech stretching for a short speech segment. Moreover, the generating process of an inversion speech using an estimated target articulatory required prior knowledge. These prior knowledge were: speech duration, F0 pressure, and time to reach target articulatory. Second, the speech inversion was limited in a German speech production space. That was because the model learned from its German vowel space and the calibrated German gesture configuration. Thus, even the model accurately inversed the Thai speech vowel, the 60  produced inversion speech perceptually resembled the German vowel space more than Thai vowel space. Last, the actual perception test was not performed in this study.  In summary, the results in this study demonstrated that the proposed AAI model was effectively simulated speech production acquisition using a self-learning strategy. The further improvement in this model is to develop a proper data preprocessing pipeline to handled variations presented in the actual recorded data, e.g., a proper syllable segmentation method. This speech production acquisition model framework can be adapted with the consonant-vowel speech data.       REFERENCES    1. Toutios, A. and Margaritis, K., 2003, â€œA Rough Guide to the Acoustic-to-articulatory Inversion of Speechâ€, In 6th Hellenic European Conference of Computer Mathematics and its Applications HERCMA-2003, 25 July 2003, Athens, Greece, pp. 746-753.  2. Qin, C. and Carreira-PerpiÃ±Ã¡n, M. Ã., 2007, â€œAn Empirical Investigation of the Non-uniqueness in the Acoustic-to-articulatory Mappingâ€, In Eighth Annual Conference of the International Speech Communication Association,  31 August 2007, Antwerp, Belgium, pp. 74-77.  3. Ohala, J.J., 1993, â€œCoarticulation And Phonologyâ€, Language and Speech, Vol. 36, pp. 2â€“3.  4. Illa, A. and Ghosh, P.K., 2020, â€œThe Impact of Speaking Rate on Acoustic-To-Articulatory Inversionâ€, Computer Speech & Language, Vol. 59, pp.75-90.  5. Afshan, A. and Ghosh, P.K., 2015, â€œImproved Subject-Independent Acoustic-To-Articulatory Inversionâ€, Speech Communication, Vol. 66, pp.1-16.   6. Wrench, A.A., 2000, â€œA Multichannel Articulatory Database and Its Application for Automatic Speech Recognitionâ€, In Proceedings 5th Seminar of Speech Production, 1 May 2000, Bavaria, Germany, pp. 305-308.  7. Richmond, K., Hoole, P., and King, S., 2011, â€œAnnouncing the Electromagnetic Articulography (Day 1) Subset of the MNGU0 Articulatory Corpusâ€, In Twelfth Annual Conference of the International Speech Communication Association, 31 August 2011, Florence, Italy, pp. 1505-1508.   8. Maeda, S., 1990, â€œCompensatory Articulation During Speech: Evidence from The Analysis and Synthesis of Vocal-Tract Shapes Using an Articulatory Modelâ€, In Speech Production and Speech Modelling, Springer, Dordrecht, pp. 131-149.   9. Birkholz, P., JackÃ¨l, D. and Kroger, B. J., 2006, â€œConstruction and Control of a Three-Dimensional Vocal Tract Modelâ€, In Acoustics, Speech and Signal Processing ICASSP Proceedings, 14 May 2006, Toulouse, France, Vol. 1,  pp. 1-1.     10. Li H., Tao J., Yang M. and Liu B., 2015, â€œEstimate Articulatory MRI Series from Acoustic Signal Using Deep Architectureâ€, IEEE Transactions on Acoustics, Speech and Signal Processing, 6 August 2015, Brisbane, QLD, Australia, pp. 4854 - 4858.     62 11. Uria B., Renals S. and Richmond K., 2011, â€œA Deep Neural Network for Acoustic-articulatory Speech Inversionâ€, NIPS 2011 Workshop on Deep Learning and Unsupervised Feature Learning, January 2012, Paris, France, p. 1.  12. Tobing P.L., Kameoka H. and Toda T., 2017, â€œDeep Acoustic-to-articulatory Inversion Mapping with Latent Trajectory Modeling,â€ Asia-Pacific Signal and Information Processing Association Annual Summit and Conference,  30 December 2017, Kuala Lumpur, Malaysian, pp. 1274 â€“ 1277.  13. Zexin C., Qin X., Cai D., Li M., Liu X. and Zhong H., 2019, â€œThe DKU-JNU-EMA Electromagnetic Articulography Database on Mandarin and Chinese Dialects with Tandem Feature Based Acoustic-to-articulatory Inversionâ€, International Symposium on Chinese Spoken Language Processing,  26 November 2018, Taipei City, Taiwan, pp. 235-239.   14. Peng L., Yu Q., Wu Z., Kang S., Meng H., and Cai L., 2015, â€œA Deep Recurrent Approach for Acoustic-to-articulatory Inversionâ€, IEEE International Conference on Acoustics, Speech and Signal Processing, 19 April 2015, Brisbane, QLD, Australia, pp. 4450-4454.   15. Panchapagesan, S. and Alwan, A., 2011, â€œA Study of Acoustic-to-articulatory Inversion of Speech by Analysis-by-synthesis Using Chain Matrices and the Maeda Articulatory Modelâ€, The Journal of the Acoustical Society of America, Vol. 129, No. 4, pp. 2144-2162.  16. Prom-on, S., Birkholz, P. and Xu, Y., 2014, â€œIdentifying Underlying Articulatory Targets of Thai Vowels from Acoustic Data Based on an Analysis-by-synthesis Approachâ€, EURASIP Journal on Audio, Speech, and Music Processing, Vol. 1, p. 23.   17. Fairee, S., Sirinaovakul, B. and Prom-on, S, 2015, â€œAcoustic-to-Articulatory Inversion Using Particle Swarm Optimizationâ€, In Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2015 12th International Conference, 20 June 2015, Thailand, pp. 1-6.  18. Gao, Y., Stone, S. and Birkholz, P., 2019, â€œArticulatory Copy Synthesis Based on A Genetic Algorithmâ€, Proceedings of INTERSPEECH, 15-19 September 2019, Graz, Austria, pp. 3770-3774.   19. Birkholz, P., VocalTractLab Towards High-quality Articulatory Speech Synthesis, [Online], Available: http://www.vocaltractlab.de [2 October 2020].   20. Zolnay, A., Schluter, R. and Ney, H., 2005, â€œAcoustic Feature Combination for Robust Speech Recognitionâ€, In Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP), 2005 IEEE International Conference,  18 March 2005, Philadelphia, Pennsylvania, Vol. 1, pp. I-457.    63 21. Ling, Z.H., Richmond, K., Yamagishi, J. and Wang, R.H., 2009, â€œIntegrating Articulatory Features into HMM-Based Parametric Speech Synthesisâ€, IEEE Transactions on Audio, Speech, and Language Processing, Vol. 17, No. 6, pp. 1171-1185.   22. Sivaraman, G., Espy-Wilson, C. and Wieling, M., 2017, â€œAnalysis of Acoustic-to-articulatory Speech Inversion Across Different Accents and Languagesâ€, Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2017, 18 August 2017, Stockholm, Sweden, pp. 974-978.   23. Zhao, J., Yuan, H., Leung, W.K., Meng, H., Liu, J., and Xia, S., 2013, â€œAudiovisual Synthesis of Exaggerated Speech for Corrective Feedback in Computer-Assisted Pronunciation Trainingâ€, In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference, 26 May 2013, Vancouver, Canada, pp. 8218-8222.   24. Leung, K.Y., Mak, M.W. and Kung, S.Y., 2004, May, â€œApplying Articulatory Features to Telephone-based Speaker Verificationâ€, In Acoustics, Speech, and Signal Processing, 2004 Proceedings (ICASSP'04) IEEE International Conference, May 2004, Quebec, Canada, Vol. 1, pp. 1-85.   25. Hofer, G. and Richmond, K., 2010, â€œComparison of Hmm and TMD Methods for Lip Synchronizationâ€, Proceedings of INTERSPEECH 2010, September 2010, Chiba, Japan, pp. 454â€“457.   26. Guenther, F.H. and Vladusich, T., 2012, â€œA Neural Theory of Speech Acquisition and Productionâ€, Journal of Neurolinguistics, Vol.25, No.5, pp.408-422.  27. Terband, H., Maassen, B., Guenther, F.H., and Brumberg, J., 2009, â€œComputational Neural Modeling Of Speech Motor Control in Childhood Apraxia of Speech (CAS)â€, Journal of Speech, Language, and Hearing Research, Vol. 52, No. 6, pp. 1595â€“1609.  28. Richards, B.A., Lillicrap, T.P., Beaudoin, P., Bengio, Y., Bogacz, R., Christensen, A., Clopath, C., Costa, R.P., de Berker, A., Ganguli, S. and Gillon, C.J., 2019, â€œA Deep Learning Framework For Neuroscienceâ€, Nature Neuroscience, Vol. 22, No.11, pp.1761-1770.    29. Hornik, K., Stinchcombe, M. and White, H., 1989, â€œMultilayer Feedforward Networks Are Universal Approximatorsâ€, Neural Networks, Vol. 2, No. 5,  pp. 359-366.   30. Marblestone, A.H., Wayne, G. and Kording, K.P., 2016, â€œToward an Integration of Deep Learning and Neuroscienceâ€, Frontiers in Computational Neuroscience, Vol. 10, p. 94.    64 31. Lukose, S., and Upadhya, S.S., 2017, â€œText to Speech Synthesizer-Formant Synthesisâ€, In Nascent Technologies in Engineering (ICNTE), 2017 International Conference, 27 January 2017, Mumbai, India, pp. 1-4.  32. Oloko-oba, M., Ibiyemi, T.S. and Osagie S., 2016, â€œText-to-Speech Synthesis Using Concatenative Approachâ€, International Journal of Trend in Research and Development, Vol. 3, pp. 559-462.   33. Tokuda, K., Nankaku, Y., Toda, T., Zen, H., Yamagishi, J. and Oura, K., 2013, â€œSpeech Synthesis Based on Hidden Markov Modelsâ€, Proceedings of the IEEE, May 2013, New York, USA, Vol. 101, No. 5, pp. 1234-1252.  34. Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.W. and Kavukcuoglu, K., 2016, WaveNet: A Generative Model for Raw Audio, [Online], Available: https://arxiv.org/pdf/1609.03499.pdf [21 April 2020].   35. Arslan, L. M. and Talkin, D., 1997, â€œVoice Conversion by Codebook Mapping of Line Spectral Frequencies and Excitation Spectrumâ€, In Fifth European Conference on Speech Communication and Technology, September 1997, Rhodes, Greece, p. 113.   36. Palo, P., 2006, A Review of Articulatory Speech Synthesis, Master of Science in Technology, Department of Electrical and Communications Engineering, Helsinki University of Technology, Helsinki.   37. Shadle, C. H. and Damper, R.I., 2002, â€œProspects for Articulatory Synthesis: A Position Paperâ€, Proceedings of 4th ISCA Workshop on Speech Synthesis, September 2002, Perth Shire, Scotland, pp. 121â€“126.   38. Birkholz, P., 2013, â€œModeling Consonant-Vowel Coarticulation for Articulatory Speech Synthesisâ€, PloS one, Vol. 8, No. 4, p. e60603.   39. Birkholz, P. and KrÃ¶ger, B.J., 2006, â€œVocal Tract Model Adaptation Using Magnetic Resonance Imagingâ€, In Proceedings of the 7th International Seminar on Speech Production, December 2006, Ubatuba, Brazil, pp. 493-500.   40. Prom-On, S., Xu, Y. and Thipakorn, B., 2009, â€œModeling Tone and Intonation in Mandarin and English as a Process of Target Approximationâ€, The Journal of the Acoustical Society of America, Vol. 125, No. 1, pp. 405-424.   41. Gaikwad, S.K., Gawali, B.W. and Yannawar, P., 2010, â€œA Review on Speech Recognition Techniqueâ€, International Journal of Computer Applications, Vol. 10, No. 3, pp. 16-24.      65 42. Qin, C. and Carreira-PerpiÃ±Ã¡n, M.Ã., 2007, â€œA Comparison of Acoustic Features for Articulatory Inversionâ€, In Eighth Annual Conference of the International Speech Communication Association, August 2007, Antwerp, Belgium, pp. 2469-2472.  43. Prasanta, K.G. and Shrikanth, N., 2010, â€œA Generalized Smoothness Criterion for Acoustic-to-Articulatory Inversion,â€ The Journal of the Acoustical Society of America, Vol. 128, No. 4, pp. 2162â€“2172,    44. Davis, S.B., and Mermelstein, P., 1990, â€œComparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentencesâ€, in IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. 28, No. 4, pp. 357-366.  45. Kingma, D.P. and Ba, J., 2014, Adam: A Method for Stochastic Optimization [Online], Available: https://arxiv.org/pdf/1412.6980.pdf [10 April 2020].   46. Rumelhart, D.E., Hinton, G.E. and Williams, R.J., 1985, â€œLearning Internal Representations by Error Propagationâ€, in Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations, MITP, 1987, pp. 318-362.  47. Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012, â€œImagenet Classification with Deep Convolutional Neural Networksâ€, In Advances in Neural Information Processing Systems, Vol. 24, No. 2, pp. 1097-1105.  48. He, K., Zhang, X., Ren, S. and Sun, J., 2015, â€œDelving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classificationâ€, In Proceedings of The IEEE International Conference on Computer Vision, December 2015, Santiago, Chile, pp. 1026-1034.  49. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R., 2014, â€œDropout: A Simple Way to Prevent Neural Networks from Overfittingâ€, The Journal of Machine Learning Research, Vol. 15, No. 1,  pp. 1929-1958.  50. Ko, T., Peddinti, V., Povey, D. and Khudanpur, S., 2015, â€œAudio Augmentation for Speech Recognitionâ€, In Sixteenth Annual Conference of The International Speech Communication Association, September 2015, Dresden, Germany, pp. 3586-3589.  51. Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. and Le, Q.V., 2019, SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition [Online], Available: https://arxiv.org/pdf/1904.08779.pdf [10 April 2020].   52. Charpentier, F. and Stella, M., 1986, â€œDiphone Synthesis Using an Overlap-Add Technique for Speech Waveforms Concatenationâ€, In ICASSP'86. IEEE International Conference on Acoustics, Speech, And Signal Processing,  7-11 April 1986, Tokyo, Japan, Vol. 11, pp. 2015-2018.   66  53. Verhelst, W. and Roelands, M., 1993, â€œAn Overlap-Add Technique Based on Waveform Similarity (WSOLA) For High Quality Time-Scale Modification of Speechâ€, In 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing, 1993, MN, USA, Vol. 2, pp. 554-557.  54. Ragni, A., Knill, K., Rath, S.P. and Gales, M., 2014, â€œData Augmentation for Low Resource Languagesâ€, In INTERSPEECH 2014, September 2014, Singapore, pp. 810-814.  55. Rosenberg, A., Zhang, Y., Ramabhadran, B., Jia, Y., Moreno, P., Wu, Y. and Wu, Z., 2019, Speech Recognition with Augmented Synthesized Speech [Online], Available: https://arxiv.org/pdf/1909.11699.pdf [11 April 2020].   56. Tran, T., Pham, T., Carneiro, G., Palmer, L. and Reid, I., 2017, A Bayesian Data Augmentation Approach for Learning Deep Models [Online], Available: https://arxiv.org/pdf/1710.10564.pdf [11 April 2020].    57. Yao, Y., Rosasco, L. and Caponnetto, A., 2007, â€œOn Early Stopping In Gradient Descent Learningâ€, Constructive Approximation, Vol. 26, No. 2, pp. 289-315.  58. Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O., 2016, Understanding Deep Learning Requires Rethinking Generalization [Online], Available: https://arxiv.org/pdf/1611.03530.pdf [12 April 2020].    59. Fukushima, K., 1980, â€œNeocognitron: A Self-Organizing Neural Network Model for A Mechanism of Pattern Recognition Unaffected By Shift In Positionâ€, Biological Cybernetics, Vol. 36, No. 4, pp. 193-202.   60. Hochreiter, S. and Schmidhuber, J., 1997, â€œLong Short-Term Memoryâ€, Neural Computation, Vol. 9, No. 8, pp. 1735-1780.   61. Schuster, M. and Paliwal, K. K., 1997, â€œBidirectional Recurrent Neural Networksâ€, IEEE Transactions on Signal Processing, Vol. 45, No. 11,  pp. 2673-2681.  62. Ouni, S. and Laprie, Y., 2005, â€œModeling the Articulatory Space Using a Hypercube Codebook for Acoustic-to-articulatory Inversionâ€, The Journal of the Acoustical Society of America, Vol. 118, No. 1, pp. 444-460.  63. Hiroya, S.,and Honda, M., 2004, â€œEstimation of Articulatory Movements from Speech Acoustics Using an HMM-Based Speech Production Modelâ€, IEEE Transactions on Speech and Audio Processing, Vol. 12, No. 2, pp. 175-185.   64. Toda, T., Black, A. and Tokuda, K., 2004, â€œAcoustic-to-articulatory Inversion Mapping with Gaussian Mixture Modelâ€, In Eighth International Conference on Spoken Language Processing, October 2004, Jeju Island, South Korea,  pp. 1129-1132.    67 65. Uchida, H., Saito, D. and Minematsu, N., 2017, â€œAcoustic-to-articulatory Mapping Based on Mixture of Probabilistic Canonical Correlation Analysisâ€, In INTERSPEECH 2017, August 2017, Stockholm, Sweden, pp. 989-993.  66. Mitra, V., Nam, H., Espy-Wilson, C. Y., Saltzman, E. and Goldstein, L., 2010, â€œRetrieving Tract Variables from Acoustics: A Comparison of Different Machine Learning Strategiesâ€, IEEE Journal of Selected Topics in Signal Processing, Vol. 4, No. 6, pp. 1027-1045.   67. Uria, B., Murray, I., Renals, S. and Richmond, K., 2012, â€œDeep Architectures for Articulatory Inversionâ€, In Thirteenth Annual Conference of the International Speech Communication Association, September 2012, Portland, Oregon, pp. 867-870.  68. Najnin, S. and Banerjee, B., 2015, â€œImproved Speech Inversion Using General Regression Neural Networkâ€, The Journal of the Acoustical Society of America, Vol. 138, No. 3, pp. EL229-EL235.  69. Li, H., Tao, J., Yang, M. and Liu, B., 2015, â€œEstimate Articulatory MRI Series from Acoustic Signal Using Deep Architectureâ€, In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference, April 2015, Brisbane, Australia, pp. 4854-4858.  70. Liu, P., Yu, Q., Wu, Z., Kang, S., Meng, H. and Cai, L., 2015, â€œA Deep Recurrent Approach for Acoustic-to-articulatory Inversionâ€ In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference, April 2015, Brisbane, Australia, pp. 4450-4454.  71. Zhu, P., Xie, L. and Chen, Y., 2015, â€œArticulatory Movement Prediction Using Deep Bidirectional Long Short-Term Memory Based Recurrent Neural Networks and Word/Phone Embeddingsâ€, In Sixteenth Annual Conference of the International Speech Communication Association, September 2015, Dresden, Germany, pp. 2192-2196.  72. Yu, L., Yu, J. and Ling, Q., 2018, â€œSynthesizing 3D Acoustic-Articulatory Mapping Trajectories: Predicting Articulatory Movements by Long-Term Recurrent Convolutional Neural Networkâ€, In 2018 IEEE Visual Communications and Image Processing, 9 December 2018, Taichung, Taiwan, pp. 1-4.     73. Porras, D., SepÃºlveda-SepÃºlveda, A. and CsapÃ³, T.G., 2019, â€œDNN-based Acoustic-to-Articulatory Inversion using Ultrasound Tongue Imagingâ€, In 2019 International Joint Conference on Neural Networks, July 2019, Budapest, Hungary, pp. 1-8.   74. Howard, I. and Huckvale, M., 2005, â€œTraining A Vocal Tract Synthesiser To Imitate Speech Using Distal Supervised Learningâ€, SpeCom: 10th International Conference on Speech and Computer, October 2005, Greece, Vol. 2, pp. 159-162.    68  75. Murakami, M., KrÃ¶ger, B., Birkholz, P. and Triesch, J., 2015, â€œSeeing [U] Aids Vocal Learning: Babbling And Imitation Of Vowels Using A 3D Vocal Tract Model, Reinforcement Learning, And Reservoir Computingâ€, In 2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics, August 2015, Rhode Island, U.S.A., pp. 208-213.   76. Howard, I.S. and Birkholz, P., 2019, â€œModelling Vowel Acquisition Using the Birkholz Synthesizerâ€, Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2019, TUDPress, Dresden,  pp. 304-311.  77. Howard, I.S. and Huckvale, M.A., 2013, â€œLearning to Control an Articulatory Synthesizer by Imitating Real Speechâ€, ZAS Papers in Linguistics, 2005,  Vol. 40, pp. 63-78.   78. Mullally, S.L. and Maguire, E.A., 2014, â€œMemory, Imagination, And Predicting the Future: A Common Brain Mechanism?â€, The Neuroscientist, Vol. 20, No. 3, pp. 220-234.   79. Zhou, P. and Christianson, K., 2016, â€œAuditory Perceptual Simulation: Simulating Speech Rates or Accents?â€, Acta Psychologica, Vol. 168,  pp. 85-90.   80. Birkholz, P. and KrÃ¶ger, B.J., 2007, â€œSimulation of Vocal Tract Growth for Articulatory Speech Synthesisâ€, In Proceedings of the 16th International Congress of Phonetic Sciences, August 2007, SaarbrÃ¼cken, Germany,  pp. 377-380.   81. Dwarampudi, M. and Reddy, N.V., 2019, Effects of Padding on LSTM And CNN [Online], Available: https://arxiv.org/pdf/1903.07288.pdf  [15 April 2020].    82. Viikki, O. and Laurila, K., 1998, â€œCepstral Domain Segmental Feature Vector Normalization for Noise Robust Speech Recognitionâ€, Speech Communication, Vol. 25, No. 1-3, pp. 133-147.   83. McInnes, L., Healy, J. and Melville, J., 2018, Umap: Uniform Manifold Approximation and Projection for Dimension Reduction [Online], Available: https://arxiv.org/pdf/1802.03426.pdf [15 April 2020].  84. Boersma, P., 2001, â€œPraat, A System for Doing Phonetics by Computerâ€,  Glot International, Vol. 5, No. 9-10, pp. 314â€“345.  85. Abramson, A.S., 1962, The Vowels and Tones of Standard Thai: Acoustical Measurements and Experiments [Online], Available: http://www.haskins.yale.edu/Reprints/HL0035.pdf [15 April 2020].    69 86. Tingsabadh, K. and Abhramson, A.S., 1993, â€œThai Sound: An Acoustic Studyâ€, Journal of the International Phonetic Association, Vol. 22, No. 1, pp. 24â€“48.    70  CURRICULUM VITAE    NAME                                     Mr. Thanat Lapthawan DATE OF BIRTH     21 April 1995  EDUCATIONAL RECORD HIGH SCHOOL                    High School Graduation                                       Sarasas Ektra School, 2012 BACHELORâ€™S DEGREE      Bachelor of Engineering (Computer Engineering) King Mongkutâ€™s University of Technology Thonburi, 2016  SCHOLARSHIP/        Research Grant for Graduate Student   RESEARCH GRANT         Petchra Prajomklao Scholarship, 2017  PUBLICATION Lapthawan, T. and Prom-on, S., 2019, â€œAcoustic-to-Articulatory Inversion of a Three-dimensional Theoretical Vocal Tract Model Using Deep Learning-based Modelâ€, IEEE 10th International Conference on Awareness Science and Technology (iCAST), 23-25 October 2019, Morioka, Japan, pp. 1-5.     