Text Summarization on Covid 19 News
Since the spread of Coronavirus disease or Covid-19 at the end of 2019, there has been an extensive amount of news about Covid-19, and it takes a long time for humans to read the news, process it and retrieve important information from it. Therefore, automatic text summarization is necessary in this matter as it can help us process information faster and use it to make better decisions. Currently, there are two main approaches to automatic text summarization: extractive and abstractive. Extractive text summarization is conducted by identifying important parts of the text and extracting a subset of sentences from the original text. Abstractive text summarization is closer to the human method as it is the reproduction or rephrasing based on interpretation and understanding of the text using natural language processing techniques. In this research, we applied an abstractive summarization method on a specific dataset, namely, Canadian Broadcasting Corporation (CBC) news dataset about Covid-19 news. Data augmentation was also exploited in the pre-processing part to be an example case of working with data that are not perfect or diverse enough. Two Long Short Term Memory (LSTM) models, with and without data augmentation, were used to generate summaries. The resulting summaries were analyzed and compared with related work using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics.
abstractive text summarization/ coronavirus/ COVID-19/ natural language processing

CHAPTER 1 INTRODUCTION   Natural Language Processing (NLP) is the computer engineering of machines to process and understand human languages in order to perform NLP tasks. It is the core of many applications nowadays including machine translation, natural language understanding, information extraction, text classification and text summarization. As for automatic text summarization, it is the task of generating concise summaries from voluminous texts for easy understanding and further information processing. For automatic text summarization which is the focus of this research, there are two main methods: Extractive and Abstractive text summarization. Extractive text summarization involves the selection of important contents of a text document. Abstractive text summarization applies rules that generate a summary not based on the words in the original document but based on its content. In this research, we will focus on the Abstractive text summarization which we consider to be close to how humans make summaries.   The domain of our research is on the news about Coronavirus or Covid-19. Covid-19 is a disease that originated in Wuhan, China in December 2019. Since then, it became a global pandemic and people want to have knowledge about its symptoms, preventive measures, vaccines, etc. and they follow the news for such knowledge or information. However, there are many news articles to process, which are too long and include many technical words that make them hard to be understood. Automatic text summarization is, thus, useful and can contribute to information processing and decision making.    1.1 Statement of Problem  This research focuses on automatic text summarization using the abstractive method, on the dataset of coronavirus news. The research aims at answering the following questions.  1.1.1. How to apply the abstractive summarization method to automatically summarize the dataset of coronavirus news?  1.1.2. How to apply the data augmentation in text preprocessing? 1.1.3. How to evaluate the results of the summarized texts? 1.1.4. How to apply NLP metrics for evaluation?  1.2 Objectives  The objective of this project research is to implement the Abstractive Text Summarization and generate summaries from the dataset of Coronavirus news. The objectives are as follows: 1.2.1. Study and review text summarization methods.  1.2.2. Study about the neural network like Recurrent Neural Network (RNN) i.e. Long Short-Term Memory model (LSTM), encoder-decoder model and their applications. 1.2.3. Study and apply abstractive text summarization methods on COVID19 news as well as augmenting the datasets in preprocessing steps.  1.2.4. Evaluate results using NLP metrics.   2  1.3 Scopes  The scope of this research to apply abstractive method for text summarization on Covid-19 using Canadian Broadcasting Corporation or CBC news dataset and can be further defined as follows:  1.3.1. The implementation of this research is conducted in Python 1.3.2. The evaluation of text summaries results is done using a set of metrics of Recall-Oriented Understudy for Gisting Evaluation (ROUGE) framework as well as comparison with the related work.  1.4 Expected Benefit  The expected benefit of this research project is that the outcome is obtained in the following orders.  1.4.1. Study and implementation of Abstractive Text Summarization.  1.4.2. Study and research on neural network and its classes.  1.4.3. Apply NLP metrics for evaluating automatic text summarization task.   1.5 Thesis Organization This thesis is organized as follows: Chapter 1 introduces the problem, research questions, and objectives of this research. In Chapter 2, Literature Review and related research are discussed. Chapter 3 describes the methodology and processes of the research. Chapter 4 presents the experiment results and evaluation. Chapter 5 presents our conclusion and discusses the possible directions for future work.    CHAPTER 2 BACKGROUND AND LITERATURE REVIEW   This chapter provides background and related work on automatic text summarization in the following topics: (1) Text summarization: extractive and abstractive methods, (2) Recurrent Neural Network and Long Short Term Memory, (3) Encoder-Decoder Architecture and Transformer, (4) Attention mechanism and (5) Recall-Oriented Understudy for Gisting Evaluation (ROUGE) framework.    2.1 Text Summarization Text summarization is an important NLP task. It involves shortening original texts while still preserving the meaning. There are two approaches to text summarization: extractive and abstractive methods.   2.1.1 Extractive Text Summarization  Several researchers apply the extractive text summarization method as in [4]. Extractive summarization is the method that selects information from the original text by ranking or scoring sentences and generating a summary. Sentence scoring solves the problem of deciding which sentences to include in the summary by including the sentences with the highest scores. Another method is to use Term Frequency-Inverse Document Frequency Method (TF-IDF).   Term Frequency (TF) and Inverse Document Frequency Methods (IDF) are statisticals that show how the importance of a word given in a document. This is done by multiplying two metrics: how many times a word appears in a document (TF), and the inverse document frequency of the word across a set of documents (IDF). After getting the scores of TF and IDF, sentences with high TF-IDF scores will be used to generate a summary. It should be noted that the problem with TF-IDF is that sometimes longer sentences have high scores simply due to the fact that they contain more words.  2.1.2 Abstractive Text Summarization Abstractive text summarization [5] is the summarization that is close to how a human summarizes a document. First, a document is processed and a summary is generated using new words, sentences, and rephrasing in Figure 2.1. The abstractive text summarization was mostly applied by using deep learning or machine learning such as a long short-term memory (LSTM) [6]. However, this method could take a long time to process because it includes lots of methods such as content preprocessing, word embedding, fundamental model design, discourse rules, etc. [7].   4       Figure 2.1 Abstractive text summarization  Talukder et al.[7]  reviewed and compared the technique to summarize texts based on the abstractive text summarization such as Word Graph Methodology, Semantic Graph Reduction Algorithm, and Markov Clustering Principle.   Another research using the abstractive text summarization was Logeesan et al. [8] who applied the abstractive text summarization on stock market news articles. [8] uses keyword-based weighting to extract the sentences and graph algorithms to analyze the relatedness among the sentences. Each sentence was represented as a node in the graph to show the relationship between the sentences in order to create a summary of the document that contained only the significant contents.  2.2 Recurrent Neural Network Recurrent Neural Network (RNN)[9] is a type of artificial neural network which uses sequential data or time series data and can be used with text processing tasks including text summarization. RNN is essentially a fully connected neural network that contains a refactoring of some of its layers into a loop. That loop is typically an iteration over the addition or concatenation of two inputs, a matrix multiplication and non-linear functions as in Figure 2.2. The concept of RNN is to introduce the memory in neural networks to their feedback, and these networks can learn information based on the context in which the output generated from the previous move which is used as input for the current move in order to prevent losing information during the process. Also, RNN provides a solution by using a hidden layer. Figure 2.3 shows the depiction of RNN being unfolded into a network. For the input sequence of n words, the network would be separated into an n-layered neural network in which a layer denoted each word. For example, if we consider the language model that tries to predict the next word in the previous sentence such as trying a prediction of the last word in “there are crowds on the …,” we do not need any future context. So, the next word is likely be “street”. It means that there is a small gap between the relevant information, and RNN can learn from the past information. However, the sentence may have more contexts that need to pass through the next word such as “I would really love rose and … but I hate Mac’n cheese.” In this information the next word is the name of thing that is loved but the following information is about something being hated so the gap between the relevant information becomes very large. As the gap expands, RNN becomes unable to learn to connect the information.    
 
 
 
 
Text Sentences Sentences Sentences 
Abstractive Text Summarization 
Summary New Sentences 5  
  Figure 2.2 A recurrent neural network [9] 
 Figure 2.3 An unfolded recurrent neural network [5]  2.2.1 Long Short-Term Memory Long short-term memory (LSTM)[9,10] is a unit of RNN that can identify and remember the data pattern for a certain period. Moreover, LSTM can solve the problem of gradient vanishing in the RNN training process, which uses Back-Propagation Through Time. LSTM can also lead to many successful runs and learn faster. LSTM is achieved by an efficient, gradient-based algorithm for an architecture. Figure 4 shows the architecture of an LSTM cell that includes the forget gate, input gate, and an output gate while working using the sigmoid activation function. The forget gate decides which information to keep or forget. The input gate updates the state of the cell and the output gate decides what would be the next hidden state. It is composed by multiplicative gate units which are able to be opened and closed for learning constant error flow. For example, in order to predict the next word of “I would really love rose and … but I hate Mac’n cheese.” that needs to contain the context of loved things and hated things and the LSTM has three gates that can control this information. The forget gate can thus prevent overload information in the LSTM and the input 
6  gate decides which word to update or replace by a new word. Finally, the output gate decides the output of LSTM. The LSTM and Recurrent Neural Network Language Model (RNNLM)[11] can be used to compose word attention and sentence attention for language modeling such as BoydCut[12], an NLP framework for identifying sentence boundaries based on Bidirectional LSTM-CNN Model, which was applied for Thai sentence segmentation. Moreover, the BoydCut framework can utilize words, characters, and parts of speech (POS) to create new sentences. Another related work is, Hayatin et al.[15] applied abstractive text summarization to COVID-19 news and also used the LSTM to generate the summaries based on transformer architecture.   
  Figure 2.4 The repeating module in a LSTM contains four interacting layers [5]  2.3 Encoder-Decoder Architecture and Transformer The Encoder-Decoder Architecture [20] is a way of organizing recurrent neural networks for sequence prediction problems such as machine translation and text summarization. The encoder reads the input sequence and summarizes the information as the internal state vectors and then generates the context vector which will be passed to the decoder as input. The decoder generates an output sequence based on the context vector as shown in Figure 2.5 shows an example used in the machine translation task which has a major problem with the variable-length sequences.  A way to handle the input and output problem are the design of two major components.   Another deep learning model is discussed here which is Transformer. A transformer, similar to RNN, is designed to process sequential input data, such as natural language. However, unlike RNN, the transformer processes the entire input all at once. An example is Bidirectional Encoder Representations from Transformers (BERT) [22] which are bi-directional, pre-trained language models that have been employed as encoders for natural language understanding tasks. The pre-trained model of BERT can also be used for many NLP tasks including text summarization.   
7     Figure 2.5 The Encoder-Decoder architecture  2.4 Attention Mechanism The attention mechanism as explained in [23] used in this research is based on the concept of the attention mechanism applied in the sequence-to-sequence model. The attention mechanism has to be used with the deep learning architecture known as alignment models.  As for attention, it is used for humans to focus on the importance of something such as a sentence. If we pay attention to the sentence, we can understand the information from the sentence. For deep learning tasks, an attention mechanism is applied to the model that can pay attention to certain factors during processing data because the context data is important while processing the text input as shown in Figure 2.6. Moreover, the attention mechanism could help the deep learning approach to learn the long-range data with the context vector that contains the information in every time step and has the important information of each input corresponding to the input as shown in Figure 2.7. This helps the model to overcome the vanishing gradient problem and also could help the model run faster compared to the model that does not use the attention mechanism.     Figure 2.6 The attention mechanism focuses on a word in the sentence  
8  
  Figure 2.7 The attention mechanism for the text summary  For this research, we attempt to use the attention mechanism layer in the deep learning model to improve the model in generating a summary by keeping the important information correspondingly, as well as improving the speed and ascertain the correctness of the input.   2.4.1 Bahdanau et al.’s Attention Mechanism The attention mechanism was first proposed by Bahdanau et al.[24] who used the sequence-to-sequence model by adding the alignment scores that were computed from the information of all hidden states of the encoders and the information of the previous state of the decoder which allows the model to find relationships between tokens.   After finding that the sequence-to-sequence model had a problem of the bottleneck for the encoding part that squeeze the information to a fixed-length that cause the performance of the sequence-to-sequence model to drop the length of the input sequence, [24] proposed a mechanism that allows the language model to automatically soft search for a part of an encoding related to the decoder by looking at the information both forward and backward for hidden state. Then, the context vector was sent to compute the attention score call Bi-directional RNN as shown in Figure 2.8.  
9  
  Figure 2.8 The architect of Bahdanau's attention [24]  2.5 Data Augmentation Data Augmentation [16] (DA) is a set of algorithms that constructs new data from the available dataset by using the strategy that increases the diversity of the dataset. In some cases, the datasets can be limited due to various reasons such as data sparsity. Furthermore, when the data is changed and augmented, this can increase the robustness, DA is widely used for machine learning areas such as computer visualization, especially classification tasks, e.g. image classification, and semantic classification. Mostly, DA is used for preventing overfitting and for NLP tasks by considering the reusable tokens and shuffling them as a new dataset. DA has 4 types based on the symbolic or neural methods.  2.5.1 Rule-based Augmentation Rule-based augmentation constructs rules from augmented examples by inserting new words and rearranging words from the existing data. Rule-based augmentation consists of 3 methods that are processed at the token level. First, the random swapping which randomly selects one word in the sentence and randomly pushes the selected word into a new position of a word in the sentence. Second, the random deletion which randomizes a word in the sentence and then removes a selected word, Finally, the random insertion which a new word and introduces to a sentence in a random position.  Table 2.1 Examples of Rule-based augmentation  Rule-based augmentation Example Example with Rule-based augmentation Random swapping I am running I like running Random deletion I am running I running Random insertion I am running I am joy running   
10  Table 2.1 shows how the sentences would be for the result for three methods of the Rule-based augmentation. This is the strategy that is processed at the token level of the document in order to increase the samples of the document in the simplest way.   2.5.2 Graph-structured Augmentation Graph-structured augmentation consists of relations and encodings for knowledge graphs, grammatical structures in syntax trees. For example, if we want to replace the word “lion”, the graph-structured augmentation will know the word that has the relation to the “lion” node such as “tiger”, or “jaguar”. In addition, there are kinds of graphs that have been developed with notables such as WordNet [17], Penn Treebank [18].    Jiajun et al.[19] applied the Graph-structured augmentation for classification to identify the category labels of graphs with limitations of the dataset that can easily lead to the overfitting. [9] exploited Graph-structured augmentation, using a generic model evolution framework called M-Evolve.  they have used M-Evolve, a generic model evolution framework, which combines Graph-structured augmentation, data filtration and model retraining. For the experiment, M-Evolve could help prevent overfitting in the graph and yielded the improvement of 3-12% of accuracy on the classification tasks.  2.5.3 MixUp Augmentation MixUp augmentation is a meshing the existing examples together, MixUp can be done by combining of a half of a sentence with half another sentence and concatenate it with half of another sentence in order to create a new example in Table 2.2.   Table 2.2 Examples of MixUp augmentation  Original sentences  MixUp augmentation I want to buy some snacks and drinks. I cross the street when the green light turns on. I cross the street to buy some snacks and drinks. I want to buy it when the green light turns on.  2.5.4 Feature Space Augmentation  Feature Space augmentation describes augmenting data that considers the intermediate representation space of Deep Neural Networks in which the input data is transformed into task-specific predictions. Also, Feature Space augmentation isolates the features and applies the noise to the new data. Moreover, the noise can be sampled from the standard uniform or gaussian distractions.  2.6 Recall-Oriented Understudy for Gisting Evaluation Framework (ROUGE) Recall-Oriented Understudy for Gisting Evaluation (ROUGE) framework [13,14] is a set of metrics and software packages used for evaluating natural language processing tasks such as summarization. Also, it includes several automatic evaluation methods that measure the similarity 11  between summaries. This is done by benchmarking a summary with one reference summary produced by humans using precision (𝑃) as (2.2), the ratio of overlapped word over system and recall (𝑅) as (2.1). As for F-score which is the statistical measurement combining precision and recall as (2.3) with b value that needs to be created to compute, the ratio of overlapped words over the reference summary, in the context.  It means that the ROUGE framework is evaluates the summary by comparing the reference summary and generated summary in order to show how the generated summary is close to the reference summary. The ROUGE framework also has different kinds of measurement such as ROUGE-N, ROUGE-L and ROUGE-S  𝑅𝑒𝑐𝑎𝑙𝑙(𝑅)=	#'()&*	,-	,.&*%/00"#$	1,*2!3,3/%	1,*2!	"#	*&-&*&#4&	!'((/*5                                               (2.1)  𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛(𝑃)=	#'()&*	,-	,.&*%/00"#$	1,*2!3,3/%	1,*2!	"#	$&#&*/3&2	!'((/*5                                              (2.2)  𝐹=	6789!	:.<	.=<8	9!=                                                                               (2.3)   2.6.1 ROUGE-N ROUGE-N measures the overlap of n-grams such as unigram, bigram, trigram, and higher-order n-gram between the reference summary and generated summary.in which n represents the length of the n-gram (𝑛). For example, 𝑅𝑂𝑈𝐺𝐸−1 is used to evaluate the summary based on the unigram or each word in the summary and also 𝑅𝑂𝑈𝐺𝐸−2 evaluates the summary by using the bigrams or a pair of words in the summary as follows:   𝑅𝑂𝑈𝐺𝐸−𝑁=	∑∑?,'#3"#$%&'(#")∈+($*/())+∈{-./.+1""#(2}∑∑?,'#3($*/())'(#")∈++∈{-./.+1""#(2}                          (2.4)  Other research work such as Hayatin et al.[15] used ROUGE-1, and ROUGE-2 for evaluating the summary of the COVID-19 news based on transformer architecture and encoder-decoder layers in the model. the best score of evolution for the ROUGE scores was 0.58 and 0.42 respectively.  2.6.2 ROUGE-L ROUGE-L which is represented as the longest common subsequence (LCS) to evaluate the summary sentences will be treated as a sequence of the maximum length of N-grams. Apart from ROUGE-N and ROUGE-L, there are variants of ROUGH including ROUGE-W which is evaluating the results based on weighted LCS, and ROUGE-S which is based on Skip-bigram.        CHAPTER 3 METHODOLOGY   In this chapter, the methodology is discussed and the process of the experiment is described in detail.  3.1 Overview of the Methodology The methodology can be divided into the following three steps: 1) Data collection: the data collection that we use is “Evolution of CBC news articles during COVID-19” in the Kaggle online database. 2) Text Preprocessing with the data augmentation 3) Generate the summary: using the abstractive summarization method 4) Evaluate the results: using the ROUGE framework.  3.2 Data Collection Our dataset is from “Evolution of CBC news articles during COVID-19” on Kaggle, the world's largest data science community, which contains news articles related to COVID-19 from CBC news. CBC (Canadian Broadcasting Corporation) is the news publisher in Canada. The dataset contains a total of 6,786 records since December 2020. Figure 3.1 shows the example records from the Kaggle. Each record consists of columns of authors who publish articles on the internet, the title of each article, date of publication, time that articles were published, details of the title, and contents of articles. Some of the articles include symbols and the URL as references in comma-separated values (CSV) format. In addition, about 93% of news articles, of them contain approximately 800 words while the summary of each contains approximately 15 words. 13  
  Figure 3.1 CBC News articles about COVID-19  Also, this collection of data can be plotted as a word cloud only in January, 2020 with 117,666 words as Figure 3.1 shows. China appears the biggest one because at that time most of the news articles contained the word China. Due to the limitation of the amount of data that can be processed on the computer, we consider the period of March and April 2020 as this period containing the word ‘COVID’ most frequently as in Figure 3.2 and it consists of approximately 3,000 records. 
    Figure 3.2 Word cloud in January, 2020 
14  
  Figure 3.3 Word cloud in March to April, 2020  3.3 Text Preprocessing Text preprocessing is applied over the dataset and it consists of the following steps: tokenizing, stop word removal, stemming, and lemmatization as shown in Figure 3.4.  
  Figure 3.4 Text preprocessing diagram 
15   3.3.1 Tokenizing Tokenizing is the method that separates a piece of text into smaller units called tokens. Tokens can be words, characters, or sub words. Broadly, tokenization is classified into 3 types such as word, character and sub word (n- gram) tokenization. For example, “I love hamburgers.” The tokenization can be separated into I-love-hamburgers. Moreover, tokenization can be done by character tokenization like h-a-m-b-u-r-g-e-r-s or by sub word tokenization like hamburger-s.  3.3.2 Stop Word Removal Stop word is a commonly used word like “a, an, the, in” that a search engine ignores for a better result of retrieving information. Also, the stop word would not be needed for computation because it will take more computing time. So, the reason is that we have to remove the stop word as in Figure 3.5 which is to remove “an” as a stop word.   Figure 3.5 Stop word removal  3.3.3 Stemming Stemming is the process of reducing the words to their word stem or root form. In general, the stemming is not identical to the morphological root of the word. It is usually sufficient that related words map to the same stem. Moreover, the advantages of the stemming are making a word to be in a simple form, faster to compute than using the original form, and easy to handle exceptions. In addition, the stemming has many algorithms in order to transform the origin word into root word, such as prefix-suffix stemming, production technique, etc. Is Figure 3.6 “running”, “runs”, “ran” are the root forms of “run”.    Figure 3.6 Stemming method  3.4 Data Augmentation Data augmentation is the strategy for increasing the diversity of training examples without collecting a new data. Also, the data augmentation can gain more computation efficiency with limited resources. Another profit from the data augmentation is that it can help to prevent the overfitting during the training step. Our selected data augmentation method is rule-based techniques includes random insertion, random deletion, and random swapping methods. By setting the probability that those 3 methods occur in each sentence, each sentence can be preformed by the I love an ice cream ®“I”,”love”, “ice”, “cream” 
Running, runs, ran  ® run 16  3 methods and then inserted as a new row of the training data. Each news record in the dataset can be manipulated by these three techniques. Therefore, from 3,000 records, after data augmentation, the news records become 32,000 records, as illustrated in Table 3.1. Moreover, this data augmentation was done only on the news documents. We did not take the data augmentation on the news’s summary rows in order not to augment the summary.   Table 3.1 Data augmentation generates a new sentence from the original dataset.  Original document Lily Overacker and Laurell Pallot start each gay-straight alliance meeting with everyone introducing themselves, saying their pronouns and sharing highs and lows of the week. Data augmented by insertion, deletion and swapping Lily Overacker and Laurell Pallot start each gay-straight alliance meeting with everyone themselves, saying their pronouns and sharing highs and lows of the week. Lily Overacker and Laurell Pallot start each gay-straight alliance meeting with everyone introducing themselves, saying their and highs lows of the week. Lily Overacker and Laurell Pallot start each gay-straight alliance whatever with everyone introducing themselves, saying their pronouns place sharing highs and lows of a week. Lily Overacker it and Laurell Pallot start each gay-straight alliance meeting with everyone introducing themselves, saying their pronouns menage and sharing highs and lows of the week. lily Overacker and Laurell Pallot first each gay-straight coalition cope with with everyone introducing themselves, locution their pronouns and sharing highs and lows of the week. lily Overacker and Laurell Pallot start each gay-straight alinement meeting with everyone introducing themselves, saying their pronouns and sharing highs and lows of the week. Lily Overacker and Laurell want Pallot start each gay-straight alliance meeting with everyone introducing break themselves, saying their pronouns and sharing highs and lows of the week. Lily Overacker and Laurell Pallot start each gay-straight alliance meeting with everyone  introducing themselves, saying their pronouns and a good deal sharing highs and lows of the week.  3.5 Summary Generation Figure 3.7 shows how the data flow and how to generate the summary. First, after we gather the data from Kaggle, we separate part of the data to be augmented and the other part is non-augmented. Both parts of data will then be sent to text pre-processing step as mentioned earlier. After that, the texts will be transformed into vectors before sending them to the model summary which includes the encoder, decoder, and Attention layers to generate a summary. This will create the candidate summary.  Then, the candidate summary and the reference summary by human will be evaluated using ROUGE. The process is the same for model with data augmentation and without data augmentation.    17  
  Figure 3.7 The model diagram for creating summary  The Summarization Model as shown in Figure 3.7 consists of the layers such as encoder layer, decoder layer, LSTM layer, and attention layer. All layers working together to generate the summary. For the document that passes the text preprocessing will be a token, and the token is ready to go to the encoder layer. The encoder layer converts the token to a vector for computation. Then the LSTM layer, which is used to compute the prediction vector in the sequence, compute the vector and predict the next vector in a sequence. Also the LSTM layer works with the attention layer in order to select the important vector value. Once the LSTM returns the result, the decoder takes the result vector and converts the vectors into the summary. It should be noted that the attention layer could be active in this layer to return the important word as shown as diagram in Figure 3.8.  
18  
  Figure 3.8 The summarized model layer flows  3.6 Evaluation of the Result The evaluation is conducted by inputting the news that was never used for training and the model will generate the summary of the news. The results acquired from the LSTM model as mentioned above will be evaluated using ROUGE-N metrics which includes ROUGE-1 and ROUGE-2. ROUGE-N metrics are calculated by: 𝑅𝑂𝑈𝐺𝐸−𝑁!"#$%&(𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒,𝑟𝑒𝑓𝑒𝑟𝑒𝑛𝑐𝑒)	=∑!"#!$%$!$&'$	∑&()!*+#!")*+#,(#.$/01,30#4"40,&)∑!"#!$%$!$&'$#+167/01!(/")          (3.1)  where the 𝑟"are the sentences in the reference document, Count(n-gram, candidate) is number of times the specified n-grams occur in the document and 𝑛𝑢𝑚𝑁𝑔𝑟𝑎𝑚𝑠(𝑟") is the number of n-grams of the reference sentence 𝑟". The results from the test set and real dataset could be compared in order to acquire the score that showed which data indicated the relative of the content of summary against a reference or a set of references (human-produced). We consider using ROUGE-1, ROUGE-2 and ROUGE-L specifically for our evaluation.    
CHAPTER 4 EXPERIMENTAL RESULTS  This chapter presents the discussion of data augmentation and results, as well as the ROUGE evaluation of summaries of COVID-19 news, generated using the LSTM and the encoder-decoder architecture with an attention mechanism.  4.1 Data Augmentation Before the text preprocessing step, we separated part of the data to be augmented using rule-based techniques. For each news article, 9 augmented articles were generated 3 articles from random insertion, 3 articles from random swapping, and 3 articles from random deletion. The sample results of augmented data are shown in Table 4.1-4.3  Table 4.1 Example of random insertion compared to the original sentence  Original sentence Many couples have rushed to postpone their weddings because of project COVID-19 Random insertion Many couples have rushed talk of the town atomic number  to postpone their weddings because of COVID-19  Table 4.2 Example of random swapping compared to the original sentence  Original sentence Many couples have rushed to postpone their weddings because of project COVID-19 Random swapping Many weddings have rushed to postpone their couples because of project COVID-19  Table 4.3 Example of random deletion compared to the original sentence  Original sentence Many couples have rushed to postpone their weddings because of project COVID-19 Random deletion Many couples have postponed their weddings because of COVID-19  4.1.1 Dataset Before model training, the dataset would be sperate into the training data which is training the model for fitting, and testing data for estimating the model accuracy. The dataset that was used for training the model without using the data augmentation has 3,831 records. it would be the training data for 3,031 records and the testing data for 800 records. This applied also for the dataset with augmentation which is 10 times more than the original dataset, hence the total data of 38,310 records. For the training data 37,510 records were used and 800 records were used for testing. For the validation of both models, the full dataset which contains 6,786 records were used for 20  measurement with ROUGE scores.   4.2 Model Training For our model, at the first, we tried to use one weight for both types of documents but the feature extraction did not support a different matrix in the same weight parameter in the model. Therefore, the solution was that we made one model with 2 sets of models; one set for the document that did not use the data augmentation and the other for the document that used the data augmentation.   Due to the capacity of the model, we experimented with 3,000 records of CBC news in April, 2020. The dataset from April 2020 constitutes half of the CBC news that was collected and contains high mentions of the COVID-19 term.      There were the layers and the parameters exploited during the training step which include the input layer, LSTM layer, and attention layer as set as parameters as shown in Table 4.4 with a learning rate of 0.001. Moreover, an LSTM unit for every layer have contains 300 units and using the tanh as activation function. It has dropout at 0.4 and recurrent dropout at 0.4 for the encoder. For the decoder, the dropout is 0.4 and recurrent dropout at 0.2.  The model architecture is as shown in Figure 4.1. It should be noted that the parameters going through the model are different between the one that uses data augmentation and the other that does not use data augmentation as shown in Table 4.5 and 4.6. There was no further fine-tuning conducted during the training.  Table 4.4 The model parameter setting  Layers Encoder LSTM Decoder LSTM Attention Layer Node Units 300 300 - Dropout 0.4 0.4 - Recurrent Dropout 0.4 0.2 - Activation Function tanh	tanh tanh Recurrent Function sigmoid	sigmoid sigmoid  21  
  Figure 4.1 The model architecture  Table 4.5 The model summary without the dataset that use the data augmentation.  Layer (type) Output Shape Param #  Connected to input_1(InputLayer) [(None, 80)]   0 [] embedding (Embedding)   (None, 80, 100)  3350200      ['input_1[0][0]']  lstm (LSTM)  [(None, 80, 300), (None, 300),(None, 300)]   481200        ['embedding[0][0]']  input_2 (InputLayer)   [(None, None)]  0             [] lstm_1 (LSTM)  [(None, 80, 300), (None, 300), (None, 300)] 721200       ['lstm[0][0]'] embedding_1 (Embedding)  (None, None, 100) 798000       ['input_2[0][0]'] lstm_2 (LSTM)   [(None, 80, 300), (None, 300), (None, 300)] 721200 ['lstm_1[0][0]'] lstm_3 (LSTM)    [(None, None, 300),[0]',(None, 481200       ['embedding_1[0][0]', 'lstm_2[0][1]', 
22  300), (None, 300)]    'lstm_2[0][2]'] attention_layer (AttentionLayer)  [(None, None, 300),(None, None, 80)]  180300  ['lstm_2[0][0]', 'lstm_3[0][0]']  concat_layer (Concatenate)   (None, None, 600)   0  ['lstm_3[0][0]',                                                           'attention_layer[0][0]']  time_distributed_layer (TimeDistributed)   (None, None, 7980)    4795980      ['concat_layer[0][0]']  Total params: 11,529,280 Trainable params: 11,529,280 Non-trainable params: 0  Table 4.6 The model summary of the dataset that uses the data augmentation  Layer (type) Output Shape Param #  Connected to input_1(InputLayer) [(None, 80)]   0 [] embedding (Embedding)   (None, 80, 100) 3440600   ['input_1[0][0]']  lstm (LSTM)  [(None, 80, 300), (None, 300),(None, 300)] 481200        ['embedding[0][0]']  input_2 (InputLayer)   [(None, None)] 0             [] lstm_1 (LSTM)  [(None, 80, 300), (None, 300), (None, 300)] 721200       ['lstm[0][0]'] embedding_1 (Embedding)  (None, None, 100) 455000       ['input_2[0][0]'] lstm_2 (LSTM)   [(None, 80, 300), (None, 300), (None, 300)] 721200 ['lstm_1[0][0]'] lstm_3 (LSTM)    [(None, None, 300),[0],(None, 300), (None, 300)]    481200       ['embedding_1[0][0]', 'lstm_2[0][1]', 'lstm_2[0][2]'] attention_layer [(None, None, 180300 ['lstm_2[0][0]', 23  (AttentionLayer)  300),(None, None, 80)]   'lstm_3[0][0]']  concat_layer (Concatenate)   (None, None, 600)   0  ['lstm_3[0][0]',                                                           'attention_layer[0][0]']  time_distributed_layer (TimeDistributed)   (None, None, 4550)  2734550       ['concat_layer[0][0]']  Total params: 9,215,250 Trainable params: 9,215,250 Non-trainable params: 0  4.2.2 Loss Score of Model without Using Data Augmentation In model training, each iteration will return the loss score which indicates the accuracy of the model in each iteration. If the loss is zero then the model’s prediction is flawless. The goal of training a model is to find a set of weights and biases that have low loss, on average.   For the model using the weight for the documents without the data augmentation, after training 1,000 iterations, the loss score was the score that was compared to the result of both the testing and training dataset,  which is shown in the graph in Figure 4.2.   
  Figure 4.2 The Loss score for a model that does not use the data augmentation.  The loss score was lower after the 35th iteration and close to 0 around the 307th iteration. 
24  Furthermore, after the 1,000th iteration the loss scores still did not reach 0 (for example, it still showed 0.01232134). It meant that we could train the model and set the end of the iteration at 307th, and the loss score would not show the difference between the iteration at 307th and 1,000th iterations.  4.2.3 Loss Score of Model Using Data Augmentation For the model using the weight for the documents with the data augmentation, after training 1,000 iterations, the loss score was shown as in Figure 4.3.   
  Figure 4.3 The Loss score for a model that uses the data augmentation.  The loss score become lower after the 20th iteration and was close to 0 around the 273th iteration. When we could compare the iterations between two models, the model using the data augmentation could return the loss score close to 0 earlier than the other model.  The difference of each model showed that the loss score of the model that used the weight matrix of the training model for the data augmentation could be reduced faster than that of the model that did not use the data augmentation. Also, for the total time spent on training the model data augmentation is 37,537 seconds and for the model without data augmentation is 40,991 for 1,000 iterations. Therefore, it meant that, with the data augmentation, the time that we spent in training the model could be faster as shown in Figure 4.4 and Figure 4.5.  
25  
  Figure 4.4 The time for each iteration of the model that uses the data augmentation.   
  Figure 4.5 The time for each iteration of  the model that does not use the data augmentation.  4.3 Summary Generation and Comparison For the input text as shown in Figure 4.6 below, the model would process and generate the summary shown as in Table 4.7 in comparison with the human summary of the same text.    
26  "If a week is a long time in politics, a year is an eternity. And for Premier Jason Kenney it must seem like a lifetime. A year ago Kenney was celebrating a decisive electoral victory over the NDP after running on a campaign platform that could be summed up in three words: jobs, economy, pipelines. A year later he is dealing with an unprecedented disaster that can be summed up in three words: pandemic, recession, debt.   Alberta may circumvent Health Canada to gain access to COVID-19 tests and drugs, premier says   Kenney must be looking back at April 16, 2019, with more than a little nostalgia. The year began happily enough for him as he used his summer of repeal, among other things, to fulfil election promises scrapping the provincial carbon tax, cutting corporate taxes, reversing the NDP's environmental strategy, and getting tough with the federal Liberal government as part of a 'fight back' strategy. When the economy continued to sputter and jobs continued to disappear, Kenney maintained course. He cut government spending, made plans to balance the budget, introduced business-friendly legislation, championed more private delivery of publicly funded health care, and set up a ""fair deal"" panel to explore how Alberta could have more control over provincial issues including a police force and pension plan. He was doing pretty much what he had vowed to do during the election campaign. ""Promise made, promise kept,"" became his mantra. Even when his popularity began to slip late last year after Albertans realized the cuts would be deeper than advertised, and thousands of civil service jobs would be lost, Kenney pushed ahead, using his ""historic"" one-million-vote election victory as both shield and weapon against his critics. Finance Minister Travis Toews, left, delivers the 2020 budget the Alberta legislature as Premier Jason Kenney looks on. (Jason Franson/THE CANADIAN PRESS) On February 27, he introduced a provincial budget with so many sunny predictions you needed sunglasses to get through it. Oil prices would stabilize at $58 USD a barrel. Employment would surge. The provincial deficit would be wiped out in two years.  ""There are signs that in 2020 we will turn a corner and see real economic growth return to the Alberta economy,"" declared Finance Minister Travis Toews. We turned a corner, all right. And got mugged in a dark alley by COVID-19 and OPEC+. A week after the budget was tabled, Alberta recorded its first case of the novel coronavirus. Oil prices tumbled after Russia and Saudi Arabia got into a bitter fight over production. A year ago Kenney was on top of the world. Today, he must feel like he's been tossed into the pit. 'Sharp downturn' Kenney says this year's budget deficit could hit an unprecedented $20 billion. Unemployment could reach 25 per cent. He predicts the price of Western Canadian Select oil could slump below zero later this month.  A report from the Conference Board of Canada released Wednesday predicted Alberta's economy will continue to lag behind any global recovery later this year. ""The provincial government is in a tight spot,"" says the report. ""It has, appropriately, been deploying fiscal stimulus to mitigate against the sharp downturn in economic activity. At the same time, it must grapple with the evaporation of billions in royalty revenues on which it depends heavily.""   Alberta economy on track for most severe annual decline on record, national report predicts   Kenney is spending billions of dollars, all the while he's losing billions of dollars. He's investing $1.5 billion in the Keystone XL pipeline this year and putting up $6 billion in loan guarantees next year.   Why Alberta is throwing billions behind the Keystone XL pipeline   Ironically, Kenney is on track to become the biggest-borrowing, biggest-spending, most interventionist premier in Alberta history. For a free-market, free-enterprise, small-government conservative, this must be galling.   Canada's oilpatch sheds almost $9B in spending: IHS Markit   The pandemic and downturn are not Kenney's fault but he has made things worse by picking a fight with doctors over compensation and billing practices that led to the Alberta Medical Association launching a $250-million lawsuit against his government last week. This 2015 file photo shows the Keystone Steele City pumping station, into which the planned Keystone XL pipeline is to connect to, in Steele City, Neb. (Nati Harnik/The Associated Press) His government gave itself a black eye by promising to maintain education funding and then days later cutting $128 million that triggered mass layoffs. NDP Leader Rachel Notley, somebody who was premier during her own crisis — the Fort McMurray fire —  has some advice for Kenney: ""Albertans need to know that they can trust Jason Kenney, that he's going to put his own agenda that was developed at a different time in a different place aside in order to come up with the best solutions for them,"" says Notley. ""That, I think, is evidenced by the doctor dispute and the education funding.""   OPINION | Kenney under fire from the front-line workers in the war on COVID-19   Notley says she's trying to work with the government during the crisis. In a private meeting with Kenney a month ago, she suggested he allow NDP members to sit on the government's economic recovery council. Kenney apparently didn't laugh her out of the room. He simply didn't respond. On Wednesday, though, his office sent me an email saying, in so many words, ""no way."" ""The NDP, of course, is welcome to provide its suggestions via the Legislature Assembly or elsewhere,"" said the premier's office. ""We have admittedly been disappointed that at times the NDP seems more interested in playing petty partisan politics."" Both the NDP and UCP can be accused of playing partisan games. That's one thing that hasn't changed in the past year.  This column is an opinion. For more information about our commentary section, please read this editor's blog and our FAQ."  Figure 4.6 The original text input   27  The resulted summaries generated by the model with and without data augmentation can be seen in the Table 4.7.  Table 4.7 Generated summary compared with human summary from the original text  Summary generated by Result Human Ontario health-care workers to hold minute of silence for cleaner who died from COVID-19 Model without data augmentation2520. keeping a big family' of sask woman ' says he was needed to isolate through advocates Model with data augmentation3624 3 long term care homes run by same company report total of 71 deaths from covid 19  As will be seen later, the summary generated by the model with data augmentation, which differs from human generated summary, receives a higher ROUGE score than the one without data augmentation.  4.4 ROUGE Framework As previously mentioned, the ROUGE framework would be used to evaluate the summaries generated by our models in comparison with the human reference summaries. The ROUGE score could be a value between 0 – 1. Tables 4.8 and 4.9 show the ROUGE scores of the two models for the whole dataset which had a total of 6,786 records.   Table 4.8 The ROUGE scores for the model with the data augmentation  Model with data augmentation ROUGE 1 ROUGE 2 ROUGE L Highest score 0.97 0.97 0.97 Lowest score 0.17 0.07 0.17 Average score 0.80 0.76 0.80 SD 0.38 0.44 0.38    28     Table 4.9 The ROUGE scores for the model without the data augmentation  Model without data augmetation ROUGE 1 ROUGE 2 ROUGE L Highest score 0.97 0.97 0.97 Lowest score 0.18 0.07 0.18 Average score 0.58 0.51 0.58 SD 0.39 0.45 0.39  To summarize, the ROUGE framework was used to compare between the reference summaries and the summaries generated by our models. Table 4.7 and Table 4.8 show the variety of the scores because some of the generated summaries had introduced the new words that did not exist in the reference summaries, and this affected the ROUGE scores which were computed based on the words in the reference summaries. Therefore, while the ROUGE framework could be used for text summarization to a certain extent, but we had to be cautious with the abstractive summarization because the summaries were newly generated and not extracted.  4.5 Comparison with Related Work As Hayatin et al. [15] proposed a transformer-based model for text summarization using a number of identical layer encoder decoders (MTDGT-N) which includes tokenization, word embedding, etc. MTDGT-N model was experimented on the same dataset as our work. Therefore, we compare the ROUGE scores of our work and the models of [15] which focused on MTDGT2, MTDGT5, and MTDGT6 (the postfix number is a representation of the number of identical layer encoder decoders) as shown in Table 4.10.  Table 4.10 The comparison between the MTDGT model result and the model result  Model ROUGE-1 ROUGE-2 The transformer C model (TCM) 0.45 0.26 MTDTG2 0.51 0.34 MTDTG5 0.56 0.38 MTDTG6 0.58 0.42 Model with data augmentation 0.79 0.75 Model without data augmentation 0.58 0.51 29    From the above table, we can see that our model using the data augmentation can outperform the MTDGT-N models with the ROUGE-1 as 0.79 and ROUGE-2 as 0.75. It can also be seen that our model has the higher ROUGE scores compared with for straightforward transformer model (the transformer C model).   CHAPTER 5 CONCLUSION AND FUTURE WORK  This research work applied the abstractive text summarization on the dataset of Covid-19 news from the CBC available from the Kaggle online database, with the data augmentation. The abstractive text summarization method, as opposed to the extractive summarization method, generates summaries based on content but does not necessarily use the same words or phrases as in the original documents. Moreover, data augmentation on the rule-based technique was applied before training the model in order to increase the volume and diversity of the dataset and improve the robustness and performance of the model as it prevents the model from overfitting. Also, the attention mechanism was exploited in order to make the model keep the important information from the document and use it to generate the summary.   After the data preparation, we applied the encoder-decoder architecture, LSTM, and attention mechanism to generate summarization. For the evaluation, the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) framework was utilized. ROUGE is a set of metrics and software packages used for evaluating natural language processing tasks such as summarization. It includes several automatic evaluation methods that measure the similarity between reference and candidate summaries. In comparison with the related work by Hayatin et al. [15], our model, with data augmentation, received higher ROUGE scores (ROUGE 1 and ROUGE 2) than their best transformer models scores at 0.79 and 0.75 respectively. One point that should be noted when using ROUGE metrics for evaluating abstractive summarization is that because some of the generated summaries have introduced new words that do not exist in the reference summaries, this affected the ROUGE score which was computed based on words in the reference summaries. Therefore, while the ROUGE framework can be used for text summarization to a certain extent, we have to be cautious with abstractive summarization because the summaries are newly generated and not extracted.  For future research, since our model only focuses on the rule-based technique for the data augmentation, it could be beneficial to experiment with mix-up augmentation or graph-structured technique in order to add diversity to the dataset. Also, the summarization method in this research used only the abstractive text summarization without special parameter tuning or adjusting specific layer based on LSTM. Therefore, it might be a benefit to experiment with the parameter tuning or changing the LSTM layer to Gated Recurrent Unit (GRU) or another layer that can handle the long input sequence. Moreover, the dataset could be expanded to include more recent Covid-19 news data which might be different now in 2022 since vaccines are available and Covid-19 has been changed from pandemic to endemic in some areas. In addition, our research used the ROUGE framework to evaluate the generated summary and candidate summary but it could be beneficial to experiment with a new framework that can be used to evaluate the summaries.