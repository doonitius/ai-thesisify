Machine Learning Classifier to Differentiate the Hissing Behavior of Eastern Honeybee, Apis cerana 
 
The honeybee is a social insect that communicates with its nestmates when engaging in its various activities. Honeybees normally emit signaling sounds to communicate information such as flight orientation, the dance circuit of recruited bees and hissing in response to disturbances. As a result, beehives have been monitored in numerous studies to gather audio data in order to classify the potential status of the beehive. This study demonstrates the technique of a classification model for hissing behavior in Asian cavity-nesting bees, Apis cerana, under different circumstances. The monitoring devices were installed to collect hissing signals related to their defensive behavior. A number of feature extraction techniques in the audio processing were explored, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms. Moreover, both the traditional and deep learning-based classification models were tested including the support vector machine, decision tree, random forest, and convolutional neural networks. The best model was selected based on two competing objectives: the minimum number of parameters and 95% baseline accuracy. The results revealed that the one-dimensional neural network trained with the temporal domain spectrogram that consists of 2 hidden layers, 32 nodes for each layer, and a minimum of 3,737 trainable parameters could provide the best accuracy.   
Convolutional neural networks / Feature extraction / Hissing signal / Machine learning/ Monitoring beehive 
LIST OF TECHNICAL VOCABULARY AND ABBREVIATIONS                            xi  CHAPTER 1   INTRODUCTION                                                                                                    1 1.1 Statement of Problem 1 1.2 Research Objectives 3 1.3 Research Scope 3 1.4 Expected Benefits 3  2  THEORETICAL ISSUE/RELATED WORK                                                        4 2.1 Related Work 4 2.2 Theory 20  3  MATERIAL AND METHODOLOGIES                                                             31 3.1 Audio Data 31 3.2 Proposed Method 31  4 RESEARCH RESULTS AND ANALYSIS                                                              35 4.1 Data Exploration 35 4.2 Feature Extraction 42 4.3 Classification Model 48 4.4 Model Deployment 55 4.5 Model Validation 57  vi  CONTENTS (Cont’d)  PAGE  CHAPTER 5 CONCLUSION                                                                                       60 5.1 Discussion 57 5.2 Conclustion 58  REFERENCES                                                                                                               63  CURRICULUM VITAE                                                                                                70                  vii  LIST OF FIGURES  FIGURE                   PAGE  2.1 Vespa velutina and Vespa mandarinia 5 2.2 Temporal occurrence of hissing signals of Apis cerana jabonica  5 2.3 Comparison of the number of hissing occurrences in four time categories  6 2.4 Sensor positions 7 2.5 Spectrogram during the swarming period 8 2.6 Occurrence of hissing signals during hornet attacks measured by Avisoft software  8 2.7 Oscillograms of hissing signals 9 2.8 Classification results after applying both STFT and SOMs techniques 10 2.9 Comparison of spectrogram resolution between STFT and S-transform  10 2.10 Electronic board with heterogeneous sensors  11 2.11 Accelerometer configuration 12 2.12 Whooping signals in spectrogram form 12 2.13 Results of supervised clustering by using PCA and LDA 12 2.15 Hardware installation 14 2.16 Result comparison for Mel frequency band, MFCC and mean normalized   frequency 15 2.17 Area under the curve score (AUC) of SVM results  15 2.18 Scatter plot of SVM analysis  16 2.19 Spectrogram of CWT and DWT  17 2.20 Performance from each classification model with different mixtures  18 2.21 Procedure for instantaneous spectra strategy  18 2.22 Procedure for spectral evolution strategy  19 2.23 Hamming, Hanning and Kaiser window functions 21 2.24 Example of zero crossing 22 2.25 Process of MFCCs 24 2.26 Sigmoid/Logistic activation function graph 25 2.27 Hyperplane of 2D and 3D data in the support vector machine 26 2.28 Convolutional neural networks architecture 28 2.29 Confusion matrix 29 viii  LIST OF FIGURES (Cont’d)  FIGURE                   PAGE  2.30 Example of ROC and AUC 30 3.1 Smart hive architecture 34 4.1 Occurrence of hissing signals during the period of observation 36 4.2 Occurrence of hissing signals for each hour 36 4.3 Occurrence of hissing signals during the daytime and night 36 4.4 Example of spectrogram produced from sliding window with different intervals 37 4.5 Example of 1-second temporal domain data from normal class 38 4.6 Example of 1-second temporal domain data from hissing class 38 4.7 Example of 1-second temporal domain data from highly active class 39 4.8 10 intervals of 1-second temporal domain data from normal class 39 4.9 10 intervals of 1-second temporal domain data from hissing class 40 4.10 10 intervals of 1-second temporal domain data from highly active class 40 4.11 Grayscale intensity histogram from the normal class spectrogram 41 4.12 Grayscale intensity histogram from the hissing class spectrogram 42 4.13 Grayscale intensity histogram from the highly active class spectrogram 42 4.14 Results from the fast Fourier transform 43 4.15 RMSE and waveform graph of normal class 43 4.16 RMSE and waveform graph of hissing class 44 4.17 RMSE and waveform graph of highly active class 44 4.18 Method of creating the spectral centroid feature 45 4.19 Example of a spectral centroid feature graph 45 4.20 Example of a spectral centroid feature in tabular form 45 4.21 Example of a spectral centroid feature in 2-dimentional PCA 45 4.22 Scatter plot of spectral centroid feature in 2-dimentional PCA 46 4.23 12 bins Mel spectrogram. 47 4.24 Confusion matrix of 35th 1DCNNs model 51 4.25 Receiver operating characteristic of 34th 1DCNNs model 52 4.26 Learning kernel generated by 1st convolution layer of 35th 1DCNNs model 52 4.27 Histogram of convoluted results from normal class 53 4.28 Convoluted results of the normal class from 1st layer 1DCNNs model 54 ix  LIST OF FIGURES (Cont’d)  FIGURE                   PAGE  4.29 Histogram of convoluted results from hissing class 54 4.30 Convoluted results of hissing class from 1st layer 1DCNNs model 54 4.31 Histogram of convoluted results from highly active class 55 4.32 Convoluted results of hissing class from 1st layer 1DCNNs model 55 4.33 Process of dividing the 10-second audio data for analysis with 35th 1DCNNs 56 4.34 Structure of video results table 56 4.35 Grafana dashboard 57 4.36 Beehive for gathering data during model validation 58 4.37 Example of recording a video from within the hive 58 4.38 Example of recording a video from outside the hive 58        x  LIST OF TABLES  TABLE                   PAGE  2.1 Results of the linear mixed model identifying the relationships between hissing onset time and ambient parameters 6 2.2 System performance benchmark of [24] 11 2.3 Performance from both deep learning and machine learning 13 2.4 Performance of classification models from two test datasets [20] 14 2.5 Feature selection results [35] 16 2.6 Results of area under the ROC curve for the classification models [35] 16 2.7 Summary of the feature extraction approaches 20 2.8 Summary of classification model 20 3.1The features and classification models in the experiment 33 3.2 Independent and controlled parameters in different classification models 34 4.1 Occurrence of hissing signals caused by stimuli 35 4.2 Proportion of dataset for each class 37 4.3 Mean and standard deviation of temporal domain from each class 38 4.4 Statistical parameters of Mel spectrogram from each class 41 4.5 Features used for training and testing machine learning models 47 4.6 Performance of support vector machine model based on different kernels 48 4.7 Performance of decision tree and random forest models 48 4.8 Importance features of the random forest model 48 4.9 Mel frequency and FFT frequency range for each Mel filter-bank 49 4.10 Structure and accuracy of the 1DCNNs models 50 4.11 Structure and accuracy of the 2DCNNs models 51 4.12 Statistical parameters from convoluted results from each class 53 4.13 Performance of 24th 1DCNNs model based on validation dataset 59     xi  LIST OF TECHNICAL VOCABULARY AND ABBREVIATIONS  AUC  = Area under the ROC Curve  BW  = Bandwidth CNNs  = Convolutional neuron networks Hz  = Hertz HHT  = Hilbert–Huang transform KNNs  = K-Nearest-Neighbors LDA  = Linear discrimination analysis LPC  = Linear predictive codding LR  = Logistic regression MFCCs = Mel-frequency cepstral coefficients PF  = Peak frequency PCA  = Principal component analysis POLS  = Procedure of optimal linear smoothing RF  = Random forest RGB  = Red green blue  RMSE  = Root mean square energy ROC  = Receiver operating characteristic curve RVF  = Root variant frequency s  = Second SC  = Spectral centroid SE  = Square error SD  = Standard deviation SOMs  = Self-Organizing Maps STFT  = Short-time Fourier transform SVM  = Support vector machine WSN  = Wireless sensor network 1DCNNs = 1-dimensional convolutional neuron networks 2DNCCs = 2-dimensional convolutional neuron networks 
CHAPTER 1 INTRODUCTION   1.1 Statement of Problem Honeybees are sophisticated social insects that live together in colonies of more than 40,000 individuals, consisting of a queen bee, thousands of worker bees (female bees) and drones (male bees). A direct benefit of honeybees to humans is that they produce raw materials used in various health and therapy application products, including honey, pollen, wax, royal jelly, and propolis. The value of the worldwide honey market alone was estimated at $7.05 million in 2016 [1]. However, the most explicit benefit that honeybees provide for humans is their ability to pollinate agricultural crops, resulting in increased productivity [2, 3]. Indeed, more than 30% of global food crops are pollinated by honeybees, at an estimated value of €155 billion annually. For this reason, honeybees are regarded as one of the most active pollinator animals worldwide [4].   Honeybees communicate with each other when they are engaged in their various activities. Numerous researchers have attempted to study their behavior and communications by installing non-invasive monitoring devices in the beehives. The pioneering era of research into their communications started with the legendary discovery of their figure-of-eight movements and waggle dance, which is a democratic decision-making method used by honeybees to recruit their nestmates to accompany them to a food source [5]. However, [6] it was first established in the 17th century that the honeybee also emits sound signals that are audible to the human ear. As a result, this discovery marked the initial era of study of signal communication in honeybees.  Associated with defending the honeybee colony, the hissing signal usually occurs within the broadband frequency of 300–3,600 Hz [6] when the nest is disturbed by potential predators, such as hornets, wasps, weaver ants, and insectivorous birds. These hissing signals are also sometimes known as the shimmering signal. At the onset of hissing, it is produced by some worker bees by moving their abdomens dorsoventrally to create a ripple effect that is synchronously harmonized [7]. Then, in response to this initial hissing from the first honeybee group, other worker bees are encouraged to join in and perform the hissing signal together. Finally, when the entire honeybees colony jointly produces this sound simultaneously, it creates a short sound (0.5-1.0 s) [6] with high amplitude  2  similar to crashing waves that is mainly used to intimidate the invaders. Moreover, Apis cerana also emits more stop signals, single pulses within the 300-400 Hz range, when they are being attacked by a natural enemy such as hornets (Vespa velutina) and large hornets (Vespa mandarinia) [8].  Various cases of monitoring beehives mainly involved the recording of audio sounds made by either the Apis mellifera or Apis cerana using the mean of microphones [9, 10], vibration sensors [11], or doppler sensors [12] installed inside or outside the beehive. The acoustic data were usually processed through a signal processing technique, then performed with feature extraction to create a set of appropriate parameters for training the classification models to identify different states in the beehive, including the presence or absence of the queen [13, 14, 15], the detection of a swarm [16, 17], the presence of varroa destructor [18], orientation flights [19], and honey being stolen [19].  Both feature extraction techniques and classification models have been applied in various studies for monitoring beehives. [20, 21, 22] presented the Mel-frequency cepstral coefficients (MFCCs), while some authors also extracted the MFCCs together with the Mel spectrogram [23]. In addition, the Wavelet Transform and the Hibbett Huang Transform (HHT) have also been applied for beehive monitoring [20]. To compare the resolution of the image-based feature, [15] it is suggested that the resolution of the spectrogram from the S-Transform be greater than the short-time Fourier transform (STFT). In [23], acoustic data were extracted using up to 5 features, including MFCCs, Chroma STFT, Mel spectrogram, STFT spectral contrast, and Tonnetz. For machine learning models to classify the status of the beehive, some researchers started from traditional machine learning including logistic regression [13, 23], K-nearest neighbors [23], random forest [23, 24], and the support vector machine [22, 24, 25]. On the other hand, convolutional neural networks (CNNs) models were constructed to learn the patterns of pixels from the image-based feature [23, 25]. In addition, [23] also used basic convo-1D neural networks training with temporal domain data.  In this study, we mainly aim to experiment and observe the production of hissing signals from Apis cerana in order to understand their defensive behavior and demonstrate which feature selection techniques and machine learning models are most effective for detecting the hissing signals. Our results could not only provide a comprehensive understanding of 3  hissing behavior and other behaviors in the beehive, but also be useful for entomologists and engineers who are involved in this field.  1.2 Research Objectives 1. To observe the characteristics and patterns of hissing signals in the defensive behavior 2. To demonstrate the feature extraction and machine learning techniques that are most appropriate for detecting the hissing signal  1.3 Research Scope 1. The dataset in this research was collected from experiments conducted at the Native Honeybee and Pollinator Research Center, King Mongkut's University of Technology Thonburi, Ratchaburi campus. 2. This research mainly focusses on the production of hissing signals by Apis cerena.  1.4 Expected Benefits 1. The gathered dataset in this research will be useful for other researchers who are interested in signal processing and honeybee signal production. 2. The results from this thesis can be applied to non-invasive hive monitoring systems. CHAPTER 2 THEORETICAL ISSUE/RELATED WORK   This chapter reviews the previous literature from this field and the theoretical background relating to the components in the process of the defensive behavior in Eastern honeybee nests, feature extraction techniques, and the fundamentals of machine learning models. The knowledge accumulated in this chapter will aid in understanding this thesis.   2.1 Related Work 2.1.1 Hissing signal emissions of hives during predator disturbances Honeybees communicate with their nestmates while engaging in their various activities by using symbolic language, chemical pheromones, and a variety of sound signals. The vibroacoustic signals emitted by honeybees are produced from gross body movements, wing movements, high-frequency muscle contractions without wing movements, and pressing the thorax against the substrates or another bee [26, 26, 27]. In general, honeybees usually communicate in the range of 300–600 Hz [6].  Hissing is one of the acoustic signals emitted by multiple worker bees in response to disturbances such as the colony being knocked or poked, the branch supporting the colony being tapped [6], or natural enemies being discovered scouting or attacking the colony (Vespa velutina, Vespa mandarinia) as shown in Figure 2.1. This hissing is sometimes called a Shimmering signal [6]. Historically, hissing behavior was firstly observed in Apis dorsata by Butler in 1954 [28]. Moreover, hissing behavior has also been observed in different honeybee species, including Apis cerana [29], Apis florea [30], and Apis mellifera [10]. The hissing signal is produced by several worker bees moving their abdomens dorsoventrally to create a ripple effect that is synchronously harmonized [7]. In response to the hissing from the first honeybee group, the other worker bees are encouraged to join in and perform the hissing signal together. Finally, when all of the honeybees in the colony jointly produce this sound simultaneously, it creates a short sound (0.5-1.0 s) with the broadband frequency of 500–5,000Hz in Apis florea or 300-3,600Hz in Apis cerana [6].    5    Figure 2.1 Vespa velutina and Vespa mandarinia  However, one previous study [31] demonstrated that Apis cerana japonica can hiss even without the presence of any obvious threatening stimulus, such as hornets or mammals near the colony. The authors installed mini-microphones connected with IC recorders located inside 6 different beehives and recorded the hives 24 hours hours a day for XX days. The sampling rate and resolution of the recordings were 44.1kHz and 16 bits, respectively. The audio data were processed using the Adobe Audition CC. The researchers defined a hissing signal as an acoustic signal within a frequency range of 300-3600Hz and with a continuous duration greater than 0.5 seconds. The results showed that Apis cerana japonica hissed only during the daytime but more 100 times every day at a mean frequency of 402.7 and SD of 223.6. The mean dominant frequency of hissing was 755.1 Hz (SD: 236.7 Hz). The mean duration of the hissing was 1.51 s (SD: 0.63 s). 
 Figure 2.2 Temporal occurrence of hissing signals of Apis cerana jabonica [31]  Moreover, another study [32] also conducted 24-h sound recordings in 2 Apis cerana japonica beehives for 40 days in addition to recording ambient temperature, humidity, and solar radiation through sensors. The authors sampled 16 days and identified hissing 
6  signals based on the method described in [31]. In the experiment, each day was separated into four time categories: dawn, day, dusk, and night. The researchers defined the period of 30 mins before and after sunrise as dawn and defined the period of 30 mins before and after sunset as dusk. The time between dawn and dusk was defined as day, while the time between dusk and dawn was defined as night. The results revealed that the mean number of hissing occurrences in one day was 295.75 time with SD of 134.45. Figure 2.3 shows a comparison of the number of hissing occurrences in the four different times categories. Most hissing signals occurred during the dawn period, while no hissing was emitted during the night period. The authors suggested that changes of season did not influence the frequency of hissing signals by Apis cerana japonica because there was no correlation between the total number of hissings in a day and mean temperature (Pearson correlation: r=−0.17, p>0.05). In addition, Table 2.1 demonstrates how the onset of hissing could be predicted by sunrise and solar radiation intensity with the Pseudo R2 value of the model being 0.99.  
 Figure 2.3 Comparison of the number of hissing occurrences                                            in four times categories [32]  Table 2.1 Results of the linear mixed model identifying the relationships between                   hissing onset time and ambient parameters [32]  Covariate Coefficient SE T statistics p value Intercept 6.71 x 10-3 1.04 x 10-2 0.65 0.53 Sunrise time 9.74 x 10-1 3.49 x 10-2 27.93 <0.001 Temperature -4.05 x 10-5 1.25 x 10-4 -0.32 0.75 Intensity of solar radiation 4.93 x 10-3 6.00 x 10-4 8.23 <0.001 Humidity -9.31x10-5 6.97 x 10-5 -1.34 0.19 
7  2.1.2 Automated classification of honeybee signals using acoustic analysis In 2008, [9] presented a method of predicting the swarming phenomenon, which is the process whereby the queen and her workers leave the beehive. In the paper, three beehives of Apis Mellifera Ligustica were monitored for 270 hours by installing microphones inside the beehives together with a temperature and humidity sensor between the looms as shown in Figure 2.4 where (a) is the position of the humidity and temperature sensor and (b) is the position of the microphone. All sensors were covered by a special net to protect against propolization from the bees. The audio data were first recorded on 8 channels with 16bit resolution and a sampling rate of 2kHz, and then analyzed via Matlab software. Swarming events were identified by an increase in the power spectral density at about 110 Hz. Nine swarming activities happened during the recordings. The duration of the swarms ranged from 13.53 to 56.23 mins with a mean duration of 35 mins and S.D. of 15.3 mins. In addition, the spectrogram revealed the quick change in the frequency content from 150 Hz up to 500 Hz when the swarming occurred, as shown in Figure 2.5. Moreover, the authors also discovered that there was a rise in temperature from 33 to 35C at the beginning of a swarming period, with the temperature then dropping to 32C and the humidity also falling when the honeybees started to evacuate the beehive.   
 Figure 2.4 Sensor positions [9]  
8  
 Figure 2.5 Spectrogram during the swarming period [9]  [10] focused on the sound emitted by honeybees (Apis mellifera cypria) in nine beehives during disturbance from the Oriental hornet (Vespa orientalis), and first observed that the honeybees could produce sounds across a wide frequency spectrum with the dominant frequency being around 6kHz with several high order harmonics with were able to reach up to 15-16 kHz. In their study, digital microphones with 16bit resolution and a 44.1 kHz sampling rate were installed at the beehive entrance. Hissing signals were detected by computing the sliding short-time Fourier transform (Hamming window duration=0.023 s, 87.5% of overlap) and then visualizing the results as a spectrogram by using Avisoft software as shown in Figure 2.6. Since the fundamental frequency was difficult to detect, the authors estimated it by measuring two successive harmonic peaks fn and fn+1 and then computing f0 = fn+1-fn=(n+1)f0-nf0. The hissing produced during the hornet attacks lasted for 0.622 ± 0.708 s. In Figure 2.7, oscillograms of hissing reveal a regular succession of pulses while those of buzzing show a sinusoidal waveform.  
 Figure 2.6 Occurrence of hissing signals during hornet attacks                                 by using the Avisoft software [10] 
9  
 Figure 2.7 Oscillograms of hissing signals [10]  In 2011, [33] presented a method for predicting swarming events several days before they occurred. The authors gathered audio data from twelve beehives every night from midnight to 3:00 a.m., and then separated each recording sound into one-second-long fragments. After that, each fragment was first normalized and them smoothed using the Procedure of Optimal Linear Smoothing (POLS) with a Gaussian kerel and then applied to the integrated sequence of original audio data. Two years later, [15] presented an approach for differentiating queenright from queenless beehives. The acoustic data in their study were recorded from four separate beehives (2 Apis mellifera ligustica and 2 Apis mellifera carnica). Both S-transform and short-time Fourier transform (STFT) were applied as feature extraction techniques. Moreover, self-organizing maps (SOMs) were used to represent high dimensional data to lower dimension, which were then adapted for visualization to classify different states (Figure 2.8). When comparing the resolutions from different spectrograms, the authors reported that the S-transform outperformed the STFT, as shown in Figure 2.9.  
10  
 Figure 2.8 Classification results after applying both STFT and SOMs techniques [34]  
 Figure 2.9 Comparison of spectrogram resolution between STFT and S-transform [15]  In 2014, [24] demonstrated a classification machine learning model for remotely detecting the presence of varroa mite infestations, which is one of the most dangerous honeybee parasites. In this study, four statistic indicators were extracted from the 0-4kHz frequency range: peak frequency (PF), spectral centroid (SC), bandwidth (BW), and root variance frequency (RVF). These four features were applied with principal component analysis (PCA) to reduce the data dimensionality, and to select the best parameters. Both support vector machine (SVM) and linear discriminant analysis (LDA) were used as classification models. The results from Table 2.2 show both the LDA or SVM models were able to perform well using only one feature (SC) to differentiate between infected and healthy beehives at a 95% accuracy rate. One year later, [4] applied wireless sensor 
11  network (WSN) technology to monitor a beehive by collecting information from both within the beehive and its surrounding area. As shown in Figure 2.10, heterogeneous sensors were installed, including microphones and temperature, humidity, CO2, NO2, O2, dust and acceleration sensors. Moreover, an infrared camera and a thermal camera were also deployed. The results show that the oxygen levels in the beehives always remained constant despite the changing conditions; weather changes had a strong influence on the humidity inside the beehive. The authors also revealed that the pollen particles affected the dust sensors. However, the energy consumption of these devices needs to be improved.  Table 2.2 System performance benchmark of [24]  Methods Computational Time Accuracy (%) SC, PF, RVF, BW ~ 20 s 96 % SC, PF. RVE ~ 16 s 96 % SC, PF ~ 14 s 95 % SC ~ 13 s 95 %  
 Figure 2.10 Electronic board with heterogeneous sensors [4]  In 2017, [34] presented an automated in-situ noninvasive method of monitoring honeybee vibrations called “whooping signals”. In their study, the vibrations from 2 hives were recorded by ultra-sensitive accelerometers embedded in the honeycomb located at the heart of the honeybee beehives as shown in Figure 2.11. The recorded whooping signals were then processed and represented in spectrogram form as shown in Figure 2.12. Each spectrogram was mapped against the spectrogram of a template pulse and then the pulsed signal was computed from the cross-correlation product and the Euclidean distance 
12  function. The authors discovered that the spectrograms of the honeybee whooping signals and those of the vibrational pulses caused by a drop of rain falling were highly similar. To distinguish between a whooping signal and a non-whooping signal, both PCA and LDA were applied. Figure 2.13 shows the outcomes of the supervised clustering of whooping signals (red cloud) and rain droplets (blue clouds) for discrimination, for which overlapping is less than 1%. 
 Figure 2.11 Accelerometer configuration [34]  
 Figure 2.12 Whooping signal in spectrogram form [34]  
 Figure 2.13 Results of supervised clustering using PCA and LDA [34] 
13  In 2018, [23] demonstrated their feature extraction and numerous machine learning models to differentiate 3 classes of sounds: honeybee sound, background sound, and cricket chirping sound. In their study, four microphones were deployed at the entrance of six Langstroth beehives. The long acoustic data were trimmed down to 2-second recordings. The authors extracted up to six features—MFCCs, chroma short-time Fourier transform, Mel spectrogram, short-time Fourier transform, spectral contrast, and Tonnet—for four disparate traditional machine learning models: linear regression (LR), K-nearest neighbors (KNNs), SVM, and random forests (RF). In addition, they also performed basic one-dimensional CNNs on the raw acoustic data called RawConvNet, and two-dimensional CNNs on an RGB spectrogram called ConvNets. Their remarkable results in Table 2.3 revealed that the one-dimensional CNNs model with a filter size of 80 was able to provide an average degree of accuracy of 95.21%.  Table 2.3 Performance from both deep learning and machine learning on BUZZI                 validation dataset [23]  Model Validation Accuracy (%) RawConvNet (n=80) 95.21 ConvNet1 (n=80) 74.00 ConvNet2 (n=80) 78.08 ConvNet3 (n=80) 75.04 Logistic regression 94.60 Random forests 93.21 KNN (n=5) 85.47 SVM OVR 83.91  [20] presented the method and an overview of the hardware infrastructures used in a beehive monitoring system used to detect the presence of a queen bee. In their study, sensitive microphones able to detect sound in the frequency range of 20-2,000 Hz were installed inside the beehives as shown in Figure 2.14. Audio data were gathered from normal beehives and orphaned beehives, with feature extraction performed by using linear predictive coding (LPC) with 14 coefficients, followed by training with the SVM model and t-distributed stochastic neighbor embedding (t-SNE). The results presented in Table 2.4 show that the performance of the proposed approach achieved overall accuracy of 91.76%.   14  
 Figure 2.14 Hardware installation [20]  Table 2.4 Performance of classification models from two test datasets [20]  Name Samples Old queen New queen Error (%) Test data old queen 72 68 4 5.55 Test data new queen 183 17 166 9.23  In 2019, numerous publications proposed various techniques based on sound analysis. [25] used the acoustic data of beehives collected from the NU-Hive project [20]. In their paper, the authors investigated the potential of SVM and CNNs methods with different combinations of features and parameters for exploiting MFCCs, Mel spectrograms, and the Hilbert Huang Transform (HHT) as the features to determine the presence of the queen bee in a hive. Figure 2.15 presents a comparison of the three extracted features in relation to a queenless beehive for a time interval of 10 minutes. From this comparison, it can be seen that the HHT-based method is the most appropriate approach to expressing the frequency behavior of the analyzed beehives. The integration of the SVM model, MFCCs feature, and HTT feature produced the highest average for the area under the curve score (AUC) at approximately 0.94 (Figure 2.16). In the same year, [35] also presented a multiclass classification model to identify 3 queen states: queenright, queenless colony with queen removed, and queenless colony with low population. In this work, the feature extraction was conducted through the 12-channel MFCCs method. The selected features are show in Table 2.5. A Lasso logistic regression model was constructed along with one feature vs all strategy. The results reveal an average AUC of 99% (Table 2.6). In addition, the authors also suggested that using scatter plots of SVD can provide visual evidence for examining the results as shown in Figure 2.17. Regarding a comparison of the spectral content based on the spectrogram, [36] five feature exaction approaches are proposed to 
15  indicate the presence of a queen bee. The five features consist of frequency spectrum, MFCCs with thirteen coefficients, HHT, continuous Wavelet transform (CWT), and discreate Wavelet transform (DWT). Figure 2.18 reveals that both CWT and DWT show the best performance, allowing a clear distinction between a normal beehive and an orphaned beehive.  
 Figure 2.15 Result comparison among Mel frequency band, MFCC                                   and mean normalized frequency [25]  
 Figure 2.16 Area under the curve score (AUC) of SVM results [25]       
16  Table 2.5 Feature selection results [35]  Class MFCCs Mean Median Queenright colony 3, 10, 12 5, 6, 9 Queenless colony (removed) 1, 12 2, 3, 5, 6, 8, 10 Queenless colony and low population 9, 12 1, 2, 5, 9, 10  Table 2.6 Results of area under the ROC curve for the classification models [35]  Class Queenright Queenless (removed) Queenless AUC (dataset 70/30) 0.994 0.976 0.997 AUC (dataset 50/50) 0.999 0.996 0.999   
 Figure 2.17 Scatter plot of SVM analysis [35] 
17  
 Figure 2.18 Spectrogram of CWT and DWT [36]  In 2020, [37] used acoustic data from the Open-Source Beehives Project to create a classification model for monitoring bee swarm activity. The audio data features were extracted by the MFCCs and LPC techniques, and then both the Hidden Markov Model (HMM) and Gaussian Mixtures Model (GMM) were applied. The difference types of classifiers and features were tested to find the best combination. From figure 2.19, it can be seen that the best performance of approximately 82% was achieved with the system based on both the 15s HMM acoustic model and the cepstral mean normalization of MFCC (MFCC_CMN). Another study that used acoustics for identifying swarming events placed accelerometers in the heart of honeybee hives to gather vibration data [38]. To predict honeybee swarming, two strategies were presented: 1) the discrimination of instantaneous spectra (one-hour of vibrational data) and 2) the discrimination between spectral evolution (10 days of vibrational data). Figure 2.20 illustrates the first strategy procedure. Each hour-long averaged spectrum is compared using the cross-correlation product (CCP) to a pair of discriminant function curves, as determined by the discriminant function analysis (DFA) algorithm. On the other hand, Figure 2.21 shows the second strategy procedure. The FFT were performed on the time course of the magnitude of all 
18  the uploaded spectral frequencies for each day between midnight and 5:00 AM, yielding 2DFT (two-dimensional Fourier transform) images. Then further calculations were conducted for each pixel of the series of 2DFTs found in the preceding days. Finally, each 3DFT (three-dimensional Fourier transform) was compared to a pair of the discriminant functions, as determined by the DFA algorithm. After comparing the two strategies, the results from the first approach, which is based on one-hour of vibrational data, show high accuracy and fewer false positives.  
 Figure 2.19 Performance from each classification model with different mixtures [37]  
 Figure 2.20 Procedure for instantaneous spectra strategy [38] 
19  
 Figure 2.21 Procedure for spectral evolution strategy [38]  Based on the previous related research, we can summarize the extraction techniques and machine learning models as shown in Tables 2.7 and 2.8, respectively. The spectrogram is frequently used in numerous studies. The three most used classification models are SVM, LR and CNNs.         
20  Table 2.7 Summary of feature extraction approaches  Approach Reference Spectrogram based [9], [10], [23], [25], [34], [38] MFCC analysis [23], [25], [35], [36] Principle component analysis [24], [34] Linear discrimination analysis [24], [34] Hibbert Huang transform [25], [36] Mel filter-bank [23], [25] Peak frequency [24] Spectral centroid [24] Spectral contrast [23] Bandwidth [24] Root variance frequency [24] Tonnetz [23] Linear predictive coding analysis [20] Wavelet [36]  Table 2.8 Summary of classification models  Classification model Reference Support vector machine (SVM) [20], [24], [23], [25] Logistic regression (LR) [23], [35] Convolutional neural networks (CNNs) [23], [25] K-nearest neighbors (KNNs) [23] Random forests (RF) [23] Hidden Markov model [37] Gaussian mixture model [37] Discriminant function analysis [38]   2.2 Theory 2.2.1 Feature Extraction Techniques The machine learning model requires robust and discriminatory features in order for it to be able to learn accurately and quickly. Models are not trained directly from the whole dataset but use some important features that represent the characteristics of the data. This section explains in brief the feature extraction techniques, including time domain features, frequency domain features, cepstral domain features, and image-based features.     21  2.2.1.1 Time domain features An audio signal is a time-series that always changes with respect to time. To analyze this signal, the windowing techniques have been presented. The window function is slid to multiply through a signal of interest, from which it can obtain a signal that is a subset of all signals. Gibbs phenomenon is the distortion caused by applying a rectangular window to a signal. To handle this problem, numerous window functions with smooth curve have been presented such as the Hamming, Hanning, and Kaiser windows as shown in Figure 2.22.  
 Figure 2.22 Hamming, Hanning and Kaiser window functions  Zero crossing rate (ZCR) is the rate of changing the sign of the signal or the number of times the signal crosses the zero level during the interested frame, as shown in Fig 2.23. The ZCR can be calculated from equation 2.1 when N is the frame length. ZCR is applied in many applications including music/speech discrimination [39, 40], music genre classification [41], voice activity detection [42], and vowel detection and analysis [43]. 
22  
 Figure 2.23 Example of zero crossing  𝑧(𝑖)= 12𝑀∑{𝑠𝑔𝑛(𝑥𝑖(𝑛))−𝑠𝑔𝑛(𝑥𝑖(𝑛))}𝑁𝑛=1  (2.1)  The loudness, volume, or energy of an audio signal is one of the features that is easy to extract and understand. The energy of a signal corresponds to the total magnitude of the signal as shown in equation 2.2. Another approach is root-mean-square energy (RMSE), which is shown in equation 2.3, when N is the number of the data point. These techniques can be found in speech/music classification [44], speech segmentation and acoustic scene classification [45].   ∑|𝑥(𝑛)|2𝑛      (2.2)  √∑(𝑚𝑒𝑎𝑛(𝑥)−𝑥𝑖)𝑛𝑛𝑖=1      (2.3)  2.2.1.2 Frequency domain features To analyze a signal in a frequency component, the temporal-domain signal is converted into a frequency domain by using the Fourier transform technique. Normally, signals in nature are in continuous form. Nevertheless, when those signals are stored in the form of analogs, the signal characteristics will be discrete signals because they were sampled. Discrete-time signal analysis mostly uses a discrete-time Fourier transform algorithm as shown in equation 2.4. However, if equation 2.4 is used to transform the signal from the 
23  time domain to the frequency domain, it is found that the processing speed is extremely slow at O (n2) where n is the amount of data.  𝑋1𝑇(𝑓)=𝐹{∑𝑥[𝑛]∙𝛿(𝑡−𝑛𝑇)}∞𝑛=−∞   (2.4)  Fast Fourier transform (FFT) is a new method that has been introduced. It can be calculated quickly at O(nlogn) by factoring the DFT matrix into a product of sparse. Nowadays, there are many FFT techniques, such as the prime-factor FFT algorithm, Bruun's FFT algorithm, Rader's FFT algorithm, Bluestein's FFT algorithm, and hexagonal fast Fourier transform.  Discrete-time short-time Fourier transform (STFT) STFT is one of the most well-known techniques used for signal analysis. It follows the principle of dividing the data in the time domain into individual frames of the same size. However, the frames overlap each other to reduce artifacts at the boundary, after which, the fast Fourier transform is applied to each frame as shown in equation 2.5. Finally, all the transformed results will be merged.                𝑆𝑇𝐹𝑇{𝑥[𝑛]}(𝑚,𝑤)=𝑋(𝑚,𝑤)= ∑[𝑛]𝑤[𝑛−𝑚]𝑒−𝑗𝑤𝑛∞𝑛=−∞𝑥  (2.5)  2.2.1.3 Cepstral domain features Mel-frequency cepstrum coefficients is a well-known technique that has been used in speech recognition [45], speech enhancement [46], speaker recognition [47], music genre classification [48], and music information retrieval [43]. As outlined in Figure 2.24, MFCCs can be derived in 4 steps as shown below: 1) Apply the signal with FT. 2) Map the power of the spectrum obtained in the previous step into the Mel scale as shown in equation 2.6. 3) Take the logs to decompose the data. 4) Take the discrete cosine transform of each result from the previous step.  24  
  Figure 2.24 Process of MFCCs  𝑀𝑒𝑙(𝑓)=2595𝑙𝑜𝑔𝑙𝑜𝑔 (1+𝑓700)    (2.6)  2.2.1.4 Image-based features The spectrograph is a visual presentation that shows signal information in terms of signal strength, which has a relationship between the time and frequency of the signal. In addition to the spectrogram name, other names such as sonographs, voiceprints, or voicegrams may be found [49]. The dimensions of the most-used spectroscopy are 2D. The horizontal axis is the time, while the vertical axis is the frequency, and the color intensity indicates the magnitude of the signal during that time. Mathematically, the magnitudes of each frequency can be calculated from equation 2.7 when t is time and w is the window width of the STFT. However, the tradeoff of the spectrogram is the optimum window size [49]. A large window will give a great resolution for time but is not good in terms of frequency detail. On the other hand, a small window size will give better resolution for frequency.                         𝑠𝑝𝑒𝑐𝑡𝑟𝑜𝑔𝑟𝑎𝑚(𝑡,𝑤)=|𝑆𝑇𝐹𝑇(𝑡,𝑤)|2    (2.7)    
25  2.2.2 Classification Machine Learning Models This section provides brief details about the supervised traditional machine learning models that were mentioned in related work. These models include logistic regression, support vector machine, K-nearest neighbors, random forests, and convolutional neural networks. Logistic regression is one of the most widely-used classification models for binary prediction. The sigmoid/logistic activation function (Figure 2.25) is applied to convert the outcome to a probability score between 0 to 1 by using equation 2.8. When the value of z increases to positive infinity, then the outcome value will be close to 1. However, while it has negative infinity, the outcome value becomes close to 0. Normally, the decision boundary is set at 0.5. If the probability score of this function is more than 0.5, then the model classifies the predicted class as 1 label, while if it is less than 0.5, then the model classifies the predicted class as 0 label. However, there are some major assumptions for this model including: 1) The dependent variable should be binary. 2) Average or no multicollinearity are required between the independent variables. 3) There should not be any outliers within the data [50]. 
 Figure 2.25 Sigmoid/Logistic activation function graph  𝑦= 11+𝑒−𝑍     (2.8)  The aim of the support vector machine classification model is to construct the optimal decision boundaries called hyperplanes. The dimension of the hyperplane depends on the number of input features. As shown in Figure 2.26, if the number of input features is two, the hyperplane is a line, while if the number of input of features is three, the hyperplane 
26  results in a two-dimensional plane. In this model, we determined the two output classes as 1 and -1. To maximize the margin between data points and the hyperplane, the loss function is applied. In addition, the regularization parameter can be added to balance the margin maximization and cost function. Regrading updating the weight of the parameters, the gradients are calculated from partial derivatives. However, the disadvantages of the support vector machine model are firstly that it is not suitable for large datasets because the distance between data points and hyperplane is highly computed and secondly, this model does not perform well when the data include more noise such as large amounts of overlapping data.  
 Figure 2.26 Hyperplane of 2D and 3D data in the support vector machine  K-nearest neighbors is an algorithm that uses the concept of distance calculations from unknown instances with other K known instances. K-NN is a simple algorithm that has been used in many tasks such as computer vision and gene expression analysis. Equation 2.9 is called the Makowski distance function, which is usually applied for calculating the distance between instances in KNN. The p value in the formula can be adjusted to obtain different distance functions. Setting p to 1 is known as the Manhattan distance function, while setting p to 2 is called the Euclidean distance function. The k parameter, which is the number of neighbors to consider when running this model, must be optimized to reduce noise data and localize anomalies. However, this model requires a large memory to store all the training data and uses high computation when there is a large amount of datasets or features. It is also sensitive to irrelevant features and unnormalized data. 
27  (∑|𝑥𝑖−𝑦𝑖|𝑛𝑖=1)1/𝑝     (2.9) Random forest (RF) consists of many individual decision trees that operate based on a bagging algorithm. The RF model can provide higher accuracy because each individual tree in the random forest splits a class prediction and the class with the majority of votes becomes the model’s prediction. Each tree model in the RF model randomly selects data. To construct an individual tree, entropy (E) is calculated from equation 2.10 to measure the uncertainly of information when p is the frequentist probability of a class i. In general, entropy is in the range of 0 to 1. A high level of entropy that is close to 1 means the dataset has high uncertainly. In terms of selecting the feature for distinguishing the data, the information gained from equation 2.11 is applied by subtracting the entropy of the target class (Y) given feature (X) from the entropy of the target class. The feature that provides the highest information gain value becomes the split node. Both steps will continue until the data cannot be divided or until a limited parameter has been reached, such as maximum node or maximum deep. The advantages of the RF model are that it reduces the overfitting problem and variance problem, and improves the accuracy. Conversely, the RF model requires more time, computational power, and resources for training compared to the decision tree. Moreover, if the RF model generates 100 decision trees, it becomes very complicated to consider the structure of each tree.  𝐸(𝑆)= ∑−𝑝𝑖log2(𝑝𝑖)𝑐𝑖=1     (2.10)  𝐼𝐺( 𝑌,𝑋)=𝐸(𝑌)−𝐸(𝑌|𝑋)      (2.11)  The convolutional neural networks (CNNs) is a deep learning-based model that is widely-used in image classification. This model’s architecture is shown in Figure 2.27. It consists of convolution and pooling layers, which are used for creating the feature map, and the classification layer or the fully connected layer. At the last layer of the CNNs model, there are numerous activation functions that are used for classification, including sigmoid (2 classes) and SoftMax (multiclass). For the regression model, the linear activation function has been widely used. The CNNs model can update their weight in feature maps on both the convolutional layer and the fully connected layer during the training process. All updated parameters are based on the gradient descent which is calculated from the partial derivative. We can distinguish the CNNs into 3 different types based on the convolution 28  method: first, the 1-dimensional convolutional neural networks (1DCNNs), which moves the kernel only 1 way through the data; secondly, the 2-dimensional convolutional neural networks (2DCNNs), which moves the kernel 2 ways through the data, including left-to-right and top-to-bottom; and finally, the 3-dimensional convolutional neural networks (3DCNNs), which moves the kernel 3 ways including left-to-right, top-to-bottom, and in-depth.  
           Figure 2.27 Convolutional neural network architecture                                 2.2.3 Model Evaluation In the machine learning pipeline, evaluating a classification model’s performance is an essential task. Numerous measurements have been proposed for this purpose. Basically, the confusion matrix is a measurement that is usually applied to describe the performance of a classification model by comparing both the actual class and the predicted class. There are 4 different types of terminology in confusion matrix (Figure 2.28) as follows:  1) True positive (TP) is the number of predicted positives when the actual value is positive. 2) False positive (FP) is the number of predicted positives when the actual value is negative. 3) False negative (FN) is the number of predicted negatives when the actual value is positive. 4) True negative (TN) is the number of predicted negatives when the actual value is negative. The previous four terminologies can also be created in combinations as follows:  1) Accuracy, which is the ratio of correctly predicted observations to the total observations calculated. 
29  2) Precision, which is the ratio of correctly positive observations to the total predicted positive observations. 3) Recall, which is the ratio of correctly predicted positive observations to all observations in the actual class.   Both precision and recall can be used for calculating the harmonic mean called the F1 score as shown in equation 2.12. Precision can be calculated by TP divided by the summation of TP and FP, while the recall can be calculated by TP divided by the summation of TP and FN.   
 Figure 2.28 Confusion matrix  F1 Score = 2×(Recall ∗ Precision)(Recall + Precision)    (2.12)  For evaluating a multi-class classification’s performance, both the area under the curve (AUC) and the receiver operating characteristics (ROC) have been used. ROC is a probability curve as show in figure 2.28, while AUC presents the ability to discriminate between classes in the model. Figure 2.28 shows an example of an AUC when TPR has a true positive rate and FPR has a false positive rate. When the AUC value is close to 1, it means that the performance of the differentiated predicted class is better. In contrast, when the AUC value is close to 0, it means that the classification model has poor performance. 
30          Figure 2.29 Example of ROC and AUC  
AUC  CHAPTER 3 MATERIAL AND METHODOLOGIES   3.1 Audio Data The audio data used in our work were gathered at the Native Honeybee Laboratory, King Mongkut's University of Technology Thonburi Ratchaburi Campus from 26th December 2017 to 10th January 2018 under the guidance of entomologists. All the audio data for this investigation were recorded from native honeybee (Apis cerana) by using the Beeconnex device, a multi-sensor board that we designed with a 16-bit resolution and a 48 kHz sampling rate. The microphone from the Beeconnex device was installed at the corner inside a Langstroth beehive. To determine the stimulus that motivated the hissing behavior, a video camera was also installed to record images from an elevated position over the beehive entrance.  For analyzing the audio data, we trimmed the long duration audio files into three-second audio files which covered the duration of the occurrence of hissing signals that mostly occur continuously for 1.0-1.5 seconds. We then listened carefully to each file and distinguished them into one of three categories: 1) normal class, 2) hissing class and 3) highly active class.  3.2 Proposed Method Based on the numerous publications on non-invasive monitoring of beehives reviewed in the previous chapter, we selected both feature extraction and machine learning models. In this section, we provide in-depth detail about the various parameters as well as an assessment of the performance of the techniques and models used throughout the experiment.  Calculations of the three-second audio signals of temporal domain data were made by using short-time energy with the RMSE technique and the results were not transformed into frequency domain but were instead converted by using the SciPy library from Python. We considered the frequency in the range 50-6,000 Hz as this covers the frequency range of the hissing signals and their harmonics [10]. For the Mel filter-bank, audio data were analyzed through STFT with a 0.1 second frame size which is 10% of the hissing 32  occurrence time and provides a value that is close to two cycles of the human perceived frequency at 20 Hz. After that, we applied the Hanning window to each interval, with 50% overlapping to minimize the effect of leakage caused by the window function. However, all frequencies were converted into the Mel frequency scale, arranged in 12-bin filter banks which is the least common parameter used in acoustical research (12-20 bins in general). We then calculated the energy for each filter bank. Finally, all Mel filter-bank matrixes were saved in math file form for the training and testing classification model.  The Mel filter-bank matrixes were visualized in Mel filter-bank spectrograms. Initially, this was in RGB form, with the size decreased by 75%. The dimensions of the data were then reduced by converting the RGB to grayscale. Finally, two different types of Mel filter-bank spectrogram were used for presenting the effects of rotation including: 1) temporal domain (vertical) spectrogram and 2) frequency domain (horizontal) spectrogram. Both vertical and horizontal spectrograms were used for training and testing the convolutional neural network.  We applied a variety of machine learning models, both traditional and deep learning-based models, to determine the best-performing model from the different features. The three traditional classification models consisted of a support vector machine, decision tree and random forest. Both one-dimensional and two-dimensional convolutional neural networks were applied as a deep learning-based classification model. The data were then divided into two parts for training (75% of the whole data) and testing the model's performance (25% of the whole data). The different extracted features were used for different training classification models. As shown in Table 3.1, short-time energy, spectral centroid, and Mel filter-bank matrix were performed on the support vector machine, decision tree, and random forest model, respectively. Both horizontal and vertical Mel filter-bank spectrograms were applied with one-dimensional and two dimensional convolutional neural networks (1DCNNs and 2DCNNs). Referring to Table 3.2, the controlled parameters in the 1DCNNs and 2DNCCs consisted of using the Adam optimizer, 100 epochs, early stop with 10 patience based on valuation accuracy, and learning rate reduction with 5 patience (0.1 factor, 0.00001 minimum). The regularization was set to 1 in the SVM model. In addition, the entropy function was used to measure the performance of the split data in both the decision tree and random forest. For both the 33  1DCNNs and 2DCNNs model,s we decided to use only one kernel because the results from the RawConvet model in [23] demonstrated that using only 1 kernel (n=80) was able to provide 95.21% accuracy. The size of the kernel, number of nodes, and number of layers were modified to present the impact on the model's performance. In SVM experiments, four different filters were tested for comparison. Finally, the number of estimators was modified in the RF model to present the relationship between the number of estimators and the model's performance.  Table 3.1 The features and classification models in the experiment Feature Classification model Short-time energy Support vector machine Decision Tree Random forest Spectral centroid Support vector machine Decision Tree Random forest Mel frequency matrix Support vector machine Decision Tree Random forest Horizontal Mel spectrogram One-dimensional convolutional neural networks Two-dimensional convolutional neural networks Vertical Mel spectrogram One-dimensional convolutional neural networks Two-dimensional convolutional neural networks          34  Table 3.2 The independent and controlled parameters in different classification models Classification Model Independent Parameter Controlled Parameter One-dimensional convolutional neural networks 1) Number of nodes for each layer [8, 16, 32, 64, 128] 2) Number of layers (1- and 2-layers of the fully connected layer 3) Mel spectrogram form [horizontal, vertical] 1) Adam optimizer 2) 1 kernel 3) Early stop with 10 patience based on valuation accuracy. 4) Learning rate reduction with 5 patience (0.1 factor, 0.00001 minimum) Two-dimensional convolutional neural networks 1) Kernel size [3x3, 5x5, 7x7, 9x9, and 11x11  1) Adam optimizer 2) 1 kernel 3) Early stop with 10 patience based on valuation accuracy. 4) Learning rate reduction with 5 patience (0.1 factor, 0.00001 minimum) Support vector machine Kernel filter [linear, poly, sigmoid and rbf] Regularization is 1 Decision tree - Entropy Random forest Number of estimators [1, 9, 19, 29, …, 99] Entropy  For selecting the best-performing model, we experimented by adjusting various parameters in the five classification models with two competing objectives: the minimum number of trainable parameters and 95% baseline accuracy. The model with the highest accuracy performance was then selected to be deployed on the smart hive system as shown in Figure 3.1. 
 Figure 3.1 Smart hive architecture
  CHAPTER 4 RESEARCH RESULTS AND ANALYSIS   4.1 Data Exploration From analyzing the 12-day long audio file recording of the honeybee behavior in our experiment, we discovered that the A. cerana honeybees being studied emitted the hissing signal 243 times at a frequency of approximately 20.25±29.69 times a day (mean±SD). The minimum and maximum number of hissing signals was 0 (Dec 27th and Jan 2nd, 3rd and 10th) and 96 (Jan 9th) times, respectively. We investigated the timing of the hissing occurrences from the video recording and found that 154 instances were caused by threatening stimuli such as disturbance by flies or encounters with ants. However, in 89 instances, no natural stimuli could be identified as the source of the interference, as shown in Table 4.1, because of the limitation of the video camera's perspective. For example, on Jan 5th, 2018, the stimuli were not defined for some instances, but as shown in Figure 4.1, there was a possibility that the disturbances were caused by the same flies that had triggered the previous disturbance. Regarding the pattern of hissing occurrences for each hour as presented in Figure 4.2, the results reveal that there was no correlation between the two parameters (Pearson correlation: r=0.043), with the hissing signals frequently occurring in the morning (10 a.m.) and afternoon (3 p.m.). However, these signals always happen during the daytime (Figure 4.3).  Table 4.1 The occurrence of hissing signals caused by different stimuli  Stimulus Frequency Ant 114 Fly 34 Human 6 Unknown 89  243     36  
 Figure 4.1 Occurrence of hissing signals during the period of observation (a),                                 Occurrence of hissing signals for each stimulus during the date (b)  
 Figure 4.2 Occurrence of hissing signals for each hour (a), occurrence of hissing                            signals for each stimulus during the hour (b).  
 Figure 4.3 Occurrence of hissing signals during the daytime and night   
37  During this study, hissing signals occurred only 243 times, which is insufficient for training and testing classification models. In addition, the unbalanced dataset makes it difficult to assess the performance of the model. In theory, data augmentation can alleviate this problem. From the basic options of data augmentation, such as sliding the window using overlap data [51] and making geometric transformations [52] (shifting, scaling, rotation, and reflection), we chose the concept of sliding the window (three second) through the signal of interest with a 0.05 second time step. This technique maintains the time series relationship of the audio data. In contrast, if we shift, scale, rotate, or reflect the audio data, the time series will be destroyed. We created different audio data, thereby increasing the number of datasets to a sufficient amount for training the robust models. Figure 4.4 presents nine different Mel spectrograms that cover the same hissing signal but at different intervals. For labeling the data, the audio files were listened to carefully following the entomologist’s guidelines. From our data exploration, we found that some of the non-hissing class results overlapped the hissing class results, which resulted from the situation of the honeybees frequently flying in/out. We classified this situation as highly active. Finally, we created 75,816 datasets consisting of 25,521 normal class, 24,774 hissing class, and 25,521 highly active class occurrences as shown in Table 4.2. 
Figure 4.4 Example of spectrogram produced from sliding window                                 with different intervals  Table 4.2 Proportion of dataset for each class  Class Number %Proportion Normal 25,521 33.66 Hissing 24,774 32.68 Highly active  25,521 33.66   
38  After creating the dataset, we calculated the mean and standard deviation of the temporal domain from each class as shown in Table 4.3. The mean of the temporal value data can be arranged in ascending order, from normal class to hissing class and then highly active class. Likewise, the standard deviation values are arranged in the same order.  Table 4.3 Mean and standard deviation of temporal domain from each class  Class Mean SD Normal -1.7019 x 10-5 8.7070 x 10-3 Hissing -1.3341 x 10-5 1.5695 x 10-2 Highly active  -1.0348 x 10-5 4.9692 x 10-2  Figures 4.5-4.7 show examples of the 1-second temporal domain data of the normal class, hissing class, and highly active class occurrences, respectively. The three examples show that the collected audio data from the honeybees’ emittances is classified as a non-deterministic signal, which has no discernible pattern of repetition and cannot be predicted by an equation. To demonstrate in more detail, we divided and displayed the temporal data into 10 intervals to show the signal changes over time, as shown in Figures 4.8-4.10. The results remain the same as in Figures 4.5-4.7 which are the non-deterministic signal.   Figure 4.5 Example of 1-second temporal domain data from normal class   Figure 4.6 Example of 1-second temporal domain data from hissing class 
39  
 Figure 4.7 Example of 1-second temporal domain data from highly active class  
 Figure 4.8 10 intervals of 1-second temporal domain data from normal class  
40  
 Figure 4.9 10 intervals of 1-second temporal domain data from hissing class  
 Figure 4.10 10 intervals of 1-second temporal domain data from highly active class 
41  The RGB-mode spectrograms were reduced by 75% of the size to the Mel spectrogram (grayscale, 76x117 pixel) when they were converted, and all values were normalized into the 0-1 range. We computed 4 different statistical parameters from each class, namely the min, max, mean, and standard deviation (std). As show in Table 4.4, the results reveal that the mean of the normalized grayscale intensity from the normal class is higher than from the hissing and highly active classes, which can be applied from the fact that most of the area of the Mel spectrogram from the normal class has a lot of white area. In contrast, the highly active class shows a mean of normalized grayscale intensity, as indicates by most of the area being dark gray or black. To demonstrate the distribution of grayscale intensity for each class, we plotted the histograms as shown in Figures 4.11–4.13. Most of the normalized grayscale intensity is less than 0.9. The distribution of most of the normalized grayscale intensity values from the normal class is in the range of 0.6-0.8 as shown in Figure 4.11. For the hissing and highly active classes, Figures 4.12 and 4.13 indicate that most of the normalized grayscale intensity values are in the range of 0.2-0.8, which can be implied from the observation that both classes had high amounts of black area.  Table 4.4 Statistical parameters of Mel spectrogram from each class  Class Min Max Mean Std Normal 0.1098 1.0000 0.7542 0.1243 Hissing 0.1098 1.0000 0.7010 0.1514 Highly active 0.1098 1.0000 0.6099 0.2148  
 Figure 4.11 Grayscale intensity histogram from the normal class spectrogram  
42  
 Figure 4.12 Grayscale intensity histogram from the hissing class spectrogram  
 Figure 4.13 Grayscale intensity histogram from the highly active class                      spectrogram  4.2 Features Extraction The FFT technique was initially applied to the 3-second raw acoustic data. In this step, we only considered the frequencies in the range of 0.5–6 kHz. The frequency distribution of the hissing signals demonstrated that there were 3 frequency ranges with dominant amplitudes: 0.5-1.0, 1.0-1.5, and 1.5-2.0 kHz (Figures 4.14 a, b, and c, respectively). However, our results indicate that identifying the hissing signals using the broadband frequency of 0.5-3.6 kHz with over 0.5–1.0 s intervals was not sufficient to conclude whether the audio signal was in fact a hissing signal. We discovered that in the highly-motile environment of the beehive, honeybee activities can generate broadband harmonics of frequency that can replicate the frequency of hissing signals, such as from the frequent flying in or out of the beehive during foraging or during the flight orientation activity (Figure 4.14 d, e, and f), as well as in some instances when the honeybee emits a continuous sound close to the recording microphone (Figure 4.14 h). 
43  
 Figure 4.14 Results from the fast Fourier transform, hissing class (a, b, c), highly                   active class (e, f, g), normal class (g, h, i).  The short-time energy of each audio file was calculated by using the RMSE equation. We initially set the maximum and minimum value of RMSE at 0.015 and 0.005 respectively to normalize the loudness value in the range of 0-1. After applying all audio data with this technique, as shown in the example (Figures 4.15-4.17), we were unable to determine the thresholds or distinct segments of all three class.  
 Figure 4.15 RMSE and waveform graph of normal class     
44             Figure 4.16 RMSE and waveform graph of hissing class  
 Figure 4.17 RMSE and waveform graph of highly active class  For creating the spectral centroid feature as shown in Figure 4.18, 10,000 audio data files were sampled. These files were converted from time domain to frequency domain by using STFT with a window size of 22,050, n_ttf 1,024 n_hop = 515, and the frequency was filtered in the range of 50–6,000 Hz. After applying the SC equation, 44 spectral centroids were obtained and visualized as shown in Figure 4.19. The SC representation of each audio file was then selected by choosing the data in the deciles of 1-9 to create tabular data as shown in Figure 4.20. We normalized the data into 0-1 values and performed PCA to reduce the dimensional data from 9 dimensions to only 2 dimensions, as shown in Figure. 4.21 and displayed in the graph as shown in Figure 4.22, respectively. The results revealed that the non-hissing and hissing classes were still highly overlapping.  
45  
  Figure 4.18 Method to create spectral centroid feature    Figure 4.19 Example of spectral centroid feature graph    Figure 4.20 Example of spectral centroid feature in tabular form  
  Figure 4.21 Example of spectral centroid feature in 2-dimensional PCA  
46  
  Figure 4.22 Scatter plot of spectral centroid feature in 2-dimensional PCA  Regarding the Mel filter-bank matrix and the Mel filter-bank spectrogram, the 3-second audio files were analyzed through FFT with a 100 ms frame size, 50% overlapping, and Hanning window. The files were also filtered to include only those in the frequency range of 50–6,000 Hz. The selected files were arranged into 12-bin filter banks, and the energy for each filter bank was calculated. As a result of these steps, each 3-second audio file was represented in 12x58 Mel matrix form. All the Mel filter bank matrix arrays were then visualized as Mel filter-bank spectrograms. Our results revealed more clearly the presence of hissing signals compared to what could be determined from the distribution of frequencies using the FFT technique. The hissing signals always appear as a vertical ellipse of black/gray areas (Figures 4.23 a, b, and c). For the highly active activity, the intensity of black/grey distribution appears on the top or throughout the spectrogram (Figures 4.23 d, e, and f), while the short black band indicates the situation where the honeybee emits a sound close to the recording microphone (Figure 4.23 h).  
47  
 Figure 4.23 12 bins Mel spectrogram, hissing class (a, b, c), highly active class (e, f, g),                      normal class (h, i, j).  In the section above, we constructed and explored several features including FFT frequency-based technique, short-time energy, spectral centroid, Mel filter-bank array and Mel spectrogram. The results from the FFT were unable to show the dominant frequency range and lacked the temporal variation. As for the RMSE results, the limitation of the unstable sound source (honeybee) affected the sound volume level. For the spectral centroid approach, the scatter plot of 3 classes demonstrated that there is still a high amount of overlapping in the data. As a result, we chose only the appropriate features, including the Mel matrix, horizontal Mel spectrogram, and vertical Mel spectrogram (Table 4.5) for both training and testing the classification models in the next section.  Table 4.5 Features used for training and testing machine learning models  Feature Classification model Mel filter-bank matrix Support vector machine Decision tree Random forest Horizontal Mel spectrogram (117x76 px) 1-dimensional convolutional neural networks 2-dimensional convolutional neural networks Vertical Mel spectrogram (76x117 px) 1-dimensional convolutional neural networks 2-dimensional convolutional neural networks  
48  4.3 Classification Model We randomly divided the dataset into 5 different folds to reduce the bias, and then split them into a training set and testing with a 75:25 ratio respectively (56,747 for training and 18,915 for testing). After this, we trained and tested the classification model based on the mean of magnitude from each Mel filter-bank (12 features). Table 4.6 shows the average accuracy results of the SVM model. The support vector machine model based on poly kernels was able to achieve the highest average accuracy of 89.83%, while other kernels had lower accuracy. The results also disclosed that the decision tree provided an accuracy percentage of 97.86%. However, increasing the number of estimators in the random forest technique could not raise the mean accuracy higher than 99% as shown in Table 4.7. We also discovered that the 4th, 10th, and 2nd of Mel filter banks were the top 3 in terms of importance features. as shown in Table 4.8. When converting the Mel's frequencies from each filter-bank to FFT frequencies, the FFT frequency ranges of 102.49-207.03, 821.84-1,852.39 and 43.53-102.49 Hz were found to be important for differentiating the three classes, as shown in Table 4.9.  Table 4.6 Performance of support vector machine model based on different kernels  Kernel Linear Poly Rbf Sigmoid Average accuracy (%)  88.24  89.81  89.37  76.35  Table 4.7 Performance of decision tree and random forest models  The number of estimators 1 9 19 29 39 49 59 69 79 89 99 Average accuracy (%)  97.86  98.19  98.31  98.36  98.11  98.39  98.29  98.44  98.32  98.47  98.40  Table 4.8 Important features of the random forest model  Mel  Filter Banks 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 11th 12th importance .0800 .0914 .0204 .2854 .0332 .0351 .0891 .0332 .0257 .2571 .0187 .0305   49  Table 4.9 Mel frequency and FFT frequency range for each Mel filter-bank  Mel filter-bank Mel frequency range FFT frequency range 1 39-106 24.64-69.06 2 68-154 43.53-102.49 3 106-214 69.03-149.37 4 154-292 102.49-207.03 5 214-390 146.37-289.43 6 292-515 207.031-405.49 7 390-674 289.43-572.99 8 515-875 405.49-821.54 9 674-1,132 572.99-1,211.27 10 875-1,458 821.84-1,852.39 11 1,132-1,873 1,211.27-2,988.67 12 1,458-2,400 1,852.3954-5,187.81  Regarding the deep learning-based classification models, we experimented with 40 different 1DCNNs models by modifying the various independence parameters. The results shown in Table 4.10 illustrate that the models trained with the frequency domain Mel spectrogram could not overcome 95% accuracy, while the 24th, 25th, 26th, 31st, and 35th models trained by the temporal domain Mel spectrogram provided an average accuracy level of higher than 99.00%. For training the 2DCNNs models, we only selected models with trainable parameters of less than 26,873, which is the maximum trainable parameters of 1DCNNs. These models were trained with both the temporal and frequency domain Mel spectrogram dataset. Finally, the results shown in Table 4.11 demonstrate that none of these 2DCNNs models had an average accuracy higher than the top 4 1DCNNs models.            50  Table 4.10 Structure and accuracy of the 1DCNNs models No. Mel Spectrogram #node dense 1 #node dense 2 Trainable Parameters Average  Accuracy (%) 1          Frequency domain (117x76 px) 128 0 15,568 87.04 2 128 8 16,240 90.31 3 128 16 17,296 87.37 4 128 32 19,408 89.63 5 128 64 23,632 88.26 6 128 128 32,080 88.37 7 64 0 7,824 86.49 8 64 8 8,176 87.17 9 64 16 8,720 89.93 10 64 32 9,808 88.19 11 64 64 11,984 88.54 12 32 0 3,952 85.73 13 32 8 4,144 89.16 14 32 16 4,432 89.20 15 32 32 5,008 88.82 16 16 0 2,016 84.97 17 16 8 2,128 87.31 18 16 16 2,288 86.23 19 8 0 1,048 85.51 20 8 8 1,120 85.21 21          Temporal domain (76x117 px) 128 0 10,361 96.61 22 128 8 11,033 97.13 23 128 16 12,089 98.81 24 128 32 14,201 99.79 25 128 64 18,425 99.35 26 128 128 26,873 99.61 27 64 0 5,241 96.22 28 64 8 5,593 98.61 29 64 16 6,137 98.84 30 64 32 7,225 98.41 31 64 64 9,401 99.36 32 32 0 2,681 94.92 33 32 8 2,873 96.68 34 32 16 3,161 96.84 35 32 32 3,737 99.10 36 16 0 1,401 94.81 37 16 8 1,513 95.63 38 16 16 1,673 96.27 39 8 0 761 89.82 40 8 8 833 91.15                 51  Table 4.11 Structure and accuracy of the 2DCNNs models No. Mel Spectrogram #node dense 1 #node dense 2 Trainable Parameters Average  Accuracy (%) 1    Frequency domain (117x76 px) 8 0 16,917 89.39 2 8 8 16,989 93.34 3 8 0 16,189 86.48 4 8 8 16,261 91.35 5 8 0 15,461 94.73 6 8 8 15,557 95.73 7 8 0 14,805 93.83 8 8 8 14,877 95.14 9 8 0 14,149 88.96 10 8 8 14,221 95.34 11    Temporal domain (76x117 px) 8 0 16,917 90.04 12 8 8 16,989 94.21 13 8 0 16,189 91.81 14 8 8 16,261 94.93 15 8 0 15,461 94.31 16 8 8 15,557 94.48 17 8 0 14,805 90.14 18 8 8 14,877 94.39 19 8 0 14,149 92.48 20 8 8 14,221 94.49  The 35th 1DCNNs model was able to provide an average accuracy of more than 99% and had the minimal number of trainable parameters at 3,737 based on dataset testing. To illustrate the results in more detail, we selected the confusion matrix as shown in Figure 4.24 where the numbers 0, 1 and 2 represent the normal, hissing, and highly active classes, respectively. The results obtained from the testing show that the high accuracy was not caused by the imbalanced dataset. Then, we converted all predicted results to the ROC curve as shown in Figure 4.25. The ROC curve indicates that the mean ROC of each class is equal to 1, which indicates that the model can predict efficiently in each class based on dataset testing.   
 Figure 4.24 Confusion matrix of 35th 1DCNNs model 
52   
 Figure 4.25 Receiver operating characteristic of 34th 1DCNNs model We explored in more detail how the 35th 1DCNNs model performed. Figure 4.26 shows the learning kernel (1x76) generated in the convolutional layer of this model. The minimum, maximum, mean, and standard deviation values are -0.8193, 2.4232, -0.0201 and 0.2846 respectively. The high values are in the index of 1-3. When convoluting this kernel to the horizontal Mel spectrogram dataset, the basic statistical parameters of convoluted results for each class are presented in Table 4.12. The normal class has minimum, maximum, mean, and standard deviation values of 0.0, 3.3304, 0.5653, and 0.6133, respectively; the hissing class has minimum, maximum, mean, and standard deviation values of 0.0, 3.3240, 0.8581, and 0.6807, respectively; the highly active class has minimum, maximum, mean, and standard deviation values of 0.0, 4.2245, 1.3781, and 1.1263, respectively.       Figure 4.26 Learning kernel generated by 1st convolution layer                                   of 35th 1DCNNs model   
53  Table 4.12 Statistical parameters from convoluted results from each class Class Min Max Mean Std Normal 0 3.3304 0.5653 0.6133 Hissing 0 3.2340 0.8581 0.6807 Highly active 0 4.2245 1.3781 1.1263  When multiplying the leaning kernel by the grayscale spectrogram, the result will be a size of 1x76 which is downgraded from 76x117. Figure 4.27 shows an example histogram of 9 different convoluted results from the normal class. The distribution is not a normal distribution, and most values are below 1.0. To display the position of each value, we converted the convoluted results as shown in Figure 4.28, which is grayscale format (1x76). The high value range is in the 0-24th and 75th. For the hissing class, the histogram of convoluted results is illustrated in Figure 4.29 with the values normally distributed from 0.0 to 2.0. The high value range is in the 0-30th and 75th as shown in Figure 4.30. Regarding the histogram from the convoluted results of the highly active class, the histogram and position of the convoluted results are similar to those of the hissing class results. When considering the results shown in Figures 4.28, 4.30 and 4.32 together with the statistical values from Table 4.12, we found that the overall values of the highly active class are greater than those of the hissing class and normal class.   
 Figure 4.27 Histogram of convoluted results from normal class  
54  
 Figure 4.28 Convoluted results of the normal class from 1st layer 1DCNNs model  
 Figure 4.29 Histogram of convoluted results from hissing class        Figure 4.30 Convoluted results of the hissing class from 1st layer 1DCNNs model  
55  
 Figure 4.31 Histogram of convoluted results from highly active class  
 Figure 4.32 Convoluted results of the hissing class from 1st layer 1DCNNs model  4.4 Model Deployment The 35th model of 1DCNNs has an average accuracy of 99.10% and a minimum trainable parameter of 3,737. As a result, this model was deployed to the smart hive system described in Chapter 3. The Beeconnex device installed inside the honeycomb gathers 10-second videos with audio data and sends them to the server to classify the state of the beehive every five minutes. We wrote a program to extract the audio and divided it into 7 intervals, consisting of 0-3, 1-4, 2-5, 3-6, 4-7, 5-8, 6-9 seconds, as shown in Figure 4.33. The reason for not including 7-10 seconds was that we found there was a problem with recordings of less than 10 seconds in some audio files. All 7 intervals of the audio signals were converted to Mel spectrograms, and then states were distinguished with the 35th model of 1DCNNs. For any predicted outputs that showed any 1-second interval in the data, the result was identified as hissing class, and this immediately implied that the 10-second audio file had interference. However, if any predicted outputs had a combination of normal class and highly active class, the result was considered to be highly active class. 
56  
 Figure 4.33 Process of dividing the 10-second audio data                                   for analysis with 35th 1DCNNs  The results from the classification model were stored in a database with the structure as shown in Figure 4.34, including: 1) id, which is an auto-generated sequence of numbers within the table; 2) device_id, which indicates the number of the device; 3) file_name, which is the name of the file used in the classification process; 4) result, which is the result of classification from the 35th 1DCNNs model; and 5) created_at, which is the timestamp that was automatically generated. In this table, the primary keys are device_id and file_name. Moreover, the 10 newest results from the prediction are displayed in the upper left corner on the Grafana dashboard as shown in figure 4.35.  
 Figure 4.34 Structure of video result table  
57  
 Figure 4.35 Grafana dashboard  4.5 Model Validation For testing the model's performance against honeybee sound data from the real environment, we conducted a 92-day trial run of the system from May 1 to July 31, 2021.  Our collecting device was installed in an Apis cerana beehive at the Native Honeybee and Pollinator Research Center, KMUTT, Ratchaburi campus, as shown in Figure 4.36. During the trial run, 22,883 video files were collected and analyzed. To identify the actual situation, we carefully reviewed the video files from both inside (Figure 4.37) and outside the beehive (figure 4.38). We found that the 35th CNNs model could not distinguish the beehive’s status based on acoustic sound, the the model was be able to handle interference from other naturally-occurring signals. When some signals were characterized as having a high frequency range and high amplitude that covers the signal characteristics used in training the model process, all of them were identified as highly active class, such as the rumbling sound of cicadas, the sound of rain hitting the beehive hard, or some workers emitting sounds close to the microphone. From Table 4.13, which is a confusion matrix of model validation, it can be seen that the model was able to predict correctly 21,852 out of 22,883 times (95.49%). However, the validation data is an unbalanced dataset. As a result, we need to consider it separately. The precision of normal, hissing and highly active classes are 95.68%, 0%, and 50% respectively.  
58  
 Figure 4.36 Beehive for gathering data during model validation  
 Figure 4.37 Example of recording a video from within the hive  
 Figure 4.38 Example of recording a video from outside the hive  
59  Table 4.13 Performance of 24th 1DCNNs model based on validation dataset   Actual Class Normal Hissing Highly Active  Predict Normal 21,799 0 983 Hissing 5 0 0 Highly Active 43 0 53  Total 21,847 0 1,036  We explored the five false predicted results in the hissing class. One time, it was caused by the loud noise of cicada outside the beehive, and the remaining four times were caused by the worker bees getting too close and emitting sounds near to the recording microphone. For false predicted results in the highly active class, two times were caused by the sound during installation, 11 times were caused by cicadas, five times were caused by the rain heavily hitting the beehive, and the rest were due to the worker bees emitting sounds close to the recording microphone. Finally, regarding the false predicted results in the normal class, all errors were caused by worker bees making sounds near the microphone.   CHAPTER 5 CONCLUSION  5.1 Discussion Based on the FFT results, using only the broadband frequency of 300-3,600 kHz with over 0.5–1.0 s interval is not sufficient to conclude whether an unknow audio signal is a hissing signal. This is because there are some activities in the honeybees’ behavior that can generate the broadband harmonic of frequency that replicates the frequency of a hissing signal, such as frequently flying in/out during foraging or flight orientation.  Transforming the FFT results to the Mel filter-bank matrix can provide better features because the Mel frequency filter bank uses the concept of a triangular filter-bank to capture the STFT results from each frame and groups them into each critical band which corresponds to better resolution at low frequency and less at high frequency. From this, it can then create a rough approximation of the spectrum. As a result, this method can reduce the number of parameters to 12 while maintaining the temporal relationship between frames.  The mean magnitude from each Mel filter bank (12 parameters) was used as an input feature for the support vector machine, decision tree and random forest. The results from testing the vector machine model disclosed that none of the models were able to provide an average accuracy of greater than 90%. The first reason causing this low performance is that the average amplitude from each Mel filter-bank lacks the relationship between time and frequency. In addition, this model does not perform very well when the dataset has more noise, as shown in the scatter plot of the spectral centroid feature in the 2-dimensional PCA from the previous section. This support vector machine is most effective when the number of features is greater than the number of observations.   The reasons preventing the high performance of the 35th 1DCNNs model include: 1) transforming the Mel filter-bank matrix (12x58) to a grayscale Mel spectrogram (76x117) provides additional detail by approximately 6 times in each row and 2 times in each column; 2) convoluting the 1D kernel (1x117) with a horizontal Mel spectrogram creates the average frequency of interest over frame indices, and this kernel is appropriate for capturing the characteristics of the 3 classes; 3) the method of updating the weight 61  parameters during the training process can make it possible to get closer to the optimal weight, which is better than the normal average. From the related work on honeybee signal classification, there is no research on the hissing signal classification model. As such, it we were able to compare the effectiveness of our model, which is the 35th 1DCNNs model, with other research on honeybee signal classification but not in the hissing signal domain. [20, 23, 24, 34] were able to provide more than 94% accuracy based on the testing dataset. However, the model's performance was not reported when applied to the real environment. The models developed in this study can be integrated into a non-invasive beehive monitoring system, which can be used for remote monitoring and distance education on honeybee behavior. From our work experience, the audio signal collection for creating a honeybee signal classification model should include a variety of data to cover both the signals emitted by the honeybees and other natural signals whose frequency range or harmonics can replicate the signal of interest. Moreover, labeling the dataset should not be based solely on the characteristics, waveform, or frequency of the sound. It is necessary to listen carefully to the real audio sound, confirm observations with the video from both inside and outside the beehive, and further consult with specialists to minimize the errors in this process.  5.2 Conclusion We analyzed “hissing” signals emitted by Apis cerana, Thailand, by extracting numerous features, including short-time energy, spectral transformation, Mel filter banks, and Mel spectrograms, for constructing classification models. However, we found that the short-time energy based on RMSE is not suitable due to the limitation of the honeybee sound source; there is an inconsistent distance. The results from SC combined with 2-dimensional PCA demonstrated that the transformed data has a high level of overlapping. Our results from the FFT approach disclosed that using the dominant frequency of 300-3,600 Hz is insufficient to conclude whether an unknown signal is a hissing signal because the harmonics caused from other honey activities can also be within this range. As a result, both the Mel frequency matrix and Mel spectrogram were used as main features in the classification models. We investigated tuning the parameters and structures of the 1DCNNs, 2DCNNs, SVM, DT, and RF models. The 1DCNNs models learning from the 62  temporal domain Mel spectrogram with the number of nodes between 32-128 in each of the two layers was able to distinguish three different classes, namely normal class, hissing class, and highly active class, with an average accuracy of greater than 99% based on the testing dataset. However, this model was not able to handle interference from other signals in the environment or the issue of unstable sound sources based on model validation. For future work, we suggest that other honeybee signal classification models need to take into consideration the fundamentals of their signals in order to perform feature extraction, which could result in a highly complex classification model not being required. The installation and position of the recording microphone used for the data acquisition also needs to be examined. Some materials may be needed to prevent the worker bees emitting sounds close to the recording microphone. Moreover, the audio dataset should collect related signals from both the beehive and the environment to cover one cycle of beekeeping and raise the model’s performance.