 AUTOMATED HYPERPARAMETERS TUNING USING THE ARTIFICIAL BEE COLONY ALGORITHM

Hyperparameter Tuning chooses optimal hyperparameters when training a machine learning model. Once an optimization function has been defined, Hyperparameter Tuning would sequentially consider many sets of values that define the hyperparameters for the machine learning models and select most optimal set. The process is difficult and time-consuming. As such, Automated Hyperparameter Tuning has been suggested to expedite the process and two well-known traditional parameter optimization methods are grid search and random search. However, these methods are computationally expensive for a large amount of hyperparamters. Furthermore, these methods only sequentially analyze through predefined sets of hyperparameters without the ability to reactively adjust the values toward an optimal solution. In this thesis, the Artificial Bee Colony (ABC) optimization algorithm was adopted to automatically adjust hyperparameters. ABC is a metaheuristic method based on bee foraging behavior. Execution time, accuracy, F1-score, and accuracy plus F1-score of using ABC were chosen as performance indicators for the comparison of different hyperparameter tuning techniques for different classification algorithms against those from grid search and random search.  
ABC/ ANOVA/ Classification/ Grid Search/ Hyperparameter Tuning/ Random Search/    iii 
CHAPTER 1 INTRODUCTION 1.1 Problem Statement and Motivation Machine learning is a well-known data analysis method used for building models. In the common machine learning process, five main tasks are performed: aggregating data, preprocessing data, training the model, testing the model, and improving the performance of the model). Hyperparameter tuning is one of the most popular techniques that can be used to improve the performance of machine learning models. Hyperparameters are parameters that govern the learning process and the complexity of machine learning models. Thus, tuning hyperparameters is the process of finding a set of hyperparameters that provides the best predictive results for the model. Even though most algorithms can perform reasonably well in a variety of situations with the default hyperparameter settings, hyperparameter tuning is still conducted to seek further improvement. However, the hyperparameter tuning process can be time-consuming and resource-intensive even when the results are not improved.  Grid search and random search are the two most well-known traditional methods of hyperparameter tuning. To tune a model, grid search uses all possible combinations of predefined values for a set of hyperparameters. Grid search will then return a set of hyperparameters with the optimal values for the trained model's best performance. Conversely, random search generates random combinations of values from a set of hyperparameters. However, for a large number of hyperparameters, these two methods are computationally expensive and lack mechanisms for tuning process optimization. Automated hyperparameter tuning methods, which automatically select the optimal hyperparameter values, was suggested as a solution that can alleviate the aforementioned issues. The implementation of optimization algorithms was applied to automated hyperparameter tuning. Methods such as Particle Swarm Optimization, Ant Colony Optimization, and Artificial Bee Colony (ABC) Optimization are just a few of the optimization algorithms that have been used to tune hyperparameters in various applications.  This thesis evaluates the automated hyperparameter tuning based on the random forest model using the Artificial Bee Colony algorithm. Random forest is an ensemble learning 2 
 algorithm that is constructed by a large number of decision trees. After each decision tree predicts an intermediate output, the random forest algorithm selects the most popular output from all the decision trees as the main output. Four popular random forest hyperparameter tuners are n-estimator, max depth, min sample leaf, and min sample split. The n-estimator represents the number of trees in the forest, max depth represents the maximum number of nodes in each tree, min sample leaf represents the minimum number of samples required to be at a leaf node, and min sample split represents the minimum number of samples to split internal node. These hyperparameters control behaviors of the algorithm directly, which can affect the model performance.   Even though only four random forest hyperparameters are studied, the effects of changes in the hyperparameters on the performance of the model was investigated using sensitivity analysis methods before performing automated hyperparameter tuning. The aim of this process is to identify sensitive hyperparameters that should be adjusted in order to improve the model's performance. The hyperparameters in this study are analyzed using two well-known sensitivity analysis methods: One-at-a-time (OAT) and Latin hypercube sampling (LHS). The statistical significance of hyperparameter sensitivity was determined using analysis of variance (ANOVA).  1.2 Objective and Research Scope Four datasets with different profiles are subjected to sensitivity analysis and automated hyperparameter tuning experiments (You should name the datasets). Firstly, sensitivity analysis was performed to determine the sensitivity of each random forest hyperparameter. Secondly, the artificial bee colony optimization algorithm was used to automatically tune the previously selected sensitive hyperparameters.  The main objective of applying the optimization algorithm to the automated hyperparameter tuning processs is to determine whether the algorithm can obtain the best optimal hyperparameter values while reducing computational costs, for instance time spent. The performance of the random forest model in terms of accuracy, F1-score, accuracy plus F1-score, and time-consumption will be compared in the automated hyperparameter tuning process using artificial bee colony, grid search, and random search.   
 CHAPTER 2 LITERATURE REVIEW AND THEORY This chapter contains a review of the literature as well as related theories. All related works on sensitivity analysis and automated hyperparameter tuning are summarized in the literature review section. The random forest algorithm, evaluation metrics, sensitivity analysis methods, and hyperparameter tuning methods are all explained in detail in the theory section.  2.1 Related Works  2.1.1 Sensitivity Analysis One-at-a-time analysis (OAT) has been used to assess the sensitivity of various machine learning hyperparameters. Bansal, et al. [1] applied OAT in attribution method models such as gradient-based, sliding patch, and meaningful perturbation. The research discovered that gradient-based and perturbation-based methods were extremely sensitive to hyperparameter changes. Therefore, the hyperparameters of the model could affect the accuracy score. Isiet and Gadala [2] employed OAT to perform sensitivity analysis on the particle swarm optimization (PSO) algorithm. The inertia weight and acceleration coefficient were discovered to be the most sensitive PSO parameters on a benchmark constrained optimization problem. Zhang and Wallace [3] adopted OAT to train a convolutional neural network on sentence classification. They reported that filter region size and feature maps have a large effect on model performance, but regularizations (dropout rate and l2 norm constraint) have a lesser effect. Teodoro, et al. [4] used Morris OAT, which is a variation of OAT, to perform sensitivity analysis on each input parameter of their tissue image segmentation model. This research demonstrated that the image analysis pipeline was sensitive to the input parameters, and their approach could reduce computation costs.  Another popular sensitivity analysis method is Latin hypercube sampling (LHS). Collins, et al. [5] applied LHS to investigate parameters that are important to the foreclosure contagion effect in an agent-based model. Their results showed that the foreclosure discount and the time it takes to complete the foreclosure process are the most important 4 
 parameters. Zhang, et al. [6] used LHS to perform sensitivity analysis on internal material parameters of thermoelectric generators. They found that the increased Seebeck coefficient, the heat transfer coefficient of the cold or hot source, and the mean value of the heat exchange area all had an effect on the system stability and reliability. Bidah, et al. [7] also employed LHS and partial rank correlation to determine the most influential parameters in their agree-disagree model. Their results presented that the most influential parameters are the ignorant to agree transmission rate, interest loss factor of agree individuals, and interest loss factor of disagree individuals.   Other methods can also be used to perform the sensitivity analysis. Hutter, et al. [8] used efficient marginalization techniques and ANOVA to find the significance of each hyperparameter of the Latent Dirichlet Allocation model. It was indicated that S was the most important hyperparameter. The researchers claimed that their method also provided users with deeper insights into how hyperparameters affect the overall performance of the model. This method, however, was time-consuming  2.1.2 Automated Hyperparameter Tuning using Optimization Algorithms Joy, et al. [9] used four types of Bayesian Optimization (BO) with deep neural networks and support vector machines, namely: Proposed-BigData-BO, Generic-BO, Transfer-BO, and Efficient-BO. Performance was measured using execution time and test error. The MNIST dataset was used in the neural networks. The best performance was achieved by Proposed-BigData-Bo, which had a test error of 0.15 and took 1550 seconds to complete. Transfer-BO had a test error of 0.15 but took 3300 seconds. The a&a dataset was used with the support vector machines. Proposed-Bigdata-BO also had the best performance, with a test error of 0.191 and a time consumption of 1300 seconds. Transfer-BO was still in second place, with a test error of 0.211 and a time of 3200 seconds.  Nalcakan and Tolga [10] used a population-based algorithm to tune four neural network hyperparameters in a handwritten digit recognition problem. The four hyperparameters are the number of hidden layers, the number of neurons in a hidden layer, the activation function, and the optimization function. They created a population of individuals that have hyperparameters as their genes. Then, to optimize their hyperparameters, they used the selection, crossover, and mutation steps as GA with a different way of calculating 5 
 fitness. They achieved 98.66% accuracy with 512 neurons per layer, 2 hidden layers, ReLu activation function, and AdaDelta optimizer.  Particle swarm optimization (PSO) with support vector regression (SVR) was used by Kabiru, et al. [11] to predict the performance of hydrocarbon reservoirs. The datasets were divided into four oil wells: A, B, C, and D. Kernel, c, and epsilon were the hyperparameters used in this work. Furthermore, they compared particle swarm optimization to random search and trial and error. The root mean square error (RMSE) was used to determine which optimization produced the best results. In oil well-A, the RMSE of PSO, random search, and trial and error were 0.61, 0.69, and 0.71, respectively. PSO still had the lowest RMSE when compared to random search and trial and error for the other datasets.  Lorenzo, et al. [12] employed particle swarm optimization as well, but with a convolutional neural network. SimpleNet-1 and CIFAR-10 data sets were used. They compared PSO to grid search and random search to demonstrate that PSO performed better. Their research revealed that PSO outperformed both SimpleNet-1 and CIFAR-10 results.  Lixing, et al. [13] predicted HVAC cooling load using ant colony optimization (ACO) and SVR. The hyperparameters were the same as in the prior PSO work, which were kernel, c, and epsilon. They compared ACO to backpropagation neural networks (BPNN). To find a better method, the normalized mean square error (NMSE) was used. They discovered that ACO had a lower NMSE of 0.14 than BPNN, which had a higher NMSE of 0.24.  Similar to the previous work, Dehuai, et al. [14] also used ACO with support vector regression (to study what?). The hyperparameters therefore were identical, but the problem was different. In this study, the researchers wanted to predict the machine cutting temperature. Furthermore, ACO-SVR was compared to SVR and BPNN. The mean absolute percentage error (MAPE) was used to compare performance. The MAPEs for ACO-SVR, SVR, and BPNN were 5.2, 6.2, and 6.5, respectively. ACO-SVR had the lowest MAPE, implying that it produced the best results.  Kang and Li [15] used artificial bee colony (ABC) with SVR to predict system failure. The SVR hyperparameters were, again, the same as in the previous works. The datasets were divided into three examples. In this paper, they compared ABC to grid search and 6 
 PSO. The mean absolute error (MAE), root mean square error (RMSE), and correlation coefficient (R) were used to assess performance. In Example 3, the (MAE, RMSE, R) of ABC, PSO, and grid search were (0.0223, 0.0295, 0.9926), (0.0225, 0.0292, 0.9927), and (0.0227,0.0297,0.9925), respectively. Because they all produced similar results, it was difficult to determine which was the best. However, the time consumed by ABC, PSO, and grid search are 159.85s (population size = 30), 500.80s (population size = 30), and 2,489.4s, respectively, indicating that ABC consumes less time while providing the same performance.  To tune least square support vector machine hyperparameters, Mustaffa and Yusof [16] used the standard ABC algorithm, two modified ABC algorithms (lv-ABC and sm-ABC), and their proposed artificial bee colony algorithm (eABC). They modified the original equations of employed bee and onlooker phases in their eABC. The MAPE test was used to assess performance. They discovered that eABC performed the best with a score of 12.0599, followed by lv-ABC (12.3757), smABC (12.4537) and standard ABC (12.5862).  Table 2.1 summarizes all of the performances of each model from the various related works discussed earlier in this section. Different optimization algorithms were used to tune the hyperparameters of each model, and each model was evaluated using different metrics.    
 7 Table 2.1 Related work evaluation of automated hyperparameter tuning algorithms  Problem  Automat ed 
hyperparameter 
tuning algorithm  Model  Performance  Year  Accuracy  NMSE  MAPE  MSE  RMSE  Test 
error  Time (s)  Real world classification dataset  Bayesian  Support Vector 
Machine  - - - - - 0.15 1550  2016  Real world classification dataset  Bayesian  Neural Ne twork  - - - - - 0.191  1300  2016  Handwritten Digits Recognition  Population -based  Neural Network  98.66  - - - - - - 2019  Prediction of Hydrocarbon 
Reservoir  Particle Swarm  Support Vector 
Regression  - - - - 0.6165  - - 2017  Prediction of Hydrocarbon 
Reservoi r Random Search  Support Vector 
Regression  - - - - 0.6955  - - 2017  Prediction of Hydrocarbon 
Reservoir  Trial and Error  Support Vector 
Regression  - - - - 0.71 - - 2017  HVAC Cooling Load  Ant Colony  Support Vector 
Regression  - 0.14 - - - - - 2010  HVAC Cooli ng Load  None  Back Propagation 
Neural Network  - 0.24 - - - - - 2010  Predicting Cutting Temperature 
of Machine  Ant Colony  Support Vector 
Regression  - - 5.2 - - - - 2012  predicting Cutting Temperature 
of Machine  None  Support Vector 
Regression  -  6.2 - - - - 2012    
 8 Table 2.1  Related work evaluation of automated hyperparameter tuning algorithms (Cont¶d)  Problem  Automated 
hyperparameter 
tuning algorithm  Model  Performance  Year  Accuracy  NMSE  MAPE  MSE  RMSE  Test 
error  Time (s)  predicting Cutting Temperature  
of Machine  None  Back Propagation 
Neural Network  - - 6.5 - - - - 2012  System Failure  Artificial Bee 
Colony  Support Vector 
Regression  - - - 0.00716 - - 35.42 (NP =30) 2016  System Failure  Particle Swarm  Support Vector 
Regression  - - - 0.00724  - - 44.87 (NP =30) 2016  System Failure  Grid Search  Support Vector 
Regression  - - - 0.0085  - - 1,481.1  2016  predicting propane¶s price  Artificial Bee 
Colony  Least Square 
Support Vector 
Machine  87.41  - 12.58  - - - - 2012  predicting propane¶s price  None  Back Propagatio n 
Neural Network  84.33  - 15.66  - - - - 2012    9 
 2.2  Theory  2.2.1 Random Forest Random forest is a machine learning algorithm which can be used for classification as well as regression. It is derived from a decision tree, which is a popular algorithm for various machine learning tasks. However, when the tree grows very deep, the decision tree may overfit because the model may learn only specific patterns. Random forest is a method for solving this problem by generating multiple decision trees. Each tree is trained using a subset of the training set. The results from each tree will then be combined to produce a final result that will improve the model's performance.  The random forest training process employs the bagging or bootstrap aggregation method. Tree bagging is composed of many sampling subsets drawn at random from the training set. The bootstrap method allows several training instances, which are sampled with replacement, to be used repeatedly in the training stage. Each decision tree is fitted to a different sampling subset, and the results from all trees are aggregated. Random forest typically uses majority vote to predict a class, as illustrated in Figure 2.1 which means that the most frequent class predicted by individual trees is the final result.  
  Figure 2.1 Random Forest chart [17]   
10 
 2.2.2 Evaluation Metrics for Random Forest Evaluating the performance of a model is a critical step in determining the model efficiency. To assess the performance of random forest models, evaluation metrics that correspond to classification models are employed. Initially, a confusion matrix is a performance metric for a machine learning classification problem with two or more classes as output. As shown in Table 2.2, a confusion matrix is a table with true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values.   - True positive is the number of predictions the model predicts the positive class correctly. - True negative is the number of predictions the model predicts the negative class correctly. - False positive is the number of predictions the model predicts the positive class incorrectly. - False negative is the number of predictions the model predicts the negative class incorrectly.  These values will be used to compute the accuracy and F1-score as shown in Equations 2.1- 2.3.  Table 2.2 Confusion Matrix   Actual Positive  Actual Negative  Predicted Positive  True Positive (TP)  False Posit ive (FP)  Predicted Negative  False Negative (FN)  True Negative (TN)   Accuracy is a straightforward performance metric. It is calculated using Equation 2.1. It is a measure of how accurate the predictions of the model are. Low accuracy indicates that predictions do not match the actual class very well. On the other hand, high accuracy means that the model can correctly predict the majority of the outcomes.  Accuracy = TP+TNTP+TN+FP+FN (2.1)  11 
 Equation 2.4 calculates the F1-score, which requires both the precision and recall scores as shown in Equations 2.2 and 2.3. High precision indicates that the model has a high probability of predicting correctly for all of the predicted positive classes. For example, suppose we have a total of 10 oranges (P) and 10 apples (N), and the model correctly predicts orange 6 times out of 10 and apple 8 times out of 10. This model has high precision. High recall means the model has a high probability of predicting correctly for all of the actual positive classes. For example, if we have total number of oranges (P) is 10, and model predict the orange correctly 9 out of 10 which mean, this model has high recall.  precision = TPTP + FP (2.2)  recall = TPTP + FN (2.3)  F1 = 2 × precisionîrecallprecision+recall (2.4)  2.2.3 Sensitivity Analysis Methods  1. One-at-a-time (OAT) The OAT analysis is the most fundamental analysis method that changes one hyperparameter at a time. This method can be called local analysis because only one hyperparameter is focused when measuring the sensitivity of the performance of the model. OAT begins with initializing the value of the hyperparameters of interest. The value of one hyperparameter is changed in each iteration while the values of the other hyperparameters remain constant. The model performance is then measured and stored in an array for further analysis with ANOVA.  2. Latin Hypercube Sampling (LHS) LHS is a statistical method that generates a set of random samples of hyperparameter values. This algorithm was designed based on the Latin square algorithm, which selects only one sample from each row and column of a square grid. LHS expanded on the concept by creating a multidimensional hypercube. Unlike OAT, all hyperparameters are considered 12 
 simultaneously. The LHS process divides the range of each hyperparameter (dimension) into equal partitions based on its cumulative density function. Then, from each hyperplane, only one sample point will be chosen at random.  3. Analysis of Variance (ANOVA) ANOVA is a statistical analysis tool that examines variation within and between groups. This method is based on the law of total variance, which divides observed variance into components attributable to different sources of variation. It also includes a statistical test for calculating the t-test and p-value for two or more populations. The p-value can be used to find hyperparameters that have a significant impact on the model's performance.  2.2.4 Hyperparameter Tuning Methods 1. Grid Search rid Search is a traditional method for performing automated hyperparameter tuning. It simply searches along with the subset of hyperparameters whose values are manually specified by the user. For example, when using random forest as a classification algorithm, the following hyperparameters values were assigned: max depth as [1, 2, 3], min sample leaf as [4, 5, 6], and min sample split as [7, 8, 9]. Grid search will iterate through all subsets of predefined value combinations and stop when it reaches the last one. It may proceed from (max depth, min sample leaf, min sample split) = (1,4,7) to the final one, which may be (3,6,9). All results produced by each set of value combinations will be ranked based on model performance.  2. Random Search Another traditional method for hypermeter tuning is random search. Ranges of hyperparameter values must be entered, but combinations of values will be chosen at random for training the model. As a result, the number of combinations to be used in training is predetermined, affecting both resources and time. Random search may not be as thorough as grid search in discovering the desired result. On the other hand, random search may by chance select the best results as found in grid search and take less time. For example, to tune random forest hyperparameters, ranges of hyperparameter values, such as 10 ± 100 for max depth, must be defined. The number of iterations or value combinations must then be specified, for example, 100. The random search algorithm will then generate 100 random sets of hyperparameter values from the predefined ranges and run the model for each random set. 13 
 3. Artificial Bee Colony  Artificial Bee Colony (ABC) is an optimization algorithm based on apian nectar-finding behavior. Bees are classified into three types: employed bees, onlooker bees, and scout bees. The employed bees will search for food sources and return the amount of nectar found in those sources. The food sources will be selected by the onlooker bees. When compared to other nearby food sources, the food source that contains a lot of nectar has a high probability of being chosen by onlooker bees. Unimproved food sources will be discarded after a certain period of time and employed bees will be replaced by scout bees to survey and find new food sources. The total number of employed bees, onlooker bees, and scout bees is equal to the total number of possible solutions proposed by Karaboga and Akay [18]. The ABC process consists of five major steps.  Firstly, during the initial phase, the initial population is generated by randomization for employed bees using Equation 2.5, where X⃑റij represents the vector of solution (food source) from randomization, i represents the number of populations, and j represents the dimension size which is the number of hyperparameters. The fitness (quality) of each solution (food source) is then calculated using accuracy or F1-score.  X⃑റij = X⃑റminj+roXnd(rand[0,1ሿ(X⃑റma[j -  X⃑റminj)) (2.5)   Secondly, during the employed bee phase, the employed bee will search for a new food source using Equation 2.6, where v⃑റij represents the new food source, urj represents a randomly selected food source, and ĭ is a random number between [-1, 1]. The fitness is then calculated using accuracy or F1-score. If the new food source has a higher fitness value than the previous one, the old food source will be replaced by the new one.  Y⃑റij = X⃑റij+ ĭij*(Xij-Xrj) (2.6)  Thirdly, during the onlooker bee phase, the onlooker bee will consider the food source based on the probability obtained from Equation 2.7. The higher the quality of the food source, the more likely it is to be chosen. The onlooker bees, like the employed bees, will then compute the fitness value of the chosen food source. 14 
 Pi = fiti(Xi)∑fitiNi=1(Xi) (2.7)   Fourthly, during the scout bee phase, scout bees would begin their work when the quality of a chosen food source does not improve for a predetermined period of time. The scout bee will replace the food source of that bee by searching for a new food source at random using Equation 2.5.  Finally, if the bee finds the best food source that matches the stopping condition or the process completes the maximum number of iterations, the algorithm will stop searching. Otherwise, steps 2±4 will be repeated until a termination criterion is met.    
 CHAPTER 3 METHODOLOGY The goal of this research is to analyze the performance of the ABC optimization algorithm and compare it with the grid search and random search methods during the automated hyperparameter tuning process while training the random forest model. The ABC algorithm, according to our hypothesis, should be able to obtain the best optimal hyperparameter values while reducing computational costs such as time spent. Figure 3.1 depicts the overall framework of this research.  
  Figure 3.1 The overall framework  The framework is divided into three main processes: data preprocessing, sensitivity analysis and hyperparameter tuning. During the data preprocessing process, data was collected from Kaggle and data.world. After that, exploratory data analysis was used to gain a basic understanding of the datasets. Data cleansing was then performed to remove any incorrect or null values, such as people with negative ages. Data transforming techniques were used to transform data in some cases, such as when some features are performed one-hot encoding. During the sensitivity analysis phase, OAT and LHS were used to generate sets of hyperparameters for training the random forest model. The accuracy and F1-score are then computed. Based on the accuracy and F1-score results, ANOVA is used to determine the significance of each hyperparameter. During the hyperparameter 
16 
 tuning process, only significant hyperparameters from the previous process are tuned when training the random forest model. The automated hyperparameter tuning experiments are then carried out using ABC, grid search, and random search. Each set of automated hyperparameter tuning experiments aims to maximize accuracy, F1-score, and accuracy plus F1-score.  3.1 Data Preprocessing Table 3.1 lists the four datasets used. Kaggle and data.world were used to collect medium imbalanced and balanced datasets with a distinct number of features and records. An imbalanced dataset is one in which the number of records for one class exceeds 60% of the total number of records.     Table 3.1 Datasets   Imbalanced  Balanced  Credit Card [19] Bank [20] Medical 
[21] Cardio 
[22] Features  25 17 14 13 Total number of records  30,000  45,211  110,527  70,000  1st class number of records  23,364  39,922  88,208  35,021  2nd class number of records  6,636  5,289  22,319  34,979   It must be noted that the medical dataset was rebalanced in order to create a new balanced dataset. The number of records in the first class has been reduced to 30,000, while the number of records in the second class has remained unchanged.   Data preprocessing is the process of converting raw datasets into clean datasets that can be used in the training model process. Data cleansing and transformation was executed after the data has been explored. Data cleansing is the process of removing null and missing values from data. Data transformation is the process of converting raw data to the format required by the model by using aggregation, normalization, and generalization.   17 
 3.2 Sensitivity Analysis  Sensitivity analysis of the four popular random forest hyperparameters was performed on four variables: n-estimator, max depth, min sample leaf, and min sample split. The OAT and LHS experiments were separately conducted. ANOVA was subsequently applied to each experiment to determine the hyperparameters that have a significant impact on the accuracy and F1-score of the model. Each experiment was studied using the four datasets described in the data preprocessing section.  Table 3.2 OAT Hyperparameter setting  Hyperparameter  Range  Default Value  Median Value  n-estimator  (5, 385)  100 195 max depth  (5, 100)  15 50 min sample leaf  (1, 20)  5 10 min sample split  (2, 40)  10 20  Table 3.2 lists the ranges of the hyperparameters used in the OAT and LHS experiments. The default and median values in the table were only used in the OAT experiment. The hyperparameter ranges and default values were derived from Yang and Shami [23], Koehrsen [24], Meinert [25], and Mithrakumar [26]. The median values were calculated by selecting the value in a set that is closet to the middle of the specific range.  The OAT analysis changes the value of one hyperparameter at a time while keeping the values of the other hyperparameters unchanged. In our OAT experiments, the number of variables was 20. In other words, for each hyperparameter, a sequence of 20 integers in the range shown in Table 3.2 was generated. For example, the set of changing values for max depth was {5, 10, 15, ..., 100} while the other hyperparameter values were fixed. For a more in-depth investigation, two OAT experiments were then performed using different sets of the fixed values, namely the default and median values in Table 3.2.   In the LHS experiment, the same hyperparameter ranges as listed in Table 3.2 were used. The number of values for each hyperparameter was set to 100, which is the most commonly used number for LHS experiments in related works such as Glen [27] and Bozkurt, et al. [28]. 18 
 Given that LHS considers all hyperparameters at the same time, a 100×4 matrix is generated for an LHS sample of 100 values of each of the 4 hyperparameters. The performance of each set of the LHS sample was then tested in the random forest model using the previous mentioned datasets.  Following the completion of the OAT and LHS experiments, ANOVA was used to identify the hyperparameters that were important to the accuracy and F1-score. The significance level or p-value used in these experiments was 0.05, which indicates that if a hyperparameter p-value was less than 0.05 (p-value < 0.05), it has a 95% statistical probability of having a significant effect on model performance.   3.3 Hyperparameter Tuning Grid Search, Random Search, and Artificial Bee Colony was used to perform automated hyperparameter tuning for the same four datasets by considering three objective functions: accuracy, F1-score, and accuracy plus F1-score. As shown in Equation (3.1), the accuracy plus F1-score objective function was added as a third performance metric to study the performance of considering both accuracy and F1-score metrics at the same time.  ACC + F1 = (0.5 × Accuracy) + (0.5 × F1) (3.1)  In grid search experiments, the same hyperparameter ranges as listed in Table 3.2 were used. 90 experiments were carried out with three different numbers of solutions: 81, 256, and 625, each containing a different set of hyperparameters as shown in Table 3.3.  Table 3.3 Grid Search setting  Experiment  Hyperparameter Values  Number 
of 
Solutions  n-estimator  max depth  min sample 
leaf min sample split  1 [5, 100, 195]  [5, 25, 50]  [1, 5, 10]  [2, 11, 20]  81 2 [5, 120, 235]  [5, 30, 60]  [1, 6, 12]  [2, 13, 25]  3 [5, 175, 285]  [5, 35, 70]  [1, 7, 14]  [2, 16, 30]  4 [5, 185, 335]  [5, 45, 85]  [1, 9, 17]  [2, 18, 35]   19 
 Table 3.3 Grid Search setting (Cont¶d)  Experiment  Hyperparameter Values  Number 
of 
Solutions  n-estimator  max depth  min sample 
leaf min sample split  5 [5, 195, 385]  [5, 50, 100]  [1, 10, 20]  [2, 21, 40]  
81 6 [25, 110, 195]  [10, 30, 50]  [2, 6, 10]  [4, 12, 20]  7 [25, 130, 235]  [10, 35, 60]  [2, 7, 12]  [4, 14, 25]  8 [25, 155, 285]  [10, 40, 70]  [2, 8, 14]  [4, 17, 30]  9 [25, 180, 335]  [10, 45, 85]  [2, 9, 17]  [4, 19, 35]  10 [25, 205, 385]  [10, 55, 100]  [2, 11, 20]  [4, 22, 40]  11 [45, 120, 195]  [15, 30, 50]  [3, 6, 10]  [6, 13, 20]  12 [45, 140, 235]  [15, 35, 60]  [3, 7, 12]  [6, 15, 25]  13 [45, 165, 285]  [15, 40, 70]  [3, 8, 14]  [6, 18, 30]  14 [45, 190, 335]  [15, 50, 85]  [3, 10, 17]  [6, 20, 35]  15 [45, 215, 385]  [15, 55, 10 0] [3, 11, 20]  [6, 23, 40]  16 [65, 130, 195]  [20, 35, 50]  [4, 7, 10]  [8, 14, 20]  17 [65, 150, 235]  [20, 40, 60]  [4, 8, 12]  [8, 16, 25]  18 [65, 175, 285]  [20, 45, 70]  [4, 9, 14]  [8, 19, 30]  19 [65, 200, 335]  [20, 50, 85]  [4, 10, 17]  [8, 21, 35]  20 [65, 225,  385]  [20, 60, 100]  [4, 12, 20]  [8, 24, 40]  21 [85, 140, 195]  [25, 35, 50]  [5, 7, 10]  [10, 15, 20]  22 [85, 160, 235]  [25, 40, 60]  [5, 8, 12]  [10, 17, 25]  23 [85, 185, 285]  [25, 45, 70]  [5, 9, 14]  [10, 20, 30]  24 [85, 210, 335]  [25, 55, 8 5] [5, 11, 17]  [10, 22, 35]  25 [85, 235, 385]  [25, 60, 100]  [5, 12, 20]  [10, 25, 40]  26 [13, 186, 251]  [5, 38, 67]  [1, 7, 19]  [2, 11, 24]  27 [37, 145, 235]  [12, 43, 71]  [2, 9, 16]  [4, 14, 27]  28 [59, 161, 301]  [17, 48, 76]  [3, 10, 13]  [5, 17, 31]  29 [71, 186, 332]  [23, 54, 84]  [4, 12, 17]  [7, 19, 34]  30 [93, 206, 364]  [31, 59, 90]  [5, 8, 15]  [9, 22, 38]  31 [5, 65, 130, 195]  [5, 20, 35, 50]  [1, 4, 7, 10]  [2, 8, 14, 20]  256 32 [5, 80, 155, 235]  [5, 20, 40, 60]  [1, 4, 8, 12]  [2, 9, 17, 25]  33 [5, 95, 190, 285]  [5, 25, 45, 70]  [1, 5, 9, 14]  [2, 11, 20, 30]  34 [5, 115, 225, 335]  [5, 30, 55, 85]  [1, 6, 11, 17]  [2, 13, 24, 35]  35 [5, 125, 260, 385]  [5, 35, 65, 100]  [1, 7, 13, 20]  [2, 14, 27, 40]  36 [25, 80, 135, 195]  [10, 20, 35, 50]  [2, 4, 7 , 10] [4, 9, 14, 20]  37 [25, 95, 165, 235]  [10, 25, 40, 60]  [2, 5, 8, 12]  [4, 11, 18, 25]   20 
 Table 3.3 Grid Search setting (Cont¶d)  Experiment  Hyperparameter Values  Number 
of 
Solutions  n-estimator  max depth  min sample 
leaf min sample split  38 [25, 1 10, 195, 285]  [10, 30, 50, 70]  [2, 6, 10, 14]  [4, 12, 21, 30]  
256 39 [25, 125, 230, 335]  [10, 35, 60, 85]  [2, 7, 12, 17]  [4, 14, 24, 35]  40 [25, 145, 265, 385]  [10, 40, 70, 100]  [2, 8, 14, 20]  [4, 16, 28, 40]  41 [45, 95, 145, 195]  [15, 25, 35, 50]  [3, 5, 7, 10]  [6, 10, 15, 20]  42 [45, 105, 170, 235]  [15, 30, 45, 60]  [3, 6, 9, 12]  [6, 12, 18, 25]  43 [45, 125, 205, 285]  [15, 30, 50, 70]  [3, 6, 10, 14]  [6, 14, 22, 30]  44 [45, 140, 235, 335]  [15, 35, 60, 85]  [3, 7, 12, 17]  [6, 15, 25, 35]  45 [45, 1 55, 270, 385]  [15, 40, 70, 100]  [3, 8, 14, 20]  [6, 17, 28, 40]  46 [65, 105, 150, 195]  [20, 30, 40, 50]  [4, 6, 8, 10]  [8, 12, 16, 20]  47 [65, 120, 175, 235]  [20, 30, 45, 60]  [4, 6, 9, 12]  [8, 13, 19, 25]  48 [65, 135, 210, 285]  [20, 35, 50, 70]  [4, 7, 10, 14]  [8, 15, 22, 30]  49 [65, 155, 245, 335]  [20, 40, 60, 85]  [4, 8, 12, 17]  [8, 17, 26, 35]  50 [65, 170, 275, 385]  [20, 45, 70, 100]  [4, 9, 14, 20]  [8, 18, 29, 40]  51 [85, 120, 155, 195]  [25, 30, 40, 50]  [5, 6, 8, 10]  [10, 13, 16, 20]  52 [85, 13 5, 185, 235]  [25, 35, 45, 60]  [5, 7, 9, 12]  [10, 15, 20, 25]  53 [85, 150, 215, 285]  [25, 40, 55, 70]  [5, 8, 11, 14]  [10, 15, 20, 30]  54 [85, 165, 250, 335]  [25, 45, 65, 85]  [5, 9, 13, 17]  [10, 15, 25, 35]  55 [85, 185, 285, 385]  [25, 50, 75, 100]  [5, 10, 15, 20]  [10, 20, 30, 40]  56 [13, 74, 186, 251]  [5, 19, 38, 67]  [1, 7, 12, 19]  [2, 11, 17, 24]  57 [37, 92, 145, 287]  [12, 22, 43, 71]  [2, 5, 9, 16]  [4, 14, 20, 27]  58 [59, 101, 161, 301]  [17, 34, 48, 76]  [3, 6, 10, 13]  [5, 11, 17, 31]  59 [71, 12 1, 186, 332]  [23, 41, 54, 84]  [4, 7, 12, 17]  [7, 19, 26, 34]  60 [93, 153, 206, 364]  [31, 48, 59, 90]  [5, 8, 12, 15]  [9, 15, 22, 38]  61 [5, 50, 95, 145, 195]  [5, 15, 25, 35, 50]  [1, 4, 6, 8, 10]  [2, 6, 10, 15, 20]  625 62 [5, 60, 115, 175, 235]  [5, 15, 30, 45, 60]  [1, 3, 6, 9, 12]  [2, 7, 13, 19, 25]  63 [5, 75, 145, 215, 285]  [5, 20, 35, 50, 70]  [1, 4, 7, 10, 14]  [2, 9, 16, 23, 30]  64 [5, 85, 165, 250, 335]  [5, 25, 45, 65, 85]  [1, 5, 9, 13, 17]  [2, 10, 18, 26, 35]  65 [5, 100, 195, 290, 385]  [5, 25, 50, 75, 100]  [1, 5, 10, 15, 20]  [2, 11, 20, 30, 40]  66 [25, 65, 105, 150, 195]  [10, 20, 30, 40, 50]  [2, 4, 6, 8, 10]  [4, 8, 12, 16, 20]  67 [25, 75, 125, 180, 235]  [10, 20, 30, 45, 60]  [2, 4, 6, 9, 12]  [4, 9, 14, 19, 25]  68 [25, 90, 155, 220, 285]  [10, 25, 40, 55, 70]  [2, 5, 8, 11, 14]  [4, 10, 16, 23, 30]  69 [25, 100, 175, 255, 335]  [10, 25, 45, 65, 85]  [2, 5, 9, 13, 17]  [4, 11, 19, 27, 35]  70 [25, 115, 205, 295, 385]  [10, 30, 50, 75, 100]  [2, 6, 10, 15, 20]  [4, 13, 22, 31, 40]  71 [45, 80, 115, 1 55, 195]  [15, 20, 30, 40, 50]  [3, 4, 6, 8, 10]  [6, 9, 12, 16, 20]  21 
 Table 3.3 Grid Search setting (Cont¶d)  Experiment  Hyperparameter Values  Number 
of 
Solutions  n-estimator  max depth  min sample 
leaf min sample split  72 [45, 90, 135, 185, 235]  [15, 25, 35, 45, 60]  [3, 5, 7, 9, 12]  [6, 10, 15, 20, 25]  
625 73 [45, 105, 165, 225, 285]  [15, 25, 40, 55, 70]  [3, 5, 8, 11, 14]  [6, 12, 18, 24, 30]  74 [45, 115, 185, 260, 335]  [15, 30, 45, 65, 85]  [3, 6, 9, 13, 17]  [6, 13, 20, 27, 35]  75 [45, 130, 215, 300, 3 85] [15, 35, 55, 75, 100]  [3, 7, 11, 15, 20]  [6, 14, 22, 31, 40]  76 [65, 95, 125, 160, 195]  [20, 25, 30, 40, 50]  [4, 5, 6, 8, 10]  [8, 11, 14, 17, 20]  77 [65, 105, 145, 190, 235]  [20, 30, 40, 50, 60]  [4, 6, 8, 10, 12]  [8, 12, 16, 20, 25]  78 [65, 120, 175, 230, 285]  [20, 30, 40, 55, 70]  [4, 6, 8, 11, 14]  [8, 13, 18, 24, 30]  79 [65, 130, 195, 265, 335]  [20, 35, 50, 65, 85]  [4, 7, 10, 13, 17]  [8, 14, 21, 28, 35]  80 [65, 145, 225, 300, 385]  [20, 40, 60, 80, 100]  [4, 8, 12, 16, 20]  [8, 16, 24, 32, 40]  81 [85, 110, 135, 165, 195]  [25, 30, 35, 40, 50]  [5, 6, 7, 8, 10]  [10, 12, 14, 17, 20]  82 [85, 120, 155, 195, 235]  [25, 30, 40, 50, 60]  [5, 6, 8, 10, 12]  [10, 13, 17, 21, 25]  83 [85, 135, 185, 235, 285]  [25, 35, 45, 55, 70]  [5, 7, 9, 11, 14]  [10, 15, 20, 25, 30]  84 [85, 145, 205, 270, 335]  [25, 40, 55, 70, 85]  [5, 8, 11, 14, 17]  [10, 16, 22, 28, 35]  85 [85, 160, 235, 310, 385]  [25, 40, 60, 80, 100]  [5, 8, 12, 16, 20]  [10, 17, 24, 32, 40]  86 [13, 74, 186, 251, 314]  [5, 19, 38, 67, 88]  [1, 7, 12, 1 5, 19]  [2, 11, 17, 24, 30]  87 [37, 92, 145, 287, 311]  [12, 22, 43, 71, 91]  [2, 5, 9, 13, 16]  [4, 14, 20, 27, 32]  88 [59, 101, 161, 239, 301]  [17, 34, 48, 76, 92]  [3, 6, 10, 13, 18]  [5, 11, 17, 31, 36]  89 [71, 121, 186, 263, 332]  [23, 41, 54, 84, 94]  [4, 7, 12, 14, 17]  [7, 12, 19, 26, 34]  90 [93, 153, 206, 273, 364]  [31, 48, 59, 72, 90]  [5, 9, 12, 15, 19]  [9, 15, 22, 29, 38]   In random search experiments, the same hyperparameter ranges as listed in Table 3.2 were used. For a fair comparison, the number of experiments and solutions were comparable to grid search experiments as shown in Table 3.4. In each experiment, a random set of hyperparameter values within the specified ranges was generated.      22 
 Table 3.4 Random Search setting  Experiment  Hyperparameter Range  Number of 
Solutions  n-estimator  max depth  min sample leaf  min sample split  1 - 30 (5, 385)  (5, 100)  (1, 20)  (2, 40)  81 31 - 60 256 61 - 90 625  In ABC experiments, the same hyperparameter ranges as listed in Table 3.2 were also used. The number of experiments was the same for grid search and random search, but the number of solutions differed slightly. As shown in Equation (3.2), the number of possible ABC solutions was calculated by adding the number of employee bees at the beginning and the total number of employed bees and onlooker bees that find food sources in all generations, plus the number of scout bees if they were triggered.  No.of Sol. = Initial NOE + ((NOE+NOO) × Generation) + NOS   (3.2)  NOE denotes the number of employed bees that went in search for new food sources. NOO denotes the number of onlooker bees that selected food sources from the employed bees. NOS denotes the number of scout bees that will replace old, worse food sources whose quality has not improved by the end of a predetermined abandon cycle. If the abandon cycle is 3, for example, the scout bee will start working after the quality of the food source has remained unchanged for two generations. Generation denotes the number of iterations of the entire ABC process. The ABC algorithm runs until the number of generations is reached or until the termination criterion is met. Another termination criterion introduced in this thesis is the number of generations in which the best objective value remains unchanged. For example, if Generation is 13 and Termination Criterion is 6, the ABC process will terminate when either the process has completed 13 iterations, or the best objective values have remained constant for 6 iterations.  Table 3.5 tabulates the ABC configurations. Even though we attempted to obtain the same number of solutions for a fair comparison, the number of ABC solutions differs slightly from grid search and random search, as previously mentioned. This distinction will be considered when the results of the three methods are compared. 23 
 Table 3.5 Artificial Bee Colony setting  Experiment  NOE  NOO  NOS  Abandon 
Cycle  Generation  Termination 
Criterion  Number 
of 
solutions  1 - 30 3 3 1 3 13 6 82 31 - 60 4 4 1 8 32 16 261 61 - 90 5 5 1 15 62 31 626    
 CHAPTER 4 EXPERIMENTAL RESULTS AND DISCUSSION This chapter presents the results of the experiments that were designed in the previous chapter. The four datasets described in the data preprocessing section are used in all experiments. Section 4.1, the ANOVA results of the sensitivity analysis experiments are reported to determine the significance of each hyperparameter. Section 4.2 shows the results of the automated hyperparameter tuning experiments using only the significant hyperparameters from the prior analysis are presented. Section 4.3 presents the results of the automated hyperparametric tuning experiments along with further investigation and discussion.  4.1  Sensitivity Analysis Two sets of experiments were carried out using the OAT and LHS techniques to examine the sensitivity of the four popular random forest hyperparameters. After the experiments were completed, ANOVA was used to determine the significance of each hyperparameter. This section contains the results of the sensitivity analysis.  4.1.1 OAT Experiments Table 4.1 displays the ANOVA results from the OAT experiment with the default fixed values. The labels Acc., F1, and Sig in the table respectively represent accuracy, F1-score, and significance. It is obvious that min sample leaf has a statistically significant effect on both accuracy and F1-score of all datasets, whereas the other hyperparameters had a significant effect on only some performance of some datasets.  Table 4.1 ANOVA result of the OAT experiment with the default fixed value  Hyperparameter  Credit Card  Bank  Cardio  Medical  Acc. F1 Acc. F1 Acc. F1 Acc. F1 n-estimator  Sig Sig Sig Sig - - Sig - max depth  Sig Sig Sig Sig - - Sig - min sample leaf  Sig Sig Sig Sig Sig Sig Sig Sig min sample split  Sig Sig - Sig Sig Sig - Sig 25 
 Table 4.2 presents the ANOVA results from another OAT experiment with fixed median values. In contrast to the previous findings, no hyperparameters were statistically significant for all measures across all datasets. Moreover, the hyperparameters that had a significant effect on the performance of one dataset in the previous experiment had no effect on the performance of the same dataset in this experiment. As shown in Table 4.1, the n-estimator and max depth had a significant impact on both the accuracy and F1-score of the credit card dataset. However, as shown in Table 4.2, the two hyperparameters had no significant effect on both accuracy and F1-score of the same dataset. Thus, the results for these OAT experiments not only reported that each hyperparameter is statistically significant to different random forest model performance for different datasets, but also revealed that using different sets of fixed values in the OAT analysis can result in contrasting outputs.  Table 4.2 ANOVA result of the OAT experiment with the median fixed value  Hyperparameter  Credit Card  Bank  Cardio  Medical  Acc. F1 Acc. F1 Acc. F1 Acc. F1 n-estimator  - - Sig - Sig Sig - - max depth  - - Sig Sig - - Sig - min sample leaf  - Sig Sig Sig Sig Sig Sig Sig min sample split  Sig - Sig Sig Sig Sig - -  4.1.2 LHS Experiments The ANOVA results are presented in Table 4.3. The interaction between hyperparameters a and b is represented by a:b in the hyperparameter column. n-estimator, max depth and min sample split have a significant effect on the performance of some datasets. In addition, the interaction of these three hyperparameters was significant to the accuracy on the medical appointment dataset.    The results also indicated that the random forest performance with these four datasets was mostly affected by min sample leaf. These findings were consistent with the OAT experiments. Furthermore, the interaction effects of min sample leaf and other 26 
 hyperparameters, such as n-estimator: min sample leaf, max depth: min sample leaf, and min sample leaf: min sample split were primarily significant to the performance.  Table 4.3 ANOVA result of LHS experiment  Hyperparameter  Credit Card  Bank  Cardio  Medical  Acc. F1 Acc. F1 Acc. F1 Acc. F1 n-estimator  Sig Sig - - Sig Sig - - max depth  - Sig Sig Sig - - Sig - min sample  leaf - Sig Sig Sig Sig Sig Sig Sig min sample  split Sig - - Sig Sig Sig Sig  n-estimator:  max depth  - - - - - - - - n-estimator: min sample leaf  - - Sig Sig - - - Sig n-estimator: min sample split  - - - - - - - - max depth:min  sample leaf  - - - - Sig Sig Sig Sig max depth:min  sample split  - - - - - - - - min sample leaf: min sample split  Sig - - - Sig Sig  Sig n-estimator:  max depth:min sample  leaf - - - - - - - - n-estimator: max depth:min sample split - - - - - - Sig - n-estimator: min sample leaf: min sample split - - - - - - - - max depth: min sample leaf: min sample split Sig - - - - - - - n-estimator: max depth: min sample leaf: min sample split - - - - - - - -  Finally, the results of the sensitivity analysis experiments showed that all hyperparameters have a relatively significant effect on the random forest performance. As a result, all hyperparameters are included in the automated hyperparameter tuning experiments in our study.      27 
 4.2 Automated Hyperparameter Tuning Grid Search, Random Search, and Artificial Bee Colony were used in the automated hyperparameter tuning experiments. Three objective functions were considered: accuracy, F1-score, and accuracy plus F1-score. The experiments were carried out using the four datasets and configurations identified in the previous chapter. Section 4.3 examines and discusses the results of automated hyperparametric tuning experiments in greater depth.  The result tables for all experiments are included in the appendix. For ease of evaluation and discussion, the four results tables presented in the following sections for each objective function were derived from the full tables. As previously stated, the three sets of the number of solutions from grid search and random search experiments (ABC experiments) were 81 (82), 256 (261), and 625 (626). It should be noted that the grey rows in each table represented the results of the 256 (261) solution experiment.  The first two tables present the best (maximum) objective value and the shortest execution time with the best set of hyperparameter values for each set of the number of solutions. In these tables, the solution with the best objective value or the shortest execution time from each set of the number of solutions is highlighted in red.  The last two tables show the statistical (min, max, and mean) values of the objective value and the execution time, respectively. The solution with the highest mean and maximum objective values or the lowest minimum and mean execution time values from each set of the number of solutions is highlighted in red.  The results of the three hyperparameter tuning algorithms with each set of solutions are compared. In all tables, the orange and green boxes represent the highest maximum objective value and the shortest minimum execution time, respectively. In the statistics tables, the yellow and blue boxes represent the highest mean objective value and the lowest mean execution time value, respectively.   28 
 4.2.1 Single Objective Function: Accuracy In this section, accuracy is considered as an objective function. Tables 4.4-4.7 present the results of the best accuracy, the shortest execution time, and the statistical (min, max, and mean) measures of both accuracy and execution time values from automated hyperparameter tuning experiments.  According to Table 4.4, ABC has the highest accuracy in three datasets: credit card, cardiovascular disease, and medical appointment on experiments with 81 (82) and 256 (261) solutions. In experiments with 625(626) solutions, random search achieves the highest accuracy in almost all datasets except for the medical appointment dataset.  Thus, when considering accuracy as an objective function with a smaller number of solutions, ABC appears to be the most appropriate algorithm whereas random search appears to be more appropriate for a larger number of solutions. The execution time results in Table 4.5, on the other hand, show that ABC took the least amount of execution time in every dataset and set of solutions. This was due to the termination criterion, which allows the process to finish before reaching the maximum generation. Furthermore, as shown in Table 4.5, ABC was still able to achieve the highest accuracy compared to the others in the majority of datasets and sets of solutions.   
 29 Table 4.4 Results of hyperparameter tuning experiments with the highest accuracy (best objective value)  Solutions  Grid Search  Random Search  Artificial Bee Colony  Accuracy 
(Obj.)  F1-
score  Execution 
Time (s)  Best 
Hyperparameter  Accuracy (Obj.) F1-
score  Execution 
Time (s)  Best 
Hyperparameter  Accuracy (Obj.) F1-
score  Execution 
Time (s)  Best 
Hyperparameter  Credit Card  81 (82)  82.24  67.97  57.13  [235, 12, 2, 27]  82.26  67.88  69.6 [141, 7, 2, 25]  82.26  67.98  100.41  [328, 10, 5, 34]  256 (261 ) 82.24  68.01  260.12  [385, 10, 2, 40]  82.29  67.97  252.82  [139, 7, 6, 8]  82.31  67.99  153.12  [148, 7, 6, 17]  625 (626)  82.24  68.01  657.21  [385, 10, 2, 40]  82.28  67.89  611.68  [106, 7, 4, 14]  82.27  68.03  745.98  [318, 94, 5, 21]  Bank Marketing  81 (82)  90 68.14 80.99  [25, 45, 2, 35]  90.02  67.69  92.25  [84, 91, 1, 37]  90 67.38  81.3 [163, 50, 2, 31]  256 (261)  90 67.42  183.77  [74, 19, 1, 24]  90.05  67.91  311.59  [69, 83, 1, 37]  90.03  67.08  102.6  [46, 64, 5, 23]  625 (626)  90.01  68 585.43  [85, 25, 1, 35]  90.07  67.27  741.29  [68, 45, 4, 36]  90.06  67.98  184.59  [25, 44, 3, 38]  Cardiovascular Disease  81 (82)  74.1 74.04  110.91  [186, 54, 17, 7]  74.1 74.04  104.53  [186, 76, 17, 31]  74.11  74.06  84.23  [163, 66, 17, 4]  256 (261)  74.12  74.07  375.72  [165, 25, 17, 35]  74.11  74.06 327.98  [173, 79, 17, 38]  74.12  74.07  313.35  [187, 73, 17, 37]  625 (626)  74.12  74.07  687.32  [165, 25, 17, 35]  74.12  74.07  866.03  [165, 63, 17, 35]  74.11  74.06  566.54  [163, 35, 17, 29]  Medical Appointment  81 (82)  60.61  55.62  56.01  [25, 40, 2, 17]  60.61  55.52  74.19  [28, 45, 4, 16]  60.62  55.61  74.06  [22, 12, 2, 10]  256 (261)  60.61  55.62  228.99  [25, 40, 2, 16]  60.63  55.57  228.03  [20, 88, 1, 8]  60.64  55.58  141.4  [20, 69, 1, 11]  625 (626)  60.61  55.62  434.87  [25, 25, 2, 16]  60.62  55.57  541.25  [18, 78, 3, 8]  60.64  56.07  275.21  [7, 16, 2, 9]     
 30 Table 4.5 Results of hyperparameter tuning experiments with the least amount of time consumed on accuracy as an objective value  Solutions  Grid Search  Random Search  Artifici al Bee Colony  Accuracy 
(Obj.)  F1-score  Execution 
Time (s)  Best 
Hyperparameter  Accuracy (Obj.) F1-score  Execution 
Time (s)  Best 
Hyperparameter  Accuracy (Obj.) F1-score  Execution 
Time (s)  Best 
Hyperparameter  Credit Card  81 (82)  82.16  67.67  43.21  [195, 5,  1, 2] 82.2 67.93  68.32  [293, 11, 3, 28]  82.12  67.57  35.36  [110, 13, 3, 8]  256 (261)  82.17  67.68  126.03  [195, 5, 4, 2]  82.2 67.87  240.06  [232, 9, 10, 15]  82.1 67.69  81.31  [82, 90, 18, 39]  625 (626)  82.17  67.68  311.12  [195, 5, 4, 2]  82.21  67.89  586.21  [352, 9, 3, 3]  82.27  67.86  288.18  [132, 7, 8, 29]  Bank Marketing  81 (82)  89.88  66.25  43.85  [195, 25, 5, 20]  89.93  67.37  83.54  [133, 76, 1, 30]  89.88  65.83  25.54  [107, 44, 12, 21]  256 (261)  89.97  67.35  142.3  [65, 20, 1, 14]  89.95  66.55  276.57  [125, 63, 6, 1 8] 89.99  66.77  99.61  [69, 57, 7, 19]  625 (626)  89.96  66.51  352.12  [50, 15, 4, 15]  90.03  67.33  679.8  [34, 27, 3, 38]  90.06  67.98  184.59  [25, 44, 3, 38]  Cardiovascular Disease  81 (82)  73.94  73.88  51.08  [195, 25, 10, 2]  74.06  74.01  94.16  [249, 42, 11, 23]  74.09  74.03  44.58  [168, 51, 14, 24]  256 (261)  73.93  73.87  166.35  [195, 20, 10, 2]  74.07  74.02  305.17  [138, 34, 17, 7]  74.09  74.03  143.94  [222, 19, 12, 23]  625 (626)  73.95  73.89  411.97  [95, 15, 6, 2]  74.08  74.02  784.8  [176, 51, 16, 8]  74.09  74.03  386.14  [168, 70, 14, 8]  Medical Appointment  81 (82)  60.51  55.52  33.68  [100, 25, 1, 2]  60.55  55.57  65.71  [13, 19, 2, 2]  60.59  55.61  15.58  [25, 61, 4, 14]  256 (261)  60.52  55.51  107.64  [65, 20, 1, 2]  60.61  55.52  208.55  [28, 75, 3, 17]  60.62  55.56  47.14  [20, 25, 2,  11] 625 (626)  60.51  55.25  303.88  [50, 5, 1, 2]  60.61  55.51  518.84  [20, 79, 2, 32]  60.62  55.58  112.42  [18, 89, 3, 10]    31 
 Table 4.6 Accuracy statistics from automated hyperparameter tuning experiments  Soluti ons Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Credit Card  81 (82)  82.04  82.24  82.15  82.13  82.26  82.19  82.04  82.26  82.17  256 (261)  82.11  82.24  82.17  82.18  82.29  82.22  82.1 82.31  82.22  625 (626)  82.12  82.24  82.18  82.2 82.28  82.23  82.18  82.27  82.23  Bank Marketing  81 (82)  89.86  90 89.93  89.91  90.02  89.95  89.87  90 89.94  256 (261)  89.9 90 89.95  89.94  90.05  89.98  89.9 90.03  89.98  625 (626)  89.92  90.01  89.96  89.96  90.07  90.01  89.94  90.06  90.00  Cardiovascular D isease  81 (82)  73.93  74.1 74.03  74.03  74.1 74.07  73.98  74.11  74.06  256 (261)  73.93  74.12  74.04  74.04  74.11  74.08  74 74.12  74.08  625 (626)  73.94  74.12  74.04  74.07  74.12  74.09  74.02  74.11  74.09  Medical Appointment  81 (82)  60.48  60.61  60.52  60.5 60.61  60.55 60.47  60.62  60.52  256 (261)  60.49  60.61  60.52  60.51  60.63  60.58  60.49  60.64  60.58  625 (626)  60.49  60.61  60.53  60.57  60.62  60.60  60.49  60.64  60.60   Table 4.6 exhibits the accuracy statistics from the 30 experiments performed for each set of solutions. It is interesting to note that random search yields no results with the lowest minimum accuracy, whereas grid search yields the most results with the lowest minimum accuracy. When maximum accuracy is assessed, ABC has the most results with the highest maximum accuracy when compared to the other. Moreover, it achieves the highest maximum accuracy in all sets of solutions from the medical appointment dataset. However, random search performs well and achieves the highest maximum accuracy in all sets of number of solutions from the bank marketing dataset. Finally, mean accuracy, the average of the maximum accuracy of the 30 experiments, is considered. According to the statistics, random search has the highest mean accuracy across all solution numbers and datasets. ABC also has many results with the highest mean accuracy, some of which are equal to those from random search, such as the 256 (261) and 625 (626) solutions from the credit card dataset. Grid search does not have the highest mean accuracy when compared to the others. 32 
 Table 4.7 Execution time statistics from automated hyperparameter tuning experiments on accuracy as an objective value  Solutions  Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Credit Card  81 (82)  43.21  100.65  70.89  68.32  82.08  76.25  35.36  108.51  70.28  256 (261)  126.03  319.13  221.89  240.06  266.61  252.11  81.31  354.34  258.42  625 (626)  311.12  790.15  560.39  586.21  635.47  613.33  288.18  813.54  560.05  Bank Marketing  81 (82)  43.85  113.24  79.48  83.54  107.47  94.24  25.54  126.48  66.13  256 (261)  142.3  357.85  249.27  276.57  316.83  295.91  99.61  370.24  207.09  625 (626)  352.12  875.46  623.42  679.8  743.06  721.03  184.59  572.54  373.67  Cardiovascular Disease  81 (82)  51.08  131.5  92.72  94.16  117.23  106.93  44.58  164.58  91.7 256 (261)  166.35  417.13  291.2  305.17  372.28  343.67  143.94  401.21  279.59  625 (626)  411.97  1015.9  728.04  784.8  866.03  829.29  386.14  821.94  623.08  Medical Appointment  81 (82)  33.68  90.81  62.42  65.71  85.06  72.51  15.58  99.15  49.47  256 (261)  107.64  274.2  191.48  208.55  242.86  227.44  47.14  216.7  119.65  625 (626)  303.88  662.82  480.59  518.84  577.72  550.61  112.42  414.87  220.23   Table 4.7 displays execution time statistics with accuracy as the objective function. ABC has the shortest minimum execution time in all sets of solutions across all datasets due to the termination criteria. When maximum execution time is observed, ABC also has the highest maximum execution time, particularly in experiments with a small number of solutions, whereas grid search has the longest execution time in nearly all experiments with larger number of solutions. With the exception of the credit card dataset with 256 (261) solutions, where grid search obtained the lowest mean execution time, ABC achieved the lowest mean execution time in all sets of number of solutions. It is also worth noting that, while random search produced the highest mean accuracy in all sets of solutions discussed previously, it also has the highest mean execution time in all cases except the credit card dataset's 256 (261) solutions.  33 
 In summary, the results in this section show how the three automated hyperparameter tuning algorithms performed when accuracy was the objective function. Random search has the highest mean accuracy for all cases but takes longer to execute than ABC. Therefore, if the only concern is accuracy rather than execution time, random search is preferable. However, if both accuracy and execution time are required, ABC is the better choice with the shortest mean execution time compared to the other two algorithms while also providing the same highest mean accuracy as random search in many cases.  4.2.2 Single Objective Function: F1-score The F1-score is an objective function of the experiments in this section. Tables 4.8 ± 4.11 exhibit the best F1-score, shortest execution time, and statistical (min, max, and mean) measures for F1-score and execution time from automated hyperparameter tuning experiments.  Table 4.8 displays the F1-score results. Grid search has the best F1-score in nearly every set of number of solutions and datasets except for the 256 (261) solutions of the bank marketing dataset, which is outperformed by ABC. In addition, ABC can get the best F1-score in the other 5 cases, similar to grid search. Table 4.8 also shows that, while the F1-score values from the three algorithms are equal, as in the cardiovascular disease dataset, the best hyperparameter values are not.  As shown in Table 4.9, ABC provides the shortest execution time in all cases apart from the bank marketing dataset, where grid search performs faster. However, ABC can only achieve two results with the highest F1-score and the shortest execution time. In contrast to Table 4.5, ABC provides the highest accuracy while taking the least amount of time to execute in the majority of datasets and sets of solutions.     
 34 Table 4.8 Result of hyperparameter tuning experiments with the highest f1-score (best objective value)  Solut ions Grid Search  Random Search  Artificial Bee Colony  Accuracy  F1-
score 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Accuracy  F1-
score  
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Accuracy  F1-
score 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Credit C ard 81 (82)  82.04  68.32  72.83  [385, 50, 1, 2]  82.22  68.22  78.02  [340, 53, 3, 40]  82.03  68.27  92.78  [351, 96, 1, 2]  256 (261)  82.11  68.32  179.9  [285, 25, 1, 2]  82.03  68.27  261.92  [291, 72, 1, 2]  82.08  68.32  212.2  [287, 48, 1, 2]  625 (626)  82.11  68.35  525.03 [335, 25, 1, 2]  82.06  68.33  609.43  [353, 88, 1, 2]  82.06  68.3 670.71  [309, 29, 1, 2]  Bank Marketing  81 (82)  90 68.14  80.73  [25, 45, 2, 35]  90 67.96  90.37  [75, 63, 1, 35]  90.02  68.04  68.99  [23, 82, 3, 32]  256 (261)  90 68.14  258.96  [25, 35, 2, 35]  89.99 67.95  286.99  [85, 30, 1, 33]  90.03  68.15  188.55  [23, 50, 3, 34]  625 (626)  90 68.14  636.13  [25, 25, 2, 35]  90.03  68.09  728.97  [29, 30, 1, 38]  90.04  68.05  296.12  [79, 36, 1, 35]  Cardiovascular Disease  81 (82)  74.09  74.04  87.18  [185, 45, 17, 2]  74.1 74.04 103.74  [175, 81, 17, 8]  74.1 74.04  52.62  [175, 56, 18, 30]  256 (261)  74.12  74.07  377.42  [165, 25, 17, 35]  74.12  74.07  344.89  [165, 27, 17, 38]  74.12  74.07  297.49  [165, 25, 17, 35]  625 (626)  74.12  74.07  691.22  [165, 25, 17, 35]  74.11  74.06  827.3  [165, 99, 17, 29]  74.12  74.07  585.61  [165, 48, 17, 35]  Medical Appointment  81 (82)  60.46  56.1 39.32  [5, 30, 1, 2]  60.34  55.99  62.68  [5, 23, 1, 37]  60.36  55.97  27 [5, 95, 11, 28]  256 (261)  60.48  56.12  106.2  [5, 20, 1, 8]  60.64  56.07  221.03  [7, 57, 2, 9]  60.44  56.09  74.79  [5, 46, 1, 13]  625 (626)  60.48  56.12  304.77  [5, 15, 1, 6]  60.46  56.1 559.98  [5, 13, 1, 2]  60.48  56.12  374.03  [5, 32, 1, 6]    
 35 Table 4.9 Results of hyperparameter tuning experiments with the least amount of time consumed on f1-score as an objective value  Solutions  Grid Search  Random Search  Artificial Bee Colony  Accuracy  F1-score 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Accuracy  F1-score 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Accuracy  F1-score 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Credit Card  81 (82)  81.97  68.09  43.31  [195, 25, 1, 2]  82.19  68.07  74.01  [305, 96, 5, 23]  81.94  67.97  18.45  [39, 24, 2, 39]  256 (261)  81.89  68.03  126.21  [195, 35, 1, 2]  82.19  68 245.07  [293, 21, 19, 1 1] 82.24  68.23  180.53  [259, 96, 5, 33]  625 (626)  81.97  68.09  311.78  [195, 25, 1, 2]  81.96  68.06  590.54  [190, 69, 1, 2]  82.12  68.1 284.58  [207, 28, 2, 31]  Bank Marketing  81 (82)  89.86  66.74  43.66  [100, 25, 1, 20]  89.89  67.6 76.92  [19, 53, 1, 40]  89.91  67.71 14.95  [37, 71, 1, 30]  256 (261)  89.97  67.35  141.8  [65, 20, 1, 14]  90.01  67.42  262.32  [51, 86, 4, 36]  90 67.17  74.78  [23, 57, 6, 11]  625 (626)  89.94  67.24  350.25  [95, 35, 1, 10]  89.97  67.89  676.74  [35, 55, 2, 38]  89.83  66.48  144.26  [21, 28, 13, 3]  Cardiovascular Disease  81 (82)  73.94  73.88  53.51  [195, 25, 10, 2]  74.07  74.02  97.75  [181, 17, 17, 32]  73.99  73.93  32.39  [171, 19, 5, 26]  256 (261)  73.93  73.87  167.62  [195, 20, 10, 2]  74.1 74.04  322.5  [175, 37, 17, 2]  74.07  74 70.05  [46, 23, 15, 33]  625 (6 26) 73.94  73.89  413.67  [95, 15, 4, 20]  74.1 74.04  802.06  [175, 82, 18, 33]  74.07  74.01  274.16  [98, 90, 14, 12]  Medical Appointment  81 (82)  60.46  56.1 36.84  [5, 25, 1, 2]  60.34  55.99  62.68  [5, 23, 1, 37]  60.49  55.49  19.74  [107, 68, 14, 34]  256 (261)  60.46 56.1 126.48  [5, 20, 1, 2]  60.34  55.99  215.46  [5, 16, 1, 24]  60.53  55.53  64.92  [82, 75, 2, 7]  625 (626)  60.48  56.12  304.77  [5, 15, 1, 6]  60.34  56.02  542.04  [5, 16, 8, 12]  60.44  56.1 182.82  [5, 41, 2, 9]    36 
 Table 4.10 F1-score statistics from automated hyperparameter tuning experiments  Solutions  Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Credit Card  81 (82)  67.69  68.32  67.98  67.86  68.22  68.04  67.61  68.27  67.94  256 (261)  67.69  68.32  68.01  67.98  68.27  68.10  67.83  68.32  68.06  625 (626)  67.72  68.35  68.04  68.04  68.33  68.14  67.93  68.3 68.14  Bank Marketing  81 (82)  66.67  68.14  67.17  66.78  67.96  67.25  65.9 68.04  67.10  256 (261)  66.69  68.14  67.26  67.05  67.95  67.54  66.61 68.15  67.51  625 (626)  66.67  68.14  67.31  67.38  68.09  67.71  66.48  68.05  67.53  Cardiovascular Disease  81 (82)  73.87  74.04  73.97  73.98  74.04  74.01  73.92  74.04  74.00  256 (261)  73.87  74.07  73.98  74 74.07  74.03  73.99  74.07  74.03  625 (626)  73.88  74.07  73.99 74 74.06  74.03  74.01  74.07  74.04  Medical Appointment  81 (82)  55.49  56.1 55.62  55.5 55.99  55.65  55.49  55.97  55.56  256 (261)  55.49  56.12  55.63  55.52  56.07  55.82  55.5 56.09  55.62  625 (626)  55.49  56.12  55.63  55.63  56.1 55.93  55.52  56.12  55.83   When observing F1-score statistics in Table 4.10, the maximum f1-score is the same as shown in Table 4.8 and previously discussed. The mean f1-score, on the other hand, was computed by averaging the maximum F1-scores of the 30 experiments. The results show that, similar to the mean accuracy, random search achieves the highest mean f1-score in almost all solutions and datasets, with the exception of the cardiovascular disease dataset with 625(626) solutions, for which ABC has the highest mean F1-score. Grid search, however, does not have a single highest mean F1-score.     37 
 Table 4.11 Execution time statistics from automated hyperparameter tuning experiments on f1-score as an objective value  Solutions  Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Credit Card  81 (82)  43.31  100.92  71.15  74.01  88.74  80.58  18.45  107.52  74.91  256 (261)  126.21  318.55  222.23  245.07  275.49  262.41  180.53  344.91  257.04  625 (626)  311.78  789.15  561.05  590.54 655.94  621.73  284.58  782.74  578.52  Bank Marketing  81 (82)  43.66  113.19  79.42  76.92  104.25  91.78  14.95  101.31  61.26  256 (261)  141.8  355.04  247.22  262.32  312.21  291.55  74.78  383.71  177.47  625 (626)  350.25  928.72  648.01  676.74  740.72  706.23  144.26  775.8 353.52  Cardiovascular Disease  81 (82)  53.51  132.62  93.38  97.75  122.3  111.08  32.39  134.04  84.204  256 (261)  167.62  416.64  292.03  322.5  370.51  343.48  70.05  377.42  259.93  625 (626)  413.67  1024.9  735.7  802.06  863.2  833.58  274.16  816.67  572.49  Medical A ppointment  81 (82)  36.84  88.97  61.35  62.68  79.75  72.52  19.74  107.82  46.06  256 (261)  106.2  272.38  190.36  215.46  240.06  229.56  64.92  350.72  167.72  625 (626)  304.77  658.14  479.45  542.04  575.23  559.24  182.82  589.72  322.79   Table 4.11 displays the execution time statistics for experiments with F1-score as the objective function. Except for the 256 (261) solutions in the credit card dataset, where grid search performs faster, ABC has the shortest minimum execution time in almost every result. In terms of mean execution time, grid search achieves the shortest mean execution time for the credit card dataset, while ABC performs better for the remaining three datasets. Although random search produces the best mean f1-score in almost all cases, it typically takes longer to execute than the other two algorithms.  In conclusion, when F1-score is chosen as the objective function, grid search yields a higher maximum F1-score while random search yields a higher mean F1-score. In terms of execution time, ABC still outperforms the other two algorithms.    38 
 4.2.3 Multiple Objective Functions: Accuracy and F1-score   The objective function used in this section is the summation of accuracy and F1-score. The best objective value (accuracy plus f1-score), shortest execution time, and statistical (min, max, and mean) objective values and execution time from automated hyperparameter tuning experiments are shown in Tables 4.12±4.15.  Table 4.12 shows the outcomes of experiments where the accuracy plus F1-score value is observed. In the experiments with 81 (82) solutions, ABC performs slightly better than grid search and random search. ABC achieves the highest objective value in two different datasets, whereas grid search and random search can only achieve the highest objective value in one dataset. In the experiments with 256 (261) solutions, ABC provides the highest objective value in three datasets: credit card, cardiovascular disease, and medical appointment. Moreover, ABC gets the highest objective value in all datasets in experiments with 625 (626) solutions. In terms of execution time, as shown in Table 4.13, ABC continues to demonstrate the shortest execution time in most experiments, apart from two cases where the grid search completed faster. Moreover, in all datasets, ABC can achieve at least one result with the highest objective value and the shortest execution time.  Table 4.14 displays the objective value statistics. In terms of mean objective value, random search still outperforms the other two algorithms in most cases. However, in three cases, ABC has a higher mean objective value than random search, while in the other two cases, ABC and random search have the same mean objective values.   
 39 Table 4.12 Result of hyperparameter tuning experiments with the highest accuracy plus f1-score (best objective value)  Solutions  Grid Search  Random Search  Artificial Bee Colony  Accuracy  F1-
score  Acc + 
F1 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Accuracy  F1-
score  Acc + 
F1 
(Obj.)  Executio n 
Time (s)  Best 
Hyperparameter  Accuracy  F1-
score  Acc + 
F1 
(Obj.)  Time  
Consuming  Best 
Hyperparameter  Credit Card  81 (82)  82.04  68.32  75.18  72.89  [385, 50, 1, 2]  82.22  68.15  75.18  75.83  [298, 99, 5, 37]  82.13  68.34  75.24  71.39  [324, 25, 1, 2]  256 (261)  82.11 68.32  75.22  180.98  [285, 25, 1, 2]  82.19  68.22  75.2 257.66  [333, 78, 2, 40]  82.32  68.22  75.27  271.55  [331, 73, 5, 21]  625 (626)  82.11  68.35  75.23  523.53  [335, 25, 1, 2]  82.29  68.17  75.23  635.06  [329, 44, 5, 20]  82.24  68.23  75.24  477.29  [259, 44, 5, 33 ] Bank Marketing  81 (82)  90 68.14  79.07  81.51  [25, 45, 2, 35]  90.01  67.95  78.98  89.47  [85, 90, 1, 38]  90.03  68.05  79.04  36.09  [25, 53, 3, 33]  256 (261)  90 68.14  79.07  261.8  [25, 35, 2, 35]  89.96  68.1 79.03  297.24  [35, 15, 1, 38]  90.05  68.03  79.04  118.88  [81, 62, 1, 35]  625 (626)  90 68.14  79.07  650.94  [25, 25, 2, 35]  90.02  68.11  79.07  757.11  [23, 93, 3, 35]  90.03  68.15  79.09  118.48  [23, 93, 3, 34]  Cardiovascular Disease  81 (82)  74.1 74.04  74.07  113.42  [186, 54, 17, 7]  74.1 74.04  74.07  98.57  [186, 51, 1 7, 36]  74.1 74.05  74.08  62.29  [179, 22, 17, 19]  256 (261)  74.12  74.07  74.1 379.22  [165, 25, 17, 35]  74.1 74.05  74.08  324.76  [271, 96, 11, 40]  74.12  74.07  74.1 289.04  [165, 51, 17, 37]  625 (626)  74.12  74.07  74.1 689.97  [165, 25, 17, 35]  74.11  74.06  74.08  828.91  [165, 22, 17, 33]  74.12  74.07  74.1 639.04  [187, 90, 17, 37]  Medical Appointment  81 (82)  60.46  56.1 58.28  33.26  [5, 25, 1, 2]  60.62  56.03  58.33  67.85  [7, 88, 1, 6]  60.49  55.83  58.16  58.93  [6, 31, 3, 35]  256 (261)  60.48  56.12  58.3 106.02  [5, 20, 1,  8] 60.6 55.99  58.3 220.25  [7, 66, 1, 13]  60.64  56.07  58.36  44.68  [7, 96, 2, 9]  625 (626)  60.48  56.12  58.3 305.25  [5, 15, 1, 6]  60.63  56.06  58.34  523.52  [7, 94, 2, 10]  60.67  56.09  58.38  212.39  [7, 11, 2, 9]    
 40 Table 4.13 Results of hyperparameter tuning experiments with the least amount of time consumed on accuracy plus f1-score as an objective value  Solutions  Grid Search  Random Search  Artificial Bee Colony  Accuracy  F1-
score  Acc + 
F1 
(Obj.)  Execution 
Time (s ) Best 
Hyperparameter  Accuracy  F1-
score  Acc + 
F1 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Accuracy  F1-
score  Acc + 
F1 
(Obj.)  Execution 
Time (s)  Best 
Hyperparameter  Credit Card  81 (82)  81.97  68.09  75.03  43.5 [195, 25, 1, 2]  82.13  67.99  75.06  69.89  [259, 67, 7, 32]  82.08  67.9 74.99  45.23  [220, 100, 4, 38]  256 (261)  81.89  68.03  74.96  126.68  [195, 35, 1, 2]  82.2 68.11  75.15  246.3  [302, 80, 5, 33]  82.27  68.03  75.15  184.96  [177, 11, 3, 18]  625 (626)  81.97  68.09  75.03  311.56  [195, 25, 1, 2]  82.21  68.11  75.16 604 [349, 52, 3, 30]  82.2 68.2 75.2 290.66  [283, 26, 1, 34]  Bank Marketing  81 (82)  89.86  66.74  78.3 48.37  [100, 25, 1, 20]  89.82  67.44  78.63  80.22  [19, 24, 1, 25]  89.97  66.79  78.38  23.79  [67, 83, 7, 24]  256 (261)  89.97  67.35  78.66  143.36  [65, 20, 1, 14] 89.91  67.72  78.82  260.27  [37, 97, 2, 36]  90 67.17  78.58  55.31  [23, 26, 6, 4]  625 (626)  89.94  67.24  78.59  355.63  [95, 35, 1, 10]  90.03  67.75  78.89  743.61  [82, 32, 1, 35]  90.03  68.15  79.09  118.48  [23, 93, 3, 34]  Cardiovascular Disease  81 (82)  73.94  73.88 73.91  51.47  [195, 25, 10, 2]  74.03  73.98  74 90.18  [186, 61, 15, 9]  74.02  73.97  74 36.21  [116, 42, 18, 23]  256 (261)  73.93  73.87  73.9 166.48  [195, 20, 10, 2]  74.09  74.03  74.06  307.41  [188, 21, 17, 29]  74.08  74.03  74.06  151.65  [77, 65, 16, 33]  625 (626 ) 73.94  73.89  73.92  415.02  [95, 15, 4, 20]  74.09  74.04  74.06  793.92  [162, 87, 17, 32]  74.06  74.01  74.04  194.66  [76, 84, 17, 23]  Medical Appointment  81 (82)  60.46  56.1 58.28  33.26  [5, 25, 1, 2]  60.36  55.96  58.16  56.82  [5, 20, 11, 11]  60.38  55.74  58.06  15.75 [7, 19, 14, 29]  256 (261)  60.48  56.12  58.3 106.02  [5, 20, 1, 8]  60.64  55.57  58.11  197.52  [20, 12, 2, 13]  60.64  56.07  58.36  44.68  [7, 96, 2, 9]  625 (626)  60.48  56.12  58.3 305.25  [5, 15, 1, 6]  60.58  55.98  58.28  504.93  [7, 54, 2, 25]  60.58  55.97  58.28  100.71 [7, 63, 1, 24]   41 
 Table 4.14 Accuracy plus F1-score statistics from automated hyperparameter tuning experiments   Solutions  Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Credit Card  81 (82)  74.88  75.18  75.04  75.02  75.18  75.09  74.95  72.24  75.07  256 (261)  74.88  75.22  75.06  75.04  75.2 75.12  75 75.27  75.13  625 (626)  74.9 75.23  75.09  75.1 75.23  75.15  75.03  75.24  75.15  Bank Marketing  81 (82)  78.29  79.07  78.53  78.33 78.98  78.67  77.9 79.04  78.43  256 (261)  78.32  79.07  78.59  78.51  79.03  78.75  78.18  79.04  78.65  625 (626)  78.29  79.07  78.62  78.7 79.07  78.88  78.24  79.09  78.76  Cardiovascular Disease  81 (82)  73.9 74.07  74.00  74 74.07  74.03  73.98  74.08  74.04  256 (261)  73.9 74.1 74.01  74.03  74.08  74.06  74.03  74.1 74.07  625 (626)  73.91  74.1 74.02  74.05  74.08  74.07  74.02  74.1 74.07  Medical Appointment  81 (82)  57.98  58.28  58.07  58.01  58.33  58.11  57.98  58.16  58.03  256 (261)  58 58.3 58.07  58.04  58.3 58.15  57.99  58.36  58.12  625 (626)  57.99  58.3 58.08  58.1 58.34  58.21  58 58.38  58.20   Table 4.15 Execution time statistics from automated hyperparameter tuning experiments on accuracy plus f-score as an objective value  Solutions  Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Credit Card  81 (82)  43.5 101 71.21  69.89  86.83  78.25  45.23  97.9 70.81  256 (261)  126.68  319.68  222.81  246.3  272.96  258.64  184.96  319.7  249.01  625 (626)  311.56  775.72  556.91  604 653.41  628.06  290.66  781.5  559.26  Bank Marketing  81 (82)  48.37  113.82  80.15  80.22  100.44  91.01  23.79  157.6  69.88  256 (261)  143.36  359.78  250.38  260.27  318.14  291.43  55.31  432.8  187.22  625 (626)  355.63  882.2  629.4  743.61  795.99  766.06  118.48  750.1  376.3  42 
 Table 4.15 Execution time statistics from automated hyperparameter tuning experiments on accuracy plus f1-score as an objective value (Cont¶d)  Solutions  Grid Search  Random Search  Artificial Bee Colony  Min Max  Mean  Min Max  Mean  Min Max  Mean  Cardiovascular Disease  81 (82)  51.47  132.97  92.97  90.18  110.43  99.75  36.21  139.59  75.20  256 (261)  166.48  417.73  290.70  307.41  336.09  321.96  151.65  429.91  252.94  625 (626)  415.02  1041.2  729.33  793.92  851.89  823.80  194.66  935.46  534.23  Medical Appointme nt 81 (82)  33.26  89.54  61.45  56.82  74.06  66.14  15.75  109.61  43.60  256 (261)  106.02  271.69  189.84  197.52  227.1  215.53  44.68  286.93  132.75  625 (626)  305.25  656.39  466.73  504.93  546.21  524.47  100.71  374.58  246.82   Table 4.15 reveals statistics on execution time. Again, ABC achieves the shortest minimum execution time in all cases except two, where grid search outperforms ABC. ABC achieves the lowest mean execution time in three datasets, which is similar to when F1-score is used as an objective function. Furthermore, it is interesting to observe that in the credit card dataset grid search can achieve the shortest mean execution time in more cases than ABC. However, in most cases, ABC saves more time than grid search and random search.  In summary, when the value of accuracy plus F1-score is used as an objective function, ABC is better suited to maximize objective value, while random search is better suited to maximize mean objective value. In terms of execution time, ABC still outdoes the other two algorithms.  According to all the experimental results presented in this section, when only the maximum objective value is considered, ABC is better suited for using accuracy and the value of accuracy plus F1-score as objective functions, whereas grid search is better suited for using f1-score as an objective function. Random search is the best algorithm if only the mean objective value is assessed. However, the results of the best objective values and highest mean are not significantly different. Sometimes the differences in best objective values and highest mean between the three techniques are less than 0.03. One-way ANOVA 43 
 was used to ascertain whether the differences in automated hyperparameter tuning techniques are statistically significant. In the following section, the ANOVA results are discussed.  4.3 Discussion Table 4.16 displays the results of a one-way ANOVA between the objective values and the three different sets of number of solutions in each dataset.  Table 4.16 One-way ANOVA between the objective values and the three different sets of number of solutions  Dataset  Accuracy (Obj.)  F1-score (Obj.)  Accuracy plus F1 -score 
(Obj.)  Grid 
Search  Random 
Search  ABC  Grid 
Search  Random 
Search  ABC  Grid 
Search  Random 
Search  ABC  Credit Card  - Sig Sig - Sig Sig Sig Sig Sig Bank 
marketing  Sig Sig Sig - Sig Sig - Sig Sig Cardiovascular 
disease  - Sig Sig - Sig Sig - Sig Sig Medical 
appointment  - Sig Sig - Sig Sig - Sig Sig  The ANOVA results in Table 4.16 show that the results of random search and ABC are significant in all objectives and datasets, implying that the mean results of each set of number of solutions from random search and ABC are not all equal. Grid search is the only algorithm whose results are insignificant in almost all datasets. The results of the hyperparameter tuning are then thoroughly examined.   The following box plots investigate why grid search is the only algorithm that produces insignificant results. The box plots in Figures 4.1, 4.2, and 4.3 show that the ranges of results from grid search, particularly the mean values from different numbers of solutions, are not as varied as those from random search and ABC. Moreover, a closer examination of Figures 4.4, 4.5, and 4.6 reveals that the results of grid search in large number of solutions are lower than those of random search and ABC. These plots thus clearly indicate that 44 
 grid search is not significant because increasing the number of solutions does not improve the overall mean.  
  Figure 4.1 Box plot of accuracy values from the credit card dataset  
  Figure 4.2 Box plot of accuracy plus f1-score values from the cardiovascular disease dataset    
45 
   Figure 4.3 Box plot of accuracy values from the medical appointment dataset  
  Figure 4.4 Bo[ plot of accXrac\ YalXes from the credit card dataset¶s 625(626) solXtions  
46 
   Figure 4.5 Box plot of accuracy plus f1-score values from the cardiovascular disease dataset¶s 625(626) solXtions  
  Figure 4.6 Box plot of accuracy values from the medical appointment dataset’s 625(626) solutions 
47 
 A one-way ANOVA was also performed between execution time and the three different sets of solution numbers. The ANOVA results show statistically significant differences in all cases because the execution time from each set of number of solutions is completely different, as shown in Figures 4.7 and 4.8. On the other hand, the execution time shown in these box plots can be analyzed.   Figures 4.7 and 4.8 respectively depict the execution time from the credit card and bank marketing datasets when accuracy is used as an objective function. As we discussed in Section 4.2, the execution time of ABC for the credit card dataset is indistinguishable to that of gird search. However, in other datasets, the execution time of ABC outperforms the other two algorithms. As shown in Figure 4.8, ABC uses slightly less execution time than grid search and random search for the 81(82) and 256(261) solutions, but for the 625(626) solutions, ABC saves significantly more execution time than the two algorithms.  
  Figure 4.7 Box plot of execution time from the credit card dataset when accuracy is used as an objective function  
48 
   Figure 4.8 Box plot of execution time from the bank marketing dataset when accuracy is used as an objective function  Figure 4.9 compares the execution time of the three algorithms for the 625(626) bank marketing solutions when accuracy is used as an objective function. As shown in the figure, the box plot of ABC is significantly lower than grid search and random search. In other words, the mean execution time used in the 626 ABC solutions is significantly shorter than those used in grid search and random search for the 625 solutions.  
49 
   Figure 4.9 Bo[ plot of e[ecXtion time from the bank marketing dataset¶s 625(626) solutions when accuracy is used as an objective function       
 
 CHAPTER 5 CONCLUSION Machine learning is widely used for data analytics. Data science and other disciplines use this method to build their model for a multitude of proposes such as business and recommendation. However, the tuning of the hyperparameters used to generate these models is a significant problem as they have to use plenty of time to train and validate the model until the model has the best performance. Grid search and random search are 2 traditional ways that perform automated hypermeter tuning to help get the best performance with less time. Thus, we are interested in using the artificial bee colony optimization algorithm to automatically tune hyperparameters and compare the results as well as the performance with those using the conventional methods.  After reviewing the related works, we found that the automated hyperparameter tuning has been done by different techniques. Most of them used support vector machines as a learning model. Some work applied optimization algorithms such as ant colony and population-based algorithms to the automated hyperparameter tuning process. However, the performance of each method is measured in different ways depending on their objective. Different advantages and disadvantages of each optimization algorithm in terms of accuracy, execution time, and error have been presented.   The results of our automated hyperparameter tuning experiments suggest that grid search is worse than random search and ABC in terms of objective value. Because the mean of objective value from random search and ABC increases as the number of solutions increases, but grid search does not due to insignificant results from fixed values. This suggests that the probability of obtaining good objective values from random search and ABC is higher than that of grid search.  In terms of execution time, ABC takes less time than grid search and random search in all datasets except the credit card dataset. Furthermore, when there are a larger number of solutions, ABC can save significantly more time than grid search and random search. Thus, using an ABC algorithm in automated hyperparameter tuning can save us more time while the objective value does not differ significantly from grid search and random search.   
 