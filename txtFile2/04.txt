Thai Character-Word LSTM Language Models with  Dropout and Batch Normalization Thesis Credits
Due to the emerging of Long Short-Term Memory neuron network (LSTM) which is a variation of deep neuron network, it is proven to be essential to the improvement of Natural Language Processing especially Language Modelling. Many researches applied LSTM to model many well-defined languages and gain performance in term of accuracy. However, this new approach is rarely applied to Thai language. Unfortunately, the characteristic of Thai language is significantly different than other well-defined languages, particularly English or Latin-based languages. This thesis applied LSTM in Language Modelling to predict the next word in the sequence. Seven LSTM models have been designed and compared the results with word-level LSTM model. The experiment showed that character-word LSTM can improve the performance of Natural Language Modelling (NLM) on Thai dataset. Especially when using character-word LSTM with dropout value of 0.75 and batch normalization, the perplexity is lower than baseline word-level LSTM up to 21.10%.  

Deep Learning/ Long Short Term Memory/ Neural Language Model/ Natural                  Language Processing        
CHAPTER 1 INTRODUCTION   Typing text on mobile devices is slow and inconvenient compare to typing with keyboard.  Fortunately, word prediction has been developed to assist inputting on mobile devices. Word prediction is the system of predicting words which were likely to be the next word by analyzing the text already occur in the input. The system usually predicts the next word right after the previous word is completed. For example, typing ‘I would like’ may cause the system to suggest the word like ‘to’.  The word prediction system plays an important role in communicating with text. Good word prediction can increase typing speed, save keystrokes, and reduce errors [1, 2] since using word prediction makes a single key press resulting in full word. The system can also help users that lack ability to type whom each keystroke is an effort [3]. Moreover, word prediction helps devices with a limit number of input keys to type messages efficiently [4].  A language modeling (LM) is the development of probabilistic models over sequences of words that are able to predict the next word in the given sequence. LM is useful in many natural language processing (NLP) applications, especially the applications that generate text as an output such as word prediction [5]. There are many approaches to create language models such as N-gram, feed-forward neural network, recurrent neural network, long-short term memory network, and so on which will be discussed in the following paragraphs.  Traditionally, N-gram is the most popular method of LM [6]. Unfortunately, N-gram has problems with unseen words because it relies on the exact pattern [7] and also has a data sparsity problem [8]. These reasons lead to the idea of applying deep learning [9] and neural networks (NNs) to solve the problem of LM, which is called neural language modeling (NLM). The purposes of neural language models are to automatically learn such syntactic and semantic features and to overcome the curse of dimensionality by generating better generalizations.    2 Initially, feed-forward neural network (FNN) models were used in the introduction of NLM approach. However, using FNN models are restricted because they only use a fixed context size that has to be determined in advance [10]. More recently, recurrent neural networks (RNN) [11], and networks with a long-term memory like the long-short term memory networks (LSTM), allow the models to learn the relevant context over much longer input sequences than the simpler FNN [12].  Undoubtedly, deep learning has provided state-of-the-art approaches that provide the best results in machine learning which is no exception with NLP [9]. The majority of development in NLP has been based on well studied languages such as English [13], mainly due to the availability of large standardized corpora. However, the Thai language is rarely seen in this field, especially with deep learning, the state-of-art method. There are many differences between Thai and English language [14, 15] that may produce different results [13]. There were many researches on Thai morphological analysis [16] which show the difference of Thai language from others at the points of phonology, morphology, and syntax levels. Thai language has rich morphology [17] in which this language allows very detailed elaboration of how events transpire and progress. Lately, there is a research worked on rich morphology languages such as Arabic, Czech, French, German, Spanish, and Russian [13] using the combination of character-based and word-based LM and outperform the baseline method that is word-level LSTM which is discussed in the following paragraphs.  Normally, NLM is mostly word-level modeling up to now. In recent years, people have started to focus on subword-level modeling and character-level modeling. Some research shows that subword-level and character-level information plays an important role in improving the representations for infrequent words and even out-of-vocabulary (OOV) words [18]. In addition, they help recognizing the information embedded in morphemes such as prefix, root, and suffix. The experiments demonstrate that the models outperform word-level LSTM baselines with fewer parameters on languages with rich morphology [13, 19, 20].  However, training deep neural network with large number of parameters on a small dataset can lead to an overfiting problem. To regularize the model, dropout and batch normalization are also applied and tested. Dropout is a regularization technique that   3 deactivates few random neurons in the neural network in order to mitigate the overfitting problem in training neural networks [21]. Batch normalization, on the other hand, regularizes the model by reducing internal covariate shift and improving accuracy of the model [22]. Dropout and batch normalization have never been implemented together in LSTM, especially in Thai. Therefore, this research focuses on designing and implementing Thai LM using the combination of character-level and word-level LSTM with dropout and batch normalization, in order to deal with rich morphology and OOV words of Thai language. The research also compares the results with the other seven methods, namely, word-level LSTM, word-level LSTM with dropout, word-level LSTM with batch normalization, word-level LSTM with both dropout and batch normalization, character-word LSTM, character-word LSTM with dropout, character-word level LSTM with batch normalization, and character-word LSTM with both dropout and batch normalization.  1.1 Statement of the Problem Thai NLP researches in the past were relatively behind research in English language. If we do not consider the problems of lacking the support to do the research or lacking the innovation to bring the research to the next level, there are two important issues to consider. First, out of the world's languages, Thai is one of the richest systems of grammatical aspect. This makes Thai to be a very ambiguous language [17]. Second, when talking about Thai NLP, there are some advance researches on text segmentation [23], speech recognition [24], text-to-speech synthesis [25], and sentiment analysis [26]. Word prediction on the other hand, even there are some researches about this topic, the method used is still very old [27].  1.1.1 Difficulty of Thai Language in Natural Language Processing Unlike English, Thai is an analytic language that has simple but complex grammar, called simplex, because it uses adjectives and adverbs to describe grammatical features instead of inflection [28]. Since the simple Thai word sequence only has subject-verb-object, therefore there usually are complex words arise from the small synthesis of words and put into sentences. Many compound words in this language use prefixes or suffixes where the purpose is only to be euphony. Moreover, the vowel graphemes can be combined in various ways to produce compound vowels [17]. Because of this simplex, the Thai   4 language is very ambiguous in almost every level, such as morphology, syntax, semantics, discourse, and pragmatics [28]. In Thai, there is no clear definition of what constitutes a word as it depends on the context. These problems set a challenging approach for deep learning [9].  1.1.2 Thai Word Prediction There are many reasons that prevent people from writing efficiently. As mentioned in section 1.1.1, Thai language is immensely ambiguous. This language is not easy for everyone because of the simplex grammar. Some vocabularies may be difficult to spell or take too much time for typing. Some words are homographs means that they look the same but have different meaning [29]. Typing incorrectly, even only a single letter, may change the meaning of the content. Another reason is that not everyone can easily type the word because some people have a physical or cognitive disability makes them lack ability to type, so each keystroke is an effort [3, 27]. Word prediction comes to solve the problems mentioned above [1, 3, 30]. In addition, there are some researches showed that word prediction can also be used to help in machine translation field [31].  When it comes to word prediction research, there are a lot of researches about this topic, but most of the data used in the experiments is in English [30, 32, 33, 34]. Some research showed that creating sequences of words using the same method may give different results when the test data is in different language [13]. This linguistic factor may affect the choice of method used in the experiment, so it is sensible to consider these differences in the design of experiment.  1.2 Research Objectives The main objective of this research is to propose the development of the system of Thai LM with the aforementioned models especially for morphologically rich languages using deep learning-based methods [9]. Moreover, to know how this model performs, which has never been used in Thai language before, they will be compared with the baseline methods in terms of the improvement of predictability for Thai words. The results of the experiment can be used to develop the system that can act as an assistant tool in the future for people who want to type Thai messages faster without spelling mistakes or who have physical or cognitive disability which make it difficult to type Thai messages.   5 1.3 Scope of Study This thesis focuses on researching and implementing LM systems for Thai language. The corpus data used in this research is BEST2010 corpus from National Electronics and Computer Technology Center (NECTEC) [35]. The system will be developed using deep learning-based method called LSTM [36] with the combination of character-level and word-level [13, 18, 19, 20]. To regularize the model, dropout and batch normalization are also used with this model. After implementing, the performance of the system will be evaluated, and the result data will be analyzed and compared to another seven models, including the baseline LSTM method. Finally, this thesis will have a discussion about the benefits and limitations of this system using the given method and gives the open questions for the future research.  1.4 Expected Benefit from the Research The system is developed to represent the implementation of Thai LM and deep learning knowledge [9]. One of the expected benefits of the research is to prove that the proposed model help solving ambiguity problems with Thai language [28] using the combination of character-level and word-level LSTM with regularization methods. Especially, knowing how they perform over baseline LSTM models, which often used in NLP for many other languages [18, 33, 37]. The results of the experiment show that which model is the most suitable to be applied in Thai language.                 6 CHAPTER 2 LITERATURE REVIEW AND RELATED    THEORY   Word prediction system usually requires the knowledge of the information embedded in collected data, how to preprocess the data, the model for training, and how to evaluate the system. Corpus is a collection of data to be used in training and testing the system. It is important to know what information is needed, how much data to be collected, and in which stage of the system the data is required. After a corpus is created, the data needs to be preprocessed. The data preprocessing is the process by which the corpus is prepared, e.g. word segmentation, stemming, and lemmatizing, before it is processed. In the processing phase, several models are designed and trained. After the processing, the evaluation takes place to measure and compare the performance of various models.  2.1 Data Collection Most of the data collections used in word prediction are popularized from large sources   of texts that contain millions of words to obtain a large number of word spelling to cover every word in a language and make the prediction system more accurate. For example, DeepTingle [32] is a text prediction system trained on the collection of novels written by the author Chuck Tingle which has over 3 million characters. Ghosh et al. [33] used a large dataset of 4.7 million English documents in Wikipedia which have the vocabulary around 130 thousand words in total.  Apart from word spelling, there is also a collection of users typing history and finding a topic from the previous context to make the predictions narrower and more accurate. Ghosh et al. [33] took the typing history to collect the topics from the context using Hierarchical Topic Model (HTM) based on the Google Rephil Large-Scale Clustering Tool that has about 50 topics, then choose the highest probability topic and process into the system. They have shown that using contextual features in the system can be beneficial for various NLP tasks such as word prediction, sentence selection, topic prediction, and so on.     7 2.2 Data Preprocessing Data preparation helps preparing the data to make the computer use the data more effectively. Especially in the works related to NLP, the preparation of information is tremendously crucial. The reason is computers cannot understand human language at all if they are not well-represented and properly managed. On the other hand, human language is used to describe things which is most of the time can be ambiguous and the meaning can change according to the surrounding contexts.  After getting data, most of the research divided data into three parts; training, testing, and validation. For example, Low [34] divided data into 20% for training, 70% for testing, and 10% for validation while Ghosh et al. [33] divided data into three parts of 80%, 10%, and 10%, for training, testing, and validating, respectively. At this stage, some data or words that are not needed in the system, such as number or punctuation, may be discarded.  In addition, the corpus is used to create the vocabulary database of the language. If a large corpus were collected, the vocabulary database will be very large. Therefore, some researches discarded the words that occur less than a specified threshold, e.g., Ghosh et al. [33] filtered out words that occurred less than 200 times and created vocabularies of 129k words. Low [34] also chose only the most commonly appearing words for the experiment and replace each character by its ordinal presentation from 0-255. However, there are many steps further to do in preprocessing process. Suppose the following sentence in figure 2.1 is required to be processed by a computer.   Figure 2.1 The example sentence in Thai  If this sentence is presented as an input, the computer may not understand because it does not process the data the same way as humans do, thus, preparation of the information is required. In preprocessing, it is obvious that from the sentence in figure 2.1, there is a character that is not meaningful, which is “!”, so, the data is needed to be cleaned before it can be used. Typically, in data cleansing process, regular expression is used to find patterns in the data which needed to be processed [38]. เขาพูดขึ(นว่า “เฮ้ย! ฉันลืมหยิบโทรศัพท์มา”    8 After cleansing the data, the next step is to separate texts into the smallest unit, which is a process called a word segmentation. A sentence is segmented into words because of an assumption that each word has different functions and meanings. In English, each word in a sentence is separated by a space, thus, the division of words is a simple task. But in other languages such as Chinese, Japanese, or Thai, word segmentation is a very daunting issue. For these languages, word segmentation is one of the first tasks in building applications of NLP. Unlike human, computers cannot understand the sentence without word boundaries [23]. Currently, there are many ways to handle word segmentation in Thai, either by using a dictionary-based with the longest word matching [39] or machine learning-based [40], such as deep learning. Figure 2.2 depicts a result of applying a word segmentation to the sentence in figure 2.1.   Figure 2.2 Word segmentation of the example sentence  The next step is tokenization process [41] that creates token which is the smallest unit of words and used in creating a collection of vocabularies as illustrated in figure 2.3.   Figure 2.3 Tokenization of the example sentence  In the example, these tokens are sufficient to be processed. However, in many cases, the word that cannot be fully expressed, e.g., not in dictionary, has to be defined as shown in figure 2.4 and 2.5, which are the original sentence and the result of word segmentation, respectively.   Figure 2.4 The original sentence in Thai  
เฮ้ย|ฉัน|ลืม|หยิบ|โทรศัพท์|มา 
เฮ้ย, ฉัน, ลืม, หยิบ, โทรศัพท์, มา 
เย็นนี(เราจะไปดูพระปรางสามยอดกัน   9  Figure 2.5 Word segmentation from Thai sentence  The result in figure 2.5 shows that พระ, ปราง, สาม, ยอด are separated, but พระปรางสามยอด is a name of a specific place which should be expressed as one word. There are several ways to deal with this problem such as entity recognition, normalization, and stemming and lemmatization.  First, named-entity recognition (NER) [42] is a process that identifies specific words such as the name of the person, the name of the organization, the name of the place, time, and number. NER may be either grammar-based, which is assigned by the experts, or machine learning-based which is automatically assigned. Both methods have different advantages and disadvantages. As shown in figure 2.5, NER can be used to solve “พระปรางสามยอด” problem.  Second, normalization [43] is an approach to transform a word into a representative synonym in the dictionary. The example is abbreviated words such as โรงพยาบาล, ร.พ., and รพ. is represented by โรงพยาบาล or synonyms meaning such as รถ and รถยนต์ is represented by รถ.  Stemming [44] and lemmatization [45] are the approaches to transform the inflectional forms of each word into a common base or root. However, these two methods are not exactly the same. The main difference is the way they work and therefore the result that each of them returns. Stemming algorithms work by cutting off the end or the beginning of the word and taking into a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be some occasions but not always successful, so this approach presents some limitation. The example of stemming is shown in figure 2.6.  
เย็น|นี(|เรา|จะ|ไป|ดู|พระ|ปราง|สาม|ยอด|กัน   10  Figure 2.6 The example of stemming  Lemmatization, on the other hand, considers the morphological analysis of words. To do so, it is necessary to have detailed dictionaries that the algorithm can look through to link back to its lemma. The example of lemmatization is shown in figure 2.7 and 2.8.   Figure 2.7 The example of lemmatization in English   Figure 2.8 The example of lemmatization in Thai  In managing the preprocessing process, it depends on the nature of the language, the collected data, and the purpose of the task.  2.3 Trained Model and Related Theory This section discusses some background knowledge on language modeling (LM), neural language modeling (NLM), convolutional neural network (CNN), recurrent neural network (RNN), long short-term memory network (LSTM), and the combining benefit of character-level with word-level NLM.  2.3.1 Language Modeling A language modeling (LM) [5] or statistical language modeling is the development of probabilistic models over sequences of words that are able to predict the next word in a given sequence. LM is useful in many NLP applications, especially ones that generate text as an output. LM is commonly used in various tasks that generate text such as word prediction, spelling correction, optical character recognition, speed recognition, machine 
run, runs, running → run 
is, am, are → be 
รับประทาน ฉัน เสวย → กิน   11 translation, part-of-speech tagging, parsing, handwriting recognition, image captioning, text summarization, and other applications [46, 47].  Traditionally, one of the most popular methods of LM is N-gram which is the probability of a sequence of N given words. N-gram makes the assumption that the probability of word only depends on the previous N words. Unfortunately, using N-gram caused a problem because new combinations of N words that were not seen in the training set are likely to occur, so the prediction result may not be accurate. Some of the smoothing methods such as various back-off models [6, 46] have been developed to help with unseen words.   Another point to consider is N-gram models rely on an exact pattern. For example, Using N-gram model that already learn “my red pen is in another room” but may not recognize a sequence like “your bag is on the table” which syntactically and semantically similar [7].  Another problem is that the dependency that is beyond the context of N words is ignored. When estimating the parameters of the N-gram model, only the context of N-1 words is considered, which is in conflict with the fact that the longer context is, the greater accuracy is. This means increasing N in this model also increased the demand for sample data and parameters to collect probabilities, causing a data sparsity problem.  These reasons lead to the idea of applying deep learning and NN [48] to the problem of LM. Additionally, applying those approaches also introduce new features such as syntactic and semantic learning along with better generalizations.  2.3.2 Neural Language Modeling Neural Language Modeling (NLM) is also known as continuous-space LM. For many years, NLM has outperformed the classical language model which is N-gram. NLM make use of NN in language modeling and solve the problem of data sparsity in classical language modeling by representing words as vectors which is called word embedding and by using them as inputs to an NLM. There are two well-known NLM approached that were developed to solve the limitation of N-gram; FNN-based LM for solving the problems of data sparsity and RNN-based LM for dealing with the problem of limited   12 context. Moreover, NLM model can also capture the contextual information at the sentence-level, corpus-level, and subword-level [11].  As mentioned in section 2.3.1, LM are trained on a very large corpus. When the number of unique words increases, the number of possible sequences of words also increases exponentially with the size of the vocabulary, causing a data sparsity problem. NLM avoids this problem by representing words using word embedding [49]. Word embedding is adapted for a real-valued vector to represent each word in a projected vector space. This process causes words with similar meaning to have a similar representation which is close in the vector space [13]. Word embedding solves the data sparsity issue and uses representation in vector space as the input for NN. This process of NLM is something that classical statistical language models cannot easily achieve [50].  2.3.3 Deep Learning Deep learning [9] is a normal neural network such as FNN with a considerably large number of hidden layers. With multiple hidden layers, deep learning attempts to learn multiple levels of representation and produces an output from raw inputs like words.  In the last few years, deep learning-based methods have been producing superior results on various NLP tasks. Collobert et al. [51] demonstrated that a simple deep learning framework outperforms most state-of-the-art approaches in several NLP tasks such as named-entity recognition (NER), semantic role labeling, and part of speech (POS) tagging. Since then, numerous complex deep learning-based algorithms have been proposed to solve more difficult NLP tasks. A list of deep learning related models and methods applied to natural language tasks includes convolutional neural network, RNN, LSTM, and so on.  2.3.3.1 Convolutional Neural Network Convolutional neural network (CNN) is one of the most popular deep learning methods that performs well in various NLP tasks, including NER, semantic role labeling (SRL), POS tagging [51], speed recognition [52], question and answering [53], machine translation [54], and LM [55]. Traditional FNN connects each input neuron to all output neurons in the next layer, which is called a fully connected layer, while CNN uses convolution function of the input neuron to create output to the next neuron. This means   13 each region of the input is connected to a neuron in the output. Collobert and Weston [55] proposed CNN for sentence modeling in many NLP tasks such as POS tags, named-entity tags, semantically similar words, and a LM model. Meanwhile, Kim [56] showed comparative results for a variety of sentence classification tasks, including sentiment, subjectivity, and type of questions.  CNN models are suitable for certain types of NLP tasks that require semantic matching beyond classification [57] and are used for projecting a specific feature [58]. The model attempted to extract rich contextual structures in a text by considering a temporal context window in a word sequence. the model captured the contextual features by the convolution layers and max-pooling layers which are then combined to form the overall sentence vector.  During the training phase, CNN automatically learns the values of its filters based on the tasks of selected layers. In the NLP tasks, most inputs are represented as a matrix. Each row of the matrix corresponds to one token (typically a word or a character), which is represented as a vector. Typically, these vectors are the result of applying word embedding method which generates low-dimensional representations, but they could also be one-hot vectors that are indices of the words in a vocabulary. The illustration in figure 2.9 represented CNN model for an NLP task.  In CNN, there are about hundreds of convolutional filters, called kernels, of different widths sliding over the entire word embedding matrix. Each kernel extracts a specific pattern of N-gram. A a series of convolution layers is usually followed by a max-pooling layer which downsamples the input typically by applying a max operation on each filter. Max pooling layer provides a fixed-length output which is then used as an input to a classification model. Max pooling maps the input to a fixed dimension outputs and also reduces the output’s dimensionality while searching for specific features across the whole sentence. This is done in many layers where each filter is able to extract particular features from anywhere in the sentence. Collobert et al. [51], Kalchbrenner et al. [59], and Kim [56] showed that CNN models have the ability to extract salient features from the input sentence to create an informative latent semantic representation of the sentence.    14 
 Figure 2.9 An example of a sentence and CNN model with two channels         (Source: Kim [56])   In summary, CNN networks are effective in mining semantic clues in contextual information. However, the proximity of a word in the sentence is important to most NLP tasks. LM tasks also require preservation of sequential information and long-term dependency but CNN do not have the ability to model long-distance contextual information and preserve sequential order in their representations [54, 59]. According to aforementioned issues, CNN models might not be suitable for LM tasks or word prediction. Other neuron networks such as an RNN model outperform a CNN model on various LM tasks.  2.3.3.2 Recurrent Neural Network It is commonly known that FNN based LM uses a fixed-length context. However, RNN does not use the limited size of context. It has the ability to model variable length of text, including very long sentences, paragraphs and even documents [60]. RNN allow neutrons with inputs from recurrent connections to represent short-term memory instead of considering just several preceding words. RNN perform very well and have obtained strong results on a variety of sequence modeling tasks, which resulted to be a strong motivation for researchers to use RNN over CNN in these areas [37].   Recently, RNN becomes increasingly more popular in NLP applications due to the necessity of processing sequential information. RNN model training perform the same steps over each instance of the sequences along with the current state of the model, thus, the output is depending on both previous and current computations [61]. Generally, a fixed-size vector is produced to represent a sequence by feeding tokens one by one to a 
  15 recurrent unit. RNN has a memory to memorize previous computations and uses this information in the current process. The hidden state of the RNN model is considered to be its most crucial element because it is the network’s memory element that accumulates information from previous time steps. This approach is naturally suitable for many NLP tasks such as LM, machine translation, speech recognition, image captioning, POS tagging, text categorization, sentiment analysis, and subjectivity detection [11, 37, 62]. In figure 2.10 is an illustration of a standard RNN diagram which shows a representation of an RNN neuron [63].  
 Figure 2.10 Standard RNN Diagram                  (Source: Olah [63])  RNN has had a huge role in the field of word-level classification. RNN also shown considerable improvement in LM over traditional methods. Graves [64] revealed the effectiveness of RNN model in processing complex sequences with long-range context structures. He also proposed deep RNN models where multiple layers of hidden states were used to enhance the performance. This work established the usage of RNN on NLP tasks. Later, Sundermeyer et al. [10] compared the gain obtained by replacing an FNN with an RNN when conducting the prediction of a word on the list of words ahead. In their work, they proposed a typical hierarchy NN architectures where FNN model gave considerable improvement over traditional count-based language models, which in turn were superseded by RNN models and later by LSTM models.  Unlike CNN, RNN has flexible computational steps that provide better modeling capability and create the possibility to capture unbounded context. This ability to handle input of arbitrary length becomes one of the selling points of RNN [65]. However, in practice, RNN is still struggling to cope with long term dependency in the sequence due 
  16 to the vanishing gradient problem [66], which makes it really hard to learn and tune the parameters of the earlier layers in the network. This problem is solved using long-short term memory network (LSTM) and gated recurrent units (GRU) which are a type of RNN neurons with a more complex computational task where LSTM is the most used RNN variants in NLP applications.  2.3.3.3 Long Short-Term Memory Network Theoretically, RNN is able to handle long-term dependencies, however, in practice, Bengio et al. [67] found that it is difficult to train RNN model to capture long-term dependencies. On the other hand, a special kind of RNN like LSTM actually capable of learning long-term dependencies easier. LSTM and GRU are usually very similar in terms of performance. Moreover, for some specific tasks, GRU may even outperforming LSTM models despite the fact that they have fewer parameters to train. However, Jozefowicz et al. [47] showed that for the task of LM, LSTM works better than GRU and outperforms the classical methods when evaluating LM over large datasets of one million words corpus.  
 Figure 2.11 The repeating module in an LSTM             (Source: Olah [63])  LSTM, first introduced by Hochreiter and Schmidhuber [36], was outstandingly performed mitigating the problem of discovering the long-term dependency. LSTM automatically recognizes information for long periods of time and works extremely well on a wide variety of challenging problems. More importantly, LSTM can also seize features of words. An LSTM repeating module [63], which is depicted in figure 2.11, consists of four interacting layers; forget gate layer, input gate layer, the cell state, and output layer. 
  17 Same as RNN, LSTM has the form of chain of repeating modules. Instead of having a single neural network layer in the repeating modules like RNN, LSTM have four layers collaborating in a very exclusive way. Two horizontal lines running through the top and bottom of the diagram in figure 2.11 show that LSTM repeating module allows two inputs from previous module that goes into their special four layer. The information of the line at the top of the diagram is the accumulation of !! in which the long-term memory is represented. On the other hand, the line in the bottom part of the diagram represents the output of the !! and the current memory state.  
 Figure 2.12 The diagram of LSTM unit  Figure 2.12 shows the diagram of LSTM unit, which consists of four gates to control the cell state: forget, candidate, input, and output. Using these gates, the LSTM repeating unit is capable of eliminating or adding information to the cell state. Firstly, LSTM unit takes input !!, "!"#, and #!"# for producing #! and "! using equation (2.1) to equation (2.6).  $!=	'(!!×*$+"!"#×,$-                                    (2.1)  .̅!=	012ℎ(!!×*%+"!"#×,%)             (2.2)  6!=	'(!!×*&+"!"#×,&)          (2.3)  7!=	'(!!×*'+"!"#×,')           (2.4)  #!=	$!×#!"#+6!×.̅!       (2.5) 
  18 "!=	7!×012ℎ(#!)               (2.6)  The × operator is the element-wise multiplication, the + operator is the element-wise addition, the ' is the element-wise sigmoid, and the 012ℎ operators are hyperbolic tangent functions. , and * are weight vectors applied to !! and "!"#, respectively, to compute the output of forget gate ($!), candidate (.̅!), input gate (6!), and output gate (7!) which are calculated using equation (2.1) to equation (2.4).  Not all LSTM models are the same, in fact, almost every paper involving LSTM uses a slightly different version. Ghosh et al. [33] used contexture LSTM (CLSTM) as a trained model of word prediction. The difference between CLSTM and baseline LSTM method is that CLSTM includes the topics of a context with input data as shown in figure 2.13. The topic can be calculated either based on the previous words of the current sentence (SentSegTopic), all words of the previous sentence (PrevSegTopic), or current paragraph (ParaSegTopic), in which the best CLSTM model are trained with all of them.  
 Figure 2.13 Contextual LSTM model                  (Source: Ghosh et al. [33])  Low [34] proposed a character-level architecture with LSTM for word prediction modeling. The proposed model is to be used with devices that have limited amount of memory such as mobile device using a character-level model. Since the model works on character instead of word, there is no need for a large word embedding metric. The results suggest that character-level models are powerful and may find the use in applications where memory is limited.  
  19 2.3.3.4 Combination of Character-based and Word-based Language Models Neural language models are mostly word-level up to now with a few character-level approaches. At first, they are not working well with subword information such as prefix, root, and suffix. For example, they do not know that “thankful” and “thankfully” should have structurally related embedding in the vector space. Word-level models require a dictionary of predefined vocabularies and tend to ignore rare words and spelling errors, as the words that do not appear in the vocabulary are replaced with an OOV token. Although LSTM approach brings a high degree of freedom in learning expressions of words, however, information about morphemes such as prefixes, roots, and suffixes are lost when the word is converted into an index. These are the major limitations of word-level modeling.  Lately, researchers have started to focus on subword-level modeling, thus, a character-level modeling were proposed [13, 18, 19]. These models rely on character-level inputs and their outputs are used as inputs to a recurrent NLM. The experiments demonstrate that the subword-level model outperforms word-level LSTM baselines with fewer parameters on languages with rich morphology such as Arabic, Czech, French, German, Spanish, and Russian.  Verwimp et al. [18] presented a character-word LSTM model which reduces both perplexity and number of parameters of the model which is compared with the word-level model baseline. To come up with the idea of using both character and word as an input, the authors point to two problems. First, the parameters for infrequent words are typically less accurate because the network requires a lot of training examples to optimize the parameters. The second problem is the fact that the model does not give priority to OOV words. These are the reason why subword information is used to help in revealing structural similarities and dissimilarities. Hence, subword information can play an important role in improving the representations of infrequent words and even OOV words. In their character-word LSTM model, the word embedding representations are concatenated with each character and feed the results of character-word embedding to the LSTM model. By concatenating the embeddings, the individual characters are preserved and the order of the characters is implicitly modeled. This results in the decreasing in the number of parameters of the model by partially replacing word embedding by character embeddings which result in a much smaller vocabulary size and a much smaller   20 embedding matrix. They tested their model on both English and Dutch, since Dutch has a richer morphology than English. This model outperforms word-level LM with the same number of hidden units and the same number of parameters.   Xie et al. [19] combined word and character-level information such as a Gated LSTM and a CNN-based LSTM to generate Shakespeare prose. Word-level models are quite effective at capturing semantic meaning while character-level models potentially contain more information about morphemes and ability to capture rhyming and metering. To generate Shakespearean poetry, the authors showed that NN models that combined both word and character information significantly outperformed classical word-LSTM model and classical character-level LSTM model. The quality of these models is confirmed by both perplexity scores and feedbacks from human evaluation.   Kim et al. [13] employed a simple NN model that relies only on character-level inputs while predictions are still made at the word-level. The model used a CNN to process input of characters, then the output from CNN is presented as an input to a LSTM model. The authors experimented on seven languages - English, Arabic, Czech, French, German, Spanish, and Russian, which the results outperformed word-level LSTM baselines and morpheme-level LSTM baselines with fewer parameters.  Additionally, a gated word-character recurrent LM is presented by Miyamoto and Cho [20] to address the same issues which are the loss of morphemes information and word problems using word-level LM. Unlike the character-level NLM which only dependent on character-level inputs, this gated word-character RNN LM utilizes both word-level and character-level inputs. In particular, the model has a gate that determines a representative of each word from one of the two ways whether to derive the word into character-level or the word-level itself. This gate is trained to make this decision based on the input word. The major contribution of this model with this kind of threshold mechanism is that it effectively utilizes the character-level information for rare and OOV words and outperforms word-level language models on several English corpora.       21 2.3.4 Model Regularization Large neural networks that trained on relatively small datasets can overfit the training data, which results in the decline of performance [21]. Dropout and batch normalization are the regularization methods that can reduce this overfitting problem.  2.3.4.1 Dropout Dropout [21] works by training only a portion of weights in the network in each training iteration, that the portion is randomly generated. Considering a single layer linear unit in a network, the output of the layer is a linear weighted sum of the inputs.  For model training evaluation, a loss function in the linear layers is needed to be minimized. In equation (2.7) and (2.8) shows how to calculate the losses in a dropout network and a regular network, respectively.  8(=#)(0−∑;&<&=&*&"#))              (2.7)  8+=#)(0−∑<&,=&*&"#))              (2.8)  The ;&’s in equation (2.7) is the parameter that control the selection of training portion which is equal to 1 with probability p and 0 otherwise. Suppose <&, in equation (2.8). equal to >&<&, therefore,  8+=#)(0−∑>&<&=&*&"#))               (2.9)  To find a relationship between the gradient of dropout and regular networks, it is necessary to find the derivatives of equation (2.7) and (2.9), which are represented in (2.10) and (2.11), respectively. The next step is to find the expectation of the gradient of the dropout network as shown in equation (2.12).  -.!-/"=−0;&=&+<&;&)=&)+∑<0;&;0=&=0*01#,03&               (2.10)  -.#-/"=−0>&=&+<&>&)=&)+∑<0>&>0=&=0*01#,03&               (2.11)   22 8?-.!-/"@=-.#-/"+<&>&(1−>&)=&)      (2.12)   In equation (2.12), the expectation of the gradient with dropout is consistent to the gradient of loss of the regular network (8+) when <&, equal to >&<&. This means minimizing the dropout loss is equivalent to minimizing a regularized network as shown in equation (2.13).  84=#)(0−∑>&<&=&*&1#))+∑>&(1−>&)<&)=&)*&1#        (2.13)   Dropout is an effective regularization method when applying to FNN [21]. However, Bayer et al. [68] addressed that conventional dropout did not perform well with RNN due to the increasing level of noises from the recurring units. Zaremba et al. [69] proposed a solution by applying dropout to the subset of connections between RNN layers. Then, the authors used batch normalization to regularize the model and reduce the need for dropout.  2.3.4.2 Batch Normalization Batch normalization [22] not only reduces overfitting by adding some noises to outputs of the activation layers but it also normalizes the input of each layer to handle the internal covariate shift problem due to the change in network parameters during training. Batch normalization works by adjusting the means and variances of each layer's inputs across one mini-batch to avoid the shift of input distribution over time.  To calculate batch normalization, let B& is a value in mini-batch C=DB#,…,6E then the mean (F) and variance (')) of mini-batch are calculated using equation (2.14) and (2.15), respectively where G is the number of elements and F7 and '7) represent the mean and variance of each mini-batch, respectively. After that, the output BH& is the normalization of each B& that is calculated with equation (2.16).  F7=	#6∑B&6&1#         (2.14)  '7)=	#6∑(B&−F7))6&1#               (2.15)    23 BH&=8""9$:;$%<=          (2.16)  CI>,?(B&)=JBH&+K           (2.17)  After normalizing the input BH&, the result is passed through a linear function with parameters J and K to calculate CI>,?(B&)	 with equation (2.17). The parameter J and K are learnable parameters of the batch normalization layer which is used to adjust mean and variance during training.  In order to use batch normalization with LSTM layers, equation (2.18) is calculated to forget gate (ft), candidate (ct), input gate (it), and output gate (ot). Then, #! and "! are produced using equation (2.19) and (2.20).  L&'(⃛'"'*'M=CI(*8!!;J8K8)+CI(,@"!"#;J@K@)        (2.18)  #!='($!)×#!"#+'(6!)×012ℎ(.⃛!)   (2.19)  "!='(7!)×012ℎ(C2(#!;J%K%)-            (2.20)  Ioffe et al. [22] developed a method to address various issues related to the training of deep neural networks, which batch normalization was able to produce significant improvements in terms of the number of iterations required to train the network. The normalization was initially introduced to handle the problem of internal covariate shift, which was the change in the input distribution of a deep neuron network (DNN). Meanwhile, the input of each layer of DNNs was affected by parameters in every input layer. Batch normalization was able to reduce internal covariate shift by correcting the means and variances of each input layer. Bjorck et al. [70] stated that batch normalization helped during optimization and improved the final test accuracy. It was able to create a well-generalized model because of the usage of a large learning rate. Finally, Chen et al. [71] combined batch normalization with dropout to construct independent activations for neurons in each intermediate weight connecting between layers in order to overcome the high computational complexity to perform independent components analysis.     24 2.4 Evaluation In the literature review presented in this thesis, word prediction models are commonly      evaluated with perplexity [18, 19, 33, 34, 66]. In NLP, perplexity is often used for measuring the usefulness of a language model which basically is a probability distribution over entire sentences, phrases, or sequences of words.  The best way to evaluate the performance of a language model is to measure how much the accuracy of the prediction of the model has improved. An intrinsic evaluation metric is used to measure the quality of a model independent of any application. For an intrinsic evaluation of an LM, the quality of a model was measured by its performance on some unseen data which is called the test set or test corpus. Two different LM models are compared by dividing the corpus data into training set and test set in order to train the parameters of both models on the same training set and then compare how well the two trained models fit the test set. The better model is the one that has a tighter fit to the test set by producing higher probability of the suggested words in the test set.  In practice, a variant which is called perplexity is used instead of using raw probability for evaluating language models. The perplexity of a LM on a test set is the inverse of the probability distribution over entire words of the test set, normalized by the number of words. Because of the inverse, the higher the conditional probability of the word sequence is, the lower the perplexity is. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the LM. This means the lower the value of the perplexity is, the better the language model is, as the model fit to the test data. Equation (2.21) to equation (2.25) are used for calculating perplexity.  PP(,)=P(<#<)…<+)"+#    (2.21)  PP(,)=R#A(/+/%…/#)#    (2.22)  PP(,) in equation (2.21) and (2.22) are the perplexity of the model which is the inverse of the probability of all words in the test set. From equation (2.22), Chain rule is applied to expand the probability of words to produce equation (2.23). Practically, language   25 model probabilities are usually very small, it is often that logarithmic function is applied as shown in equation (2.24) and (2.25).  PP(,)=R∏#AD<&E<&"#,<&"),<&"F,…G+&1##    (2.23)  PP(,)=	expX#+(log	(∏#AD<&E<&"#,<&"),<&"F,…G+&1#))\  (2.24)  PP(,)=	exp?#+(−∑]7^(P(<&|<&"#,<&"),<&"F,…)-+&1#-@   (2.25)  This perplexity equation is now be seen as the average of log probability of each word in the test data. It is basically a quantity which is now normalized with respect to the length of the test data.  Since the evaluation metric is based on the probability of word in the test set, it is important that test data have to be an unseen example corpus. If the test corpus is a part of the training corpus, the models will mistakenly assign an artificially high probability to a word when it occurs in the test set. This process introduces a bias that makes the overall probabilities seem too high and causes huge inaccuracies in perplexity. The test set should also be as large as possible, since a small test set may be accidentally unrepresentative. The idea is to pick the smallest test set that at least statistically represent the corpus so that the significant difference between two potential models can be measured.             26 CHAPTER 3 METHODOLOGY   This research proposes Thai language modeling using a combination of character-level and word-level LSTM as a predicting model. The corpus data are BEST2010 which is a commonly used Thai language corpus. For preprocessing, the corpus data are segmented into words, then filter out all rarely used words, including numbers and punctuation marks as Thai language rarely uses marks. The combination model of character and word LSTM, implementing with Python programming language and Tensorflow, is used in modeling the data in the training process. Finally, in terms of performance, perplexity is used in evaluating the model by comparing various experimental LSTM models with the baseline word-level LSTM model.   Figure 3.1 The methodology diagram  The methodologies, depicted in figure 3.1, are separated into four main phases: data collection phase, data preprocessing phase, data processing phase, and evaluating phase. First, the data collection phase is to collect Thai corpus that covers a variety of levels of Thai language. Second, the data preprocessing phase consists of two processes: Thai word segmentation and data clearing in order to filter rarely used words, including numbers and punctuation marks. Next, the assembled data is transferred to the data processing phase for training with the proposed model. Last, the evaluating phase is conducted to analyze the performance of the proposed model with Thai language.  3.1 Data Collection This work used BEST2010, the Thai corpus of about one million words from NECTEC [35], a dynamic organization responsible for the development of information technology in Thailand. BEST2010 was compiled from articles, news, encyclopedias, and novels. 
Data 
Collection 
Preprocessing 
Processing 
Evaluation   27 This corpus also covered word segmentation, entity recognition (NER), and abbreviations which were required before implementing LM.  At first, two Thai corpora which are HSE and BEST2010 were studied and compared. The first source is HSE Thai Corpus [72], which is the corpus of modern texts written in Thai language with approximately 50 million words.  In HSE Thai Corpus, the texts of around 50 million words, were collected from various Thai websites which are mostly news websites. HSE Thai Corpus is being developed by the team of students of HSE School of Linguistics in Moscow under the guidance of Professor Boris Orekhov. The part of HSE Thai Corpus is shown in figure 3.2.  
 Figure 3.2 HSE Thai corpus  The second source is the BEST2010 from NECTEC, as shown in figure 3.3, which is compiled from articles, encyclopaedias, news, and novels, that contains around 1 million words. From these two data samples, it can be seen that the HSE Corpus has bigger size of data than BEST2010. HSE Corpus provides more information about the word such as POS and English translation. However, almost all information from HSE Corpus comes from news websites which do not cover much of the language usage as BEST2010 Corpus does. Even though BEST2010 corpus has smaller size, but its data includes articles, 
  28 encyclopaedias, news, and novels. BEST2010 corpus also has word segmentation, NER, and abbreviation’s identification.  
 Figure 3.3 BEST2010 corpus  3.2 Data Preprocessing Typically, every first step of an NLP problem is preprocessing the raw corpus. To prepare data for the next processes, the data should be preprocessed by word segmentation, data cleansing, creating Thai dictionary, and NER. Because of the corpus used in this research have already segmented words and had NER process, the segmentation process in the preprocessing is unnecessary. However, data cleansing process is needed to remove unnecessary characters which makes the data easy to process.  From the BEST2010 Corpus, the data were very clean already, but there are still some unnecessary symbols to be eliminated. By cleansing the data, the vertical bar “|” that is used to separate each word was eliminated and the space was used to divide the word instead. This because “|” unnecessarily increases the size of the corpus data and may cause the processing to slow down as well. In addition, there are almost all other symbols that are cut out except dot “.” which is required for various abbreviations. Figure 3.3 and 3.4 show the sample data before and after data cleansing.  
  29 
 Figure 3.4 BEST2010 corpus after data cleansing  After cleansing the data, the data was divided into three parts: 80% of the data are for training, 10% of the data are for testing, and another 10% of the data are for validation. Then the training data are used in the data processing.  3.3 Data Processing This section covers the software used in all experiments, including parameters and configuration. After that, the proposed model and the input data are described.  3.3.1 Software Setup The models in this thesis were developed using Python version 3.6 to run scripts as it provides many extensive standard libraries for deep learning especially TensorFlow and Numpy. TensorFlow version 1.12 is used to develop and train deep learning models. NumPy version 1.16 is also used for collecting an efficient multi-dimensional of generic data. The experiments utilize NumPy arrays to deal with advanced mathematical operations and other types of operations on a large number of data.  3.3.1.1 Python Programming Language Python [73] is a high-level programming language with dynamic semantics, which makes it very attractive for rapid application development. Python was built for its readability and less complexity, so it is easy to learn and understand by humans. It provides many extensive standard libraries that can be freely distributed. More importantly, it is popular 
  30 in machine learning and artificial intelligence field because it is stable and flexible, and it also has effective tools available.  3.3.1.2 TensorFlow TensorFlow [74] is an end-to-end source for fast numerical computing controlled by a simple Python API. It has comprehensive tools, libraries, and resources that allows researchers to include the state-of-the-art in their models.  TensorFlow is a popular open source for deep learning, since it can be used to develop and train deep learning models directly or using libraries that simplify the process built on top of TensorFlow. This means TensorFlow offers multiple levels of abstraction to choose from to satisfy the requirements of the models.  Unlike other numerical libraries that are used in deep learning, TensorFlow was designed for using both in research and development and in production systems. It can run on systems with only CPU, systems with GPUs, mobile devices, or large-scale distributed systems of hundreds of machines.  3.3.1.3 NumPy NumPy [75] is a Python library that provides the fundamental package for scientific computing. It offers a multi-dimensional array object, various derived objects such as matrices, and an assortment of routines for fast operations on arrays. It contains useful mathematical logic such as linear algebra, Fourier transform, and random simulation and much more. NumPy gives the programmers complete freedom to code in whichever coding paradigm because it fully supports an object-oriented programming.  The ability of NumPy to be used as an efficient multi-dimensional container of generic data allows NumPy to be seamlessly and rapidly integrated with a wide variety of databases. NumPy arrays, which are called ndarray, facilitate advanced mathematical operations and other types of operations on large numbers of data.       31 Table 3.1 Parameters configuration  Parameter Meaning Default Setting layer (string) type of cell LSTM num_layer (integer) number of hidden layers 3* layer_size (integer) size of the hidden layer 200 batch_size (integer) size of samples from training dataset that put through the network at a time 20 num_steps (integer) number of steps used for enrolling for training with backpropagation through time 20 init_scale (float) the weights of the model will be randomly initialized, with a uniform distribution and values between -init_scale and init_scale (e.g. between -0.1 to 0.1) 0.1 forget_bias (integer) initial bias of the forget gate in the LSTM cell 0 optimizer (string) type of optimizer (e.g. ‘sgd' for stochastic gradient descent) sgd dropout (float) probability at which neurons are dropped (e.g. 0.5 means 50% of neurons are dropped) 0.25* learning_rate (float) initial learning rate 1 max_epoch (integer) number of epochs during which the initial learning rate should be used 6 lr_decay (float) determines how fast the learning rate decays 0.5 max_max_epoch (integer) the total number of epochs to train 15 word_char_concat (bool) input consists of the concatenation of the word embedding and embeddings of the characters part occurring in the word, output is still words. TRUE* num_char (integer) number of characters to add or number of character embeddings (if the current word is shorter than *num_char*, padding symbols are used; if it is longer than *num_char*, only part of the characters in the word are added) 1* char_size (integer) size assigned to each character embedding 5*     32 3.3.2 Parameters Configuration For training deep learning models, there are some relevant parameters that must be defined. Note that these parameters can be changed to tailor each experiment. All parameters used in the experiments are listed in table 3.1.  For every parameter, they are specified between brackets what type of value to expect. For example, num_layer (integer) means that the value of 'num_layer' should be an integer. This parameter configuration is similar to Verwimp et al. [18] configuration and Miyamoto and Cho [20] configuration except for some value that was changed when another value gives the better perplexity result, such as number of layers, which was addressed later in section 4.2.1. The value in a default setting column that have ‘*’ means that value will be changed depending on the experiment.  From table 3.1, all LSTM models in this section shared the same LSTM LM architecture of 3 layers and 200 hidden units in which the total size of the embedding layer was equal to the size of the hidden layer. All models were run with 15 epochs. In the first 6 epochs, the learning rate was set to 1. After that, the models were applied an exponential decay. The weights were randomly initialized with uniform random variables between -0.1 and 0.1. All models were trained using stochastic gradient descent (SGD) with 20 mini-batch sizes, where the number of steps used for enrolling for training with back-propagation through time was 20. The number of characters added to a word for character-word LSTM model is fixed to 10 in this proposed work, choosing from the experiments conducting from n=1 up to n=10. If a word is longer than n characters, only the first n characters are added. If the word is shorter than n, it is padded with a special symbol.  3.3.3 Input data The input data is preprocessed into word-based and character-based datasets. First is the word data, which is Thai corpus collected from BEST2010 as shown in figure 3.4 which is used in all models.     33 
 Figure 3.5 Character data at position 0  
 Figure 3.6 Character data at position 1  Another data is the character data created by separating each position of character of words in Thai corpus for concatenating with the word data itself. This character data is only used in character-word LSTM models. Each character of word is separated into a file of each position. For example, character at position 0 of all words in the corpus is collected together in a file call “char_0”. Figure 3.5 and 3.6 are the parts of data from file “char_0” and “char_1”, respectively, which process from the beginning of each word.  
  34 In these experiments, the default number of characters is 10, from position 0-9, since creating a data after the 10th position the remaining characters are very small. Meaning that there is rarely word that have more than 10 characters as figure 3.7 shows the character in 9th position.  
 Figure 3.7 Character data at position 9  3.3.4 Model As mentioned in section 2.3.3.3, deep learning can solve a very ambiguous language. The method that outperforms the other approaches of generating text is LSTM. In this research, the character-word LSTM model is the main focus, since some researches addressed the performance of this model in terms of perplexity with rich morphology language and also ability to solve the problem of OOV. Word-level NLM are trained on word-level representations and thus are quite effective at capturing semantic meaning. Character-level NLM, on the other hand, are trained on character-level representations and thus potentially contain more information about morphemes features. The model of this research was inspired from a recent paper by Miyamoto and Cho [20].   In character-word LSTM models, the computation of the word embedding is the result of word and character input rather than word input only. the output vector comes from concatenating the word embedding with embeddings of the characters occurring in that word as shown in figure 3.8. By concatenating the embeddings, the character of a word always corresponds to the same portion of the input vector for the LSTM. This model 
  35 reduces the number of errors made immediately after OOV words because OOV words can still be found in the character information added in character embedding.  
 Figure 3.8 The architecture of the character-word LSTM language model  After getting the output vector of word embedding, the output vector is used as an input to LSTM LM (t) and process together with the output of LSTM LM from the previous process (t-1). Finally, in the output layer, probabilities for the next word are calculated using a Softmax function.  The example of how character-word LSTM model works is shown in figure 3.9. First, the input word, “ตรวจ”, is separated into word information and character information. Second, word and character information are used for creating word embedding and character embedding from the lookup tables. Third, word embedding and character embedding are concatenated to create an embedding matrix of shape (20, 20, 200). The shape comes from batch size, the number of steps used for enrolling with backpropagation, and the number of hidden units, respectively. Then, the output of embedding goes into LSTM layers together with the output of LSTM LM from the 
  36 previous process to produce the hidden state in the matrix of shape (20, 20, 200). After the third LSTM layer, the output matrix goes into Softmax layer to obtain distribution over the next word. Finally, the output word is the best possible output from generate Softmax which in the example is the word “พิสูจน์”.  
 Figure 3.9 Example of how the model works  Figure 3.10 and 3.11 show python code of the process of character-word LSTM LM with and without batch normalization, respectively. There are four main parts to create character-word LSTM model in python. First, LSTM layers are created following the model configuration. Second, word embedding and character embedding are created and are concatenated altogether. Third, a dropout is added to input layers and each node in a hidden unit. Finally, batch normalization is added to the input layers as in figure 3.11. 
  37 
 Figure 3.10 The code of character-word LSTM process in Python  
 Figure 3.11 Batch normalization added to the model in Python  3.4 Evaluation The baseline in for evaluation process is the combination of character-word LSTM with the dropout value (the percentage of neurons to be dropped) and batch normalization method. Because the evaluation of traditional models is also needed to compare with the 
  38 proposed model, therefore, eight LSTM models were tested and compared using perplexity. The eight models are in the following list. 1) Word level LSTM - This model is used as a baseline model for comparing with the proposed model. 2) Word level LSTM with 0.25/0.50/0.75 of dropout values (25%/50%/75% of neurons are dropped). 3) Word level LSTM with batch normalization. 4) Word level LSTM with both 0.25/0.50/0.75 of dropout values and batch normalization 5) Character-word LSTM 6) Character-word LSTM with 0.25/0.50/0.75 of dropout values 7) Character-word level LSTM with batch normalization 8) Character-word level LSTM with both 0.25/0.50/0.75 of dropout values and batch normalization                      39 CHAPTER 4 IMPLEMENTATION RESULTS    This chapter shows the comparisons of the results on eight LSTM models, which are divided into five sub-section; word-level LSTM LM, character-word LSTM LM, character-word LSTM LM with dropout (DO), character-word LSTM LM with batch normalization (BN), and character-word LSTM LM with both DO and batch normalization.  4.1 Word-Level Models At first, baseline word-level LSTM model was experimented to find number of layers, that was used in subsequent experiments. Then, the perplexity of this model was calculated and later on compared with the proposed models.  Baseline word-level LSTM model is the most popular language model for many languages, including Thai. In addition to experiment with word-level LSTM to compare with proposed model, this model was also used to find number of layers that gave the lowest perplexity to be used in subsequent experiments.  Word-level model used in this experiment is a small model that was configured as shown in table 4.1 but without number of embedding characters and character embedding size as this model does not need character information. In order to find number of layers that provides the lowest perplexity, the number of LSTM layers was set from 1 up to 4.  Table 4.1 Baseline word-level LSTM models  Model Number of Layer(s) Number of Units Perplexity Word-level LSTM 1 200 316.080 2 200 269.857 3 200 255.958 4 200 259.854    40 From the results in table 4.1, the model with 3 LSTM layers produced the lowest perplexity. This configuration was used in other models, including the character-word LSTM model as well. Table 4.2 shows perplexity values of word-level LSTM models when adding dropout (DO) and batch normalization (BN) which are used later for comparing with character-word LSTM models.  Table 4.2 Word-level LSTM models with dropout and batch normalizatoin  Model Perplexity Word-level LSTM with 25% DO 262.945 Word-level LSTM with 50% DO 215.151 Word-level LSTM with 75% DO 214.096 Word-level LSTM with BN 238.018 Word-level LSTM with 25% DO and BN 275.641 Word-level LSTM with 50% DO and BN 218.782 Word-level LSTM with 75% DO and BN 208.078  4.2 Character-Word Models In the character-word LSTM model experiment, every configuration is similar to the baseline word-level LSTM model in section 4.2.1. except the number of LSTM layers that was set to 3 according to the experimental results in section 4.2.1. Since character-word LSTM model is the model that combines both word and character information, this model requires number of characters embedding and character embedding size as additional input data. Therefore, this model has 3 layers 200 units and varieties of character embedding sizes.  The result in table 4.3 shows that only the character-word LSTM models in which adding 1 and 2 character embeddings of size 15 performs worse than baseline word-level LSTM model, which is shown in table 4.2. The best performing model is the model that adds 8 character embeddings of size 15, giving a perplexity of 232.836.      41 Table 4.3 Character-word LSTM models  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM 5 1 248.958  2 245.317  3 245.745  4 246.150  5 240.061  6 237.647  7 241.431  8 238.751  9 241.787  10 242.054 10 1 251.863  2 246.883  3 240.796  4 236.816  5 245.024  6 238.732  7 237.482  8 237.330  9 237.364  10 234.952 15 1 258.229  2 256.784  3 240.649  4 240.237  5 245.250  6 237.059  7 240.087  8 232.836  9 234.767  10 234.960 20 1 250.783  2 244.808  3 239.179  4 236.188  5 239.425  6 239.456  7 235.240  8 239.215  9 241.043  10 237.372   42 Table 4.4 Character-word LSTM models with 25% dropout  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with 25% Dropout 5 1 263.554  2 262.543  3 263.943  4 261.739  5 264.341  6 263.838  7 264.561  8 263.626  9 265.051  10 268.243 10 1 260.695  2 265.013  3 264.665  4 264.567  5 265.071  6 268.797  7 271.834  8 271.966  9 273.747  10 277.278 15 1 264.458  2 265.152  3 266.027  4 267.472  5 270.243  6 273.899  7 278.383  8 284.512  9 291.967  10 300.267 20 1 264.319  2 265.021  3 269.485  4 272.009  5 277.637  6 286.762  7 292.299  8 318.751  9 337.601  10 374.537   43 Table 4.5 Character-word LSTM models with 50% dropout  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with 50% Dropout 5 1 210.045  2 209.689  3 208.45  4 206.655  5 207.28  6 208.532  7 209.472  8 207.446  9 207.24  10 207.988 10 1 207.753  2 207.156  3 208.516  4 207.142  5 206.562  6 207.334  7 207.828  8 208.56  9 210.426  10 220.309 15 1 208.355  2 208.765  3 206.745  4 206.605  5 208.413  6 207.856  7 208.201  8 212.489  9 214.851  10 219.465 20 1 207.484  2 207.035  3 207.482  4 208.989  5 210.475  6 212.542  7 214.838  8 221.146  9 234.720  10 256.256   44 Table 4.6 Character-word LSTM models with 75% dropout  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with 75% Dropout 5 1 207.853  2 206.722  3 207.297  4 205.617  5 208.423  6 206.623  7 206.517  8 203.635  9 205.105  10 204.465 10 1 209.022  2 207.817  3 205.744  4 203.603  5 207.872  6 202.260  7 204.399  8 203.008  9 204.651  10 203.335 15 1 208.855  2 207.688  3 202.862  4 202.871  5 203.625  6 202.714  7 202.274  8 202.091  9 203.991  10 203.746 20 1 207.644  2 203.158  3 204.191  4 202.98  5 202.756  6 202.819  7 201.94  8 203.563  9 210.934  10 223.586   45 4.3 Character-Word Models with Dropout This section shows the experiments of character-word LSTM model with dropout. The LSTM models have 3 layers 200 units with varieties of dropout values and character embedding size.  4.3.1 Character-Word Models with 25% Dropout According to the table 4.4, there are only three models that produced better results comparing with word-level LSTM model with the same dropout value in table 4.2. These three models are the model that adds 2 and 4 character embeddings of size 5 and the model that adds 1 character embeddings of size 10 which give 262.543, 261.739, and 260.695 perplexity value, respectively.  4.3.2 Character-Word Models with 50% Dropout Since dropout affects the performance of the model, therefore, this experiment was performed by increasing the dropout from 25% to 50%. Other parameters were the same as the previous model, including testing on various character embedding sizes.  From table 4.5, the results reveal that only the character-word LSTM models in which the concatenated character embeddings take up the majority of the total embedding perform worse. For example, for a total embedding size of 200, adding 9 character embeddings of size 15 results in a character embedding consisting of 135 units, thus there is only 65 units left for word embedding, which is not sufficient. The best two performing models are the model that adds 5 character embeddings of size 5 and 10, giving a perplexity of 206.655 and 206.562, respectively  The result in table 4.5 is similar to the experiment of Verwimp et al. [18] in terms of the analysis, even though the setup is different in number of layers, dropout value, and language of dataset.  4.3.3 Character-Word Models with 75% Dropout Since increasing dropout value from 25% to 50% improve the performance of the model, therefore, the next experiment was increased the dropout from 50% to 75% to see if there   46 are any improvement. Other setups are remaining the same as the previous model, including testing on varieties of character embedding sizes.  From the results in table 4.6, the overall perplexity is improved from 50% dropout. Every value of character embedding is better than word-level LSTM model with 75% dropout (214.096) except the one that adds 10 character embeddings of size 20 which perform the worst. The best performing models is the model that adds 7 character embeddings of size 20 resulting in perplexity of 201.94.  The results from the table 4.6 show that although the character embedding take up the majority of the total embedding, the performance is still better than baseline word-level LSTM model, unlike results of the previous models. In addition, the obvious thing is that increasing the dropout value from 50% to 75% improve overall perplexity of the model.  4.4 Character-Word Models with Batch Normalization In this section, batch normalization was added to Character-word LSTM models. The models still have 3 layers 200 units with variety of character embedding size.  The result in table 4.7 shows that only the character-word LSTM model with batch normalization in which adding 1 character embedding of size 20 performs worse than word-level LSTM model with batch normalization (238.018). The best performing model add 9 character embeddings of size 20, giving a perplexity of 222.491.  4.5 Character-Word Models with Dropout and Batch Normalization The models of character-word LSTM with combination of dropout and batch normalization also have 3 layers 200 units with batch normalization and variety of dropout values.  4.5.1 Character-Word Models with 25% Dropout and Batch Normalization Accounting to the result in table 4.8, there are only the models that produced the better result compare with word-level LSTM model with the same dropout value (275.641), which is snown in table 4.2. These three models are the model that adds 4 character embeddings of size 5 and the model that adds 2 and 3 character embeddings of size 10   47 which give 275.333, 275.218, and 275.134 perplexity value, respectively. Moreover, considering the overall result compare with table 4.4, the model with dropout and batch normalization perform worse than the model with only dropout.  4.5.2 Character-Word Models with 50% Dropout and Batch Normalization The result in table 4.9 shows that all results from Character-Word LSTM with 50% dropout and batch normalization perform worse than word-level LSTM with the same configuration (218.782), which is shown in table 4.2.  4.5.3 Character-Word Models with 75% Dropout and Batch Normalization From the results in table 4.10, the overall perplexity performs the best compare with all model which was experimented so far in this research. The best performing models is the model that adds 8 character embeddings of size 5 resulting in perplexity of 199.909.                       48 Table 4.7 Character-word LSTM models with batch normalization  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with Batch Normalization 5 1 235.265  2 232.518  3 228.942  4 254.114  5 228.798  6 232.803  7 234.516  8 226.528  9 226.878  10 228.747 10 1 232.885  2 233.752  3 229.108  4 227.179  5 227.514  6 232.028  7 230.279  8 232.221  9 226.245  10 227.611 15 1 244.695  2 236.336  3 227.871  4 229.583  5 227.458  6 228.059  7 228.32  8 226.471  9 223.304  10 224.478 20 1 238.801  2 226.938  3 232.35  4 230.339  5 229.147  6 223.398  7 226.845  8 225.820  9 222.491  10 236.655   49 Table 4.8 Character-word LSTM models with 25% dropout and batch normalization  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with 25% Dropout and Batch Normalization 5 1 275.488  2 275.911  3 276.257  4 275.333  5 278.829  6 278.865  7 278.608  8 278.076  9 279.088  10 279.619 10 1 273.817  2 275.218  3 275.134  4 276.437  5 279.353  6 281.450  7 283.558  8 284.035  9 286.071  10 287.547 15 1 276.409  2 277.873  3 278.566  4 281.914  5 281.360  6 285.887  7 286.589  8 290.773  9 296.648  10 312.382 20 1 277.320  2 278.629  3 280.004  4 282.201  5 286.672  6 294.101  7 300.008  8 316.963  9 330.873  10 392.447   50 Table 4.9 Character-word LSTM models with 50% dropout and batch normalization  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with 50% Dropout and Batch Normalization 5 1 213.970  2 214.359  3 214.343  4 214.711  5 215.438  6 215.297  7 214.997  8 214.862  9 214.701  10 214.761 10 1 215.598  2 215.645  3 216.611  4 214.338  5 218.011  6 216.778  7 215.944  8 218.493  9 218.842  10 218.694 15 1 214.538  2 214.266  3 215.542  4 215.998  5 217.045  6 218.042  7 219.207  8 220.291  9 226.078  10 229.296 20 1 217.012  2 215.644  3 215.909  4 217.749  5 218.870  6 221.664  7 226.604  8 232.007  9 241.303  10 270.151   51 Table 4.10 Character-word LSTM models with 75% dropout and batch normalization  Model Character Embedding Size Number of Character Added Perplexity Character-Word LSTM with 75% Dropout and Batch Normalization 5 1 205.485  2 201.261  3 203.991  4 204.224  5 202.055  6 201.91  7 202.858  8 199.909  9 201.719  10 202.399 10 1 201.561  2 202.903  3 202.937  4 202.809  5 202.562  6 201.419  7 204.232  8 202.946  9 202.965  10 203.711 15 1 203.767  2 204.881  3 202.64  4 203.101  5 201.74  6 205.412  7 204.53  8 206.358  9 206.49  10 208.655 20 1 206.021  2 203.641  3 202.816  4 203.091  5 203.178  6 204.3  7 208.643  8 209.565  9 216.546  10 237.248   52 CHAPTER 5 CONCLUSIONS   This chapter includes discussions, conclusions, and directions for future works of this study. In the discussion, the results are analyzed and compared in a few graphs which is easier to understand. Then the summary of work and findings are concluded, following by the discussion of what can be improved over the proposed model.  5.1 Discussions  The reason of using character-word LSTM LM with dropout and batch normalization is to increase the performance of Thai LM. The performance analysis of all models is described in this section. There are four topics to be discussed, in order to analyze all results from eight LSTM models: comparing baseline models, comparing models when added dropout, comparing models when added batch normalization, and comparing models when added both dropout and batch normalization.   
 Figure 5.1 Perplexity of word-level LSTM and character-word LSTM  5.1.1 Comparing Baseline Word-level LSTM with Character-word LSTM From figure 5.1, BW is baseline word-level LSTM, CW is character-word LSTM, and CE is character embedding used in these models. The graph shows that almost all perplexity values of character-word LSTM are better than baseline word-level LSTM. The best value is when the number of characters added is 8 and has character embedding 
  53 size of 15 (232.836), which better than the baseline 9.03%. The worst perplexity value is when the number of characters added is 15 and has character embedding size of 1 (258.229). Character-word LSTM model produces a better result than the baseline word-level LSTM model because character-word LSTM model includes subword-level information that the baseline which includes only words does not have the ability to recognize. Therefore, the Character-word LSTM model reduces OOV words with character information and increase the predicting performance.  5.1.2 Comparing Models When Adding Dropouts In table 5.1, W+DO25, W+DO50, and W+DO75 are word-level when adding 25%, 50%, and 75% dropout, respectively. The perplexity of character-word LSTM model with 75% dropout was better than every value produced from baseline as shown in figure 5.2. The best value is when number of characters added is 7 with character embedding size of 20 (201.940), which better than baseline up to 21.10%. By comparison, the perplexity of character-word LSTM with 0.25 dropout value are greater than the baseline but they were improved when the dropouts are 0.50 and 0.75, respectively.   Table 5.1 The summary of perplexities of models when adding dropout values  Model Perplexity BW 255.958 W+DO25 262.945 W+DO50 215.151 W+DO75 214.096 CW 232.836 CW+DO25 260.695 CW+DO50 206.562 CW+DO75 201.940  This is because a dropout value of 1.0 means no dropout and dropout value of 0.0 means no output from the layer; therefore, 75% dropout or dropout value of 0.75 means retaining inputs from input layers and hidden nodes for 75% out of the neural network. Inversely, the output of the layer is dropped out of the neural network by only 25%. Thus, the model improves when the dropout value is higher as a common value for dropout in each node   54 in a hidden layer is between 0.5 and 0.8 while the input layers use a larger dropout rate that close to 1.0 [76].  
 Figure 5.2 Perplexity of models when adding dropout values  5.1.3 Comparing Models When Adding Batch normalization BN in Table 5.2 is batch normalization adding to the model. The result shows that every perplexity value of character-word LSTM with batch normalization is better than baseline word-level LSTM as shown in figure 5.3. The best value is when the number of characters added is 9 and has character embedding size of 20 (222.491), which better than baseline 13.08%. However, comparing with the results of models adding 50% and 75% dropout in table 5.1, adding dropout provided better performance than adding batch normalization in terms of perplexity.  Table 5.2 The summary of perplexities of models when adding batch normalization  Model Perplexity BW 255.958 W+BN 238.018 CW 232.836 CW+BN 222.491 
  55 
 Figure 5.3 Perplexity of models when adding batch normalization  Figure 5.4 shows the relationship between perplexity and epoch during the training process of character-word LSTM model with and without batch normalization for both training dataset and validation dataset. Although, as measured by the perplexity of the training dataset, the model with batch normalization produces lower perplexity faster than the model without batch normalization, at around 6 epoch the improvement of the model with batch normalization begins to lower than the model without batch normalization. For the perplexity of the validation dataset, the model with batch normalization reaches lower perplexity faster than another model. A validation dataset is the holdout test dataset that is used for evaluating its performance at the end of each training epoch. Therefore, the conclusion is that batch normalization results in slightly lower training times because batch normalization effects on validation accuracy.  
 Figure 5.4 Perplexity for each epoch in a training process   
  56 5.1.4 Comparing Models When Adding Dropout and Batch Normalization From the result in table 5.3, the perplexity of character-word LSTM with 75% dropout and batch normalization is better than every values produced from baseline. The best value is when number of characters added is 8 with character embedding size of 5 (199.909), which better than baseline up to 21.90%. By comparison, the perplexity of character-word LSTM with 0.25 dropout value and batch normalization are greater than the baseline but they were improved when the dropouts are 0.50 and 0.75, respectively.  Table 5.3 The summary of perplexities of models when adding dropout values and batch                   normalization  Model Perplexity BW 255.958 W+DO25+BN 275.641 W+DO50+BN 218.782 W+DO75+BN 208.078 CW 201.940 CW+DO25+BN 273.817 CW+DO50+BN 213.970 CW+DO75+BN 199.909  
 Figure 5.5 Perplexity of models when adding both dropout and batch normalization   
  57 From figure 5.5, it can be seen that the best result was achieved by character-word LSTM model with 75% of neuron dropout and batch normalization. This is because retaining inputs from input layers and hidden nodes for 75% and dropping out the output of the network’s layer by 25% produce the best result in term of perplexity value as in section 5.1.2. In addition, batch normalization also helps to reduce internal covariate shift during training and improve accuracy of the model by lower perplexity in each training epoch.   5.2 Conclusions  This thesis is the design and development of LSTM LM for Thai language. Eight LSTM models had been tested on Thai dataset of one million words and compared based on the perplexity value. Word-level LSTM model is used as a baseline method. Character-word LSTM model, our main model to experiment, was created by concatenating word and character embedding before putting them through LSTM layers. Various numbers of characters concatenating together with the word had been tested. Then, the perplexity of character-word LSTM model was less than baseline word-level at every proportion of character embedding.  Moreover, dropout and batch normalization were applied to the models to mitigate the problem of overfitting. By adding dropout, the models performed better compare with baseline. This is because increasing dropout makes the model more robust as each layer does not have to depend on other layers, causing each layer to develop itself to work better on its own. Adding batch normalization to the model, even though it improves the performance of baseline, the results are worse than adding dropout. Finally, both dropout and batch normalization were added to models and the result shows that they gave the best performance on this dataset.  5.3 Future Work For future work, there are three main points to be considered. First, these models are designed as a small model, hence large models or even different datasets may give different results and conclusions. Second, there might be better procedures for corporate words with characters that are not direct concatenation. For example, implementing word-level LSTM separately with character-level LSTM and combine them later in the predicting process. Lastly, this thesis used batch normalization in order to raise the   58 performance of the model, which layer normalization can be instead considered to investigate its effect over this character-word LSTM model.                                   59 