 
LOG DATA TRAFFIC CHARACTERIZATION FOR PACKET  
LOSS ESTIMATION IN ALICE O2 SYSTEM  

Loss estimation is considerably significant for network planning processes and plays a 
main role in bandwid th allocation optimization, network design, guaranteeing quality of 
service (QoS), etc. According to The European Organization for Nuclear Research 
(CERN), the ALICE O2 computing system has nodes, called First Level Processors 
(FLPs), which collect particl e interaction data from ALICE detectors and carry out local 
processing. Log data generated by tasks running on FLPs are sent over a network to the 
Logstash. The log is then filtered and sent to the Elasticsearch and Kibana for future 
anomaly detection. Lar ge amounts of log -data traffic from FLPs over this network could 
lead to packet loss. In this research, we create FLPs in a testbed environment to 
characterize the log -data traffic generated by tasks in FLPs and fit the data to time -series 
models and proba bility distributions assuming independent interarrival times. The fitted 
models are then used to study end -to-end packet loss with input traffic from a large 
number of FLPs in a network of switches. The simulation results can help predict the 
number of FLP s and traffic intensity that the network can sustain for different kinds of 
tasks running on FLPs. Lastly, in order to find the best represented model compared to 
the real trace result, we performed model verification and took into account the end-to-
end packet loss and queue utilization of each task.  
 
Network Resource Planning/ Traffic Model/ Queuing Network/ Stochastic 
Model  iii 

 CHAPTER 1  INTRODUCTION  
1.1 Background  
CERN (The European Organization for Nuclear Research) was established in the 1950s 
and is one of the most famous organizations that performs research and experiments in 
the field of particle physics. Today it is known as the European Laboratory for Particle 
Physics. CERN aims to understand the nature of universe such as how the uni verse began 
or how the particles interact with each other which could lead to new discoveries.  
 
To find out how particles interact with each other, CERN built the Large Hadron Collider 
(LHC) deep underground in France and Switzerland. The LHC is the largest and the most 
powerful particle accelerator in the world. The machine accelerates ions at a velocity 
approaching the speed of light. The accelerated ions will collide with each other and the 
result is recorded. The data gathered from the LHC is used  in further experiments. 
Scientists hope to learn more about how the universe began to make new discoveries.  
 
The LHC consists of 4 main particle detector bases. One of the particle detector bases is 
A Large Ion Collider Experiment (ALICE), a heavy  ion det ector. During the latest 
maintenance break (2018 -2022), the ALICE experiment received a significant upgrade 
[1] to increase the capability of the data collection process in order to collect collision 
data at a rate of 50 kHz for minimal bias Pb -Pb and 200 kHz for pp and p -Pb collisions. 
Thus, ALICE has designed new facilities to support data gathering.  
 
A new Online -Offline (O2) computing system [2] has been developed to sustain the large 
detector data input. It is composed of more than 500 nodes in charge of collecting, 
aggregating , and processing the data. The system contains a set of 200 nodes called First 
Level Processor (FLP) that collects data of particle interactions from the detector at a rate 
of 3.4 TB/s. The data is then sent to a second group of m achines, the Event Processing 
Nodes (EPNs), for global aggregation and processing before being recorded for further 
analysis. The system aims to reduce the detector readout volume in order to minimize the 
cost and requirements for data processing and stora ge on the computing system.  
 2 
 CERN and King Mongkut's University of Technology Thonburi (KMUTT) have 
collaborated to come up with a future AI -based logging system for ALICE O2 facilities. 
The main role of the AI -based logging system is to utilize logged dat a generated from the 
components to monitor and identify the events registering as normal or abnormal. The 
system uses the ELK Stack (Elasticsearch, Logstash, and Kibana), which is one of the 
most popular open -source software for log management platforms. T his research focuses 
on network resource estimation for such a system by collecting the network logs on the 
FLPs. Packetbeat, which is a log -shipper, is used as a helper in network analysis for 
monitoring and capturing the network traffic.  
 
1.2 Motivation  
The ALICE O2 system is different from similar systems and ALICE experiment complexity 
is directly proportional to data. ALICE uses rapidly changing and abundant information, 
which could cause incomplete experiment data which in turn directly affects the an alysis 
process with any previously installed AI -based logging system. Thus, the AI -based 
logging system based on ALICE O2 needs to first be tested with simulations in order to 
estimate network resources before being installed in the production.  
 
Loss estim ation is considerably significant for network planning processes and plays a 
main role in bandwidth allocation optimization, network design, guaranteeing quality of 
service (QoS), etc. This thesis is a part of a collaboration between King Mongkut’s 
Univers ity of Technology Thonburi and CERN which aims to examine the performance 
of various mathematical models in representing network traffic data by comparing time -
series models and distribution models.   
 
1.3 Research Objective  
Since huge amounts of data from FLPs will be continuously sent over a local area network  
to the Logstash server, packet losses in the network can adversely affect experimental 
result analysis due to incomplete data. Services running on FLPs may also generate log 
data with unanticipated t raffic characteristics. Our objective is to examine packet loss 
performance under varying conditions in terms of the number of FLPs and  characteristics of 
generated log traffic to determine conditions at which the network can sustain negligible 
or no data loss.   3 
 Due to restrictions from accessing the production facilities at CERN, a few numbers of 
FLPs are installed in an OpenStack testbed environment and traffic data generated by 
FLPs is collected by using PacketBeat. So, determining the distribution of n etwork  traffic 
data that collected from Packetbeat is significant. The traffic data is then fitted to  various 
traffic -source models that will be used to generate inputs to the simulation models.  
 
1.4 Research Scope  
1. Only one Logstash server was considered, which is the worst -case scenario of multi -
servers as traffic would be  concentrated to a single bottleneck switch in the network.  
2.  Due to flooding log traffic data from the FLPs to the Logstash server, only the losses 
that occur between the FLPs and the Logstash server are focused on.  
3. Using OpenStack -based  CERN Cloud infrastructure and Linux CentOS 7 images , all 
instances of FLPs were created with 4 VCPUs, 7.3 GB of memory, and 40 GB of disk 
space  
4. Variables  considered for performance m odeling are link capacities in the network, 
interarrival time, buffer size and fixed packet size.  
5. Multiple types of probability distributions are considered in the stochastic distribution 
model including the time -series model, exponential distribution, and Pareto distribution . 
 
1.5 Expected Benefits  
Because of resource limitations in the testbed environment, the number of nodes that can 
be created and simulated is limited to only a few nodes. Hence, the simulation  model needs 
to be developed with OMNET++  based on the collected data instead. In the  simulation, 
hundreds and thousands of FLPs nodes were built to simulate an environment that was 
similar to the production. The simulation model that can efficiently represent input  traffic 
data so that the suita ble packet buffer capacity and bandwidth can be found before being  
installed in the production will efficiently provide good quality of service (QoS).  Moreover, 
our results will help predict the sustainability regarding the number of FLPs running different  
services for system expansion and how much traffic intensity of log data generated by 
FLPs the system can support in the future .  
 CHAPTER 2  LITERATURE REVIEW AND THEORY  
The O2 environment and logging scheme, the study of traffic flow, and queuing analysis 
are three key topics that need to be recalled and clearly understood in order to perform 
network resource and loss estimation . 
 
2.1 O2 Environment and Logging Scheme  
The Alice  Online and Offline computing system (O2) conducts physical tests and collec ts 
results. Thus, O2 requires a software framework and a common computing facility for 
both data collection and processing. Th e O2 computing system consists of two categories of 
computing nodes as shown in Figure 2.1. Each one is responsible for different data 
collection. The first level processor (FLP) collects data from the detector at the rate of 3.2 
Gb/s per FLP node, or at rate 1.1 TB/s in total [3]. The stream data will then b e analyzed 
by the Event Processing Nodes (EPNs) for aggregation and will be used in experiments 
done by scientists. The O2 facility consists of 250 FLPs and 1500 EPNs. This research 
will focus on the FLP side and ignore the EPN side. The FLPs will be the p rimary target 
of the logging scheme. The log data from the FLPs will be compiled and sent to the log 
analysis tool for analysis and visualization . 
 
 
 
Figure 2.1 O2 environment  
5 
 2.2 Traffic Distribution  
The study of traffic distribution is vital for network analysis and resource estimation as it 
describes the characteristics of the network. Errors in estimation are caused when traff ic 
distribution is not analyzed carefully. The recent study of traffic mod els has shown that 
the self -similarity, long -range dependence, and burstiness properties of network traffic 
are crucial in the study of network traffic characteristics. Chandrasekaran [4] studied 
several explanations for common traffic stream models such a s Poisson, Pareto,  Markov, and 
embedded Markov Models in order to help understand the flow of traffic in the network 
as well as prove how the model closely represents the real -time characteristics of the 
network. For the network capacity model in social op portunistic networks, Soelistijant o and 
Howarth [5] assumed the Poisson distribution and node independence. The  simulation 
generated a network with 100 nodes and increased the node count until 1000 nodes are 
applied in order to find the load distribution f or both binary and weighted networks . 
 
In order to help the providers characterize network resource usage and improve the 
performance of network and infrastructure planning, Baris et al. [6] assessed the characteri stics 
of the flow size and volume of the monitoring system. They found that the sample 
distribution of flow size and length is processed and then sent to the monitor client part 
with deterministic interarrival time .  
 
Most research tends to study the sum of sources rather than focusing on the sub set of 
packets generated by a single source. Past research focused on the multiplex internet core  
metric, which does not consider details for individual flow. For the sum of independent 
traffic sources, Cerna et al. [7] assumed a normal distribution and us ed the central limit 
theorem to estimate maximum throughput of the transport link in a base station. An 
experiment of tree topological scenarios for a network based on 3, 6 , and 16 sources was 
conducted. Premaratne and Premarathne [8] studied network traff ic in backbone links as 
a sum of independent Bernoulli sources, which resulted in a Poisson binomial  distribution 
with a  skewed Gaussian distribution. Only few research was related to the study of the 
characteristics of traffic log data. Nguyen et al. [9] studied aggregated daily traffic logs 
dataset obtained between the local subnet part of the Ivanovao State University campus 
network and the internet service provider. Analysis of the aggregated traffic data  showed that 6 
 the inter -session time and session s ize distribution could effectively be described by the 
q-exponential distribution along with results from their previous research [10] .  
 
2.3 Queu ing Analysis  
Mathematical investigations of data loss in network telecommunication are important and 
can be accomplished by using queuing analysis. Heavy  tail traffic distributions such as 
Weibull, Pareto and Log -normal are used to model queues with non -stationary arrival 
rates. One disadvantage of analyzing queuing with heavy -tail traffic distribution is the 
tendency for considerable mathematical sophistication because many heavy  tail distributions 
do not have a specific formula, as stated by Rakesh et al. [11] .  
 
Ming et al. [12] studied the traffic distribution of their input traffic data by using hyper 
expone ntial distributions fitting technique combined with a matrix geometric solution 
approach to analyze the queue performance of Pareto/M/1/K. Sheng et al. [13] analyzed 
the waiting time distribution using the same technique as Ming et al. [12]. Araik and 
Mikh ali [14] studied the Weibull/M/1 and Weibull/Weibull/1 queues with single server 
queues. Moreover, comparison was used to assess the efficiency of the mean for the  waiting 
time and the mean of sojourn time for Weibull/M/1 with M/M/1 and Weibull/Weibull/1 
with M/M/1.  
 
For packet loss and packet delay, Xiaolong and Geyong [15] proposed an analytical 
model for a single server queuing system with the self -similarity input traffic and heavy -
tail packet size distribution. The developed model of the packet size da ta is based on two 
heavy -tail distributions, the Log -normal and Pareto. Strelkovskaya and  Solovskaya [16] 
proposed a mathematical estimation of probabilities and time characteristics of QoS for 
multiservice video traffic of G/M/1 queuing system with Weibul l arrival packet flow 
distribution.  
 
Several researches have studied the ON/OFF process in queuing. An ON/OFF process is 
the state which reflects active/inactive behavior of the network. It is adopted in various 
settings and influences the performance of t he queue. Jian and Kevin [17] modeled the 
ON/OFF source traffic with Pareto and exponential distributions. This method was  relevant 7 
 since Jian and Kevin (2006), Mohsen H. (2017) [18] also described the realistic burst 
traffic data by modeling ON/OFF source  traffic with the Pareto distribution . 
 
A majority of research tends to study the M/M/1 queue system, which considers the 
Poisson arrival process and exponential service time such as Kadir [19]. Kayvan et al. 
[20] also studied the M/M/1 system. For an adva nced technique for queuing, Van and Son 
[21] applied the queuing analysis with the wireless sensor network via the M/M/1 and 
M/M/1/K in order to evaluate the optimal service rate and buffer size. Moreover, Guo et 
al. [22] applied queuing with the base stat ion in the mobile edge computing (MEC) 
service in order to find the optimal amount of communication and resources to guarantee 
the QoS for all users at a minimal total expenditure. One challenge in this research was 
determining the right service time distr ibution and they simplified it by assuming that it 
followed the exponential distribution. In conclusion, the queuing model for a base station 
was modeled as M/M/n/  and the priority level was also involved. This is similar to the 
work of Guo et al. [23] . 
 
From the related research mentioned previously, there are many types of traffic distribution 
that can be used to find the best fitted distribution among different network data and model 
the queuing analyze model. Traffic distribution can be divided into tw o commonly used 
types, non  heavy tailed traffic distributions and heavy  tail traffic distributions. The non  heavy 
tailed traffic distribution, known as the traditional approach, is the most popular because of 
ease in terms of understanding and mathematical  calculation. However, network traffic is 
sometimes difficult to interpret, so more complicated distributions such as the heavy -tailed 
traffic distribution needs to be used to analyze the traffic instead . 
 
For the topological analyses of queuing with multi ple queues, Le et al. [24] proposed the 
tandem queue in multihop wireless network which is comparable to the exact  method and 
decomposition method. Two scenarios were performed: for two queue cases and more 
than two queue cases. Loss probability and averag e delay are measured. At that moment, 
the result showed that the decomposition method got precise performance measures with 
lower computational complexity. Besides wireless networks, Kattepur and Nambiar [25] 
also applied the queuing network with multi -tiered web applications and developed a 
Mean Value Analysis (MVA) for performance analysis in order to test for high -availability 
and varying concurrency. In queuing network models for  performance testing, each of the 8 
 load generating server, web/application s erver and database server will monitor the 
following metrics which are CPU, Disk, and Network. Each server will have individual 
queues for CPU, Disk and Network. The prediction technique can precisely predict the 
mean deviations seen for throughput (pages/  second) and Response Time .  
 
Furthermore, in the complex communication system, Yang and Shan [26] simplified the 
analysis as tandem open queuing network which consisted of multiple Geo/Geo/1  clients 
and a server with a batch service queue. The study of av erage delay and overflow  
probability are validated by the analytical model and simulations. The study reported that 
the result from the analytical solution and simulations has no significant difference in 
most cases. This suggests that the analyses of queu ing networks can be done through 
simulations for more straightforward explanations .  
 
Several factors (e.g. the large number of sources or servers) and systematic complexity 
could lead to complications in the queuing model. Simulation research simplifies the 
process. Simulations help in deriving the significant decisions for the system deci sion 
making and solves the complication problem efficiently. In order to obtain delay,  Tickoo 
and Sikdar [27] applied G/G/1 queue by using Ns -2 simulation, which can be used to 
simulate different network topologies, the number of nodes, as well as the load  on the 
network. In this work, 10 and 20 source nodes with a packet size of 1000 bytes was  used as 
the setting. Comparison of the simulation result with the analytical result revealed a close 
match.  
 
For the moderate load cases, the difference between analytical and simulation results is 
acceptable because simulation helps to reduce computational complexity . PalunčIć et al. 
[28] mentioned that recent research concerning the queuing analysis of Cognitive Radio 
Network (CRNS) has been extremely complex in  order to characterize the delay,  throughput 
and other performance metrics, thus giving insight about resource allocation, medium 
access control and QoS provisioning. In addition to the banking system case study,  
Ehsanifar et al. [29] studied queuing using  Arena simulation software, which showed that 
the interaction between customers and servers in models such as the M/M/C model is 
similar to Ghaleb et al. [30]. Ghaleb et al. can also be modeled using Arena  simulation 
software to rank and select the best al ternative to use in industrial engine ering and 
operations management.  9 
  
Unlike previous works which mostly characterize aggregated traffic in backbone links 
and single -queue analysis, this research studies the characteristics of logged data traffic 
generate d by FLPs running different services and investigates the end -to-end packet loss 
performance in a network of switches under such input traffic. We found that the packet 
interarrival times are correlated and cannot be captured by traffic models that assume 
independence.  Packet loss performance under a large number of FLPs feeding traffic to 
a network of switches are evaluated under both time -series traffic models and  independence 
traffic models as benchmarks to identify any performance inconsistency.  
 
More over, we also prove that the simulation result and analytical result are not too 
different in most cases. Our results will help predict the sustainable number of FLPs 
running different services and traffic intensity of log data generated by FLPs for future  
system expansion or scaling.   
  
 CHAPTER 3  METHODOLOGY  
We aim to determine which traffic models can represent network traffic data generated 
from FLPs well and use the models to evaluate the packet loss performance. We first 
describe the Open Stack testbed environment, data acquisition from FLPs installed in the 
testbed, data preprocessing, and their statistical properties. The steps to analyze and fit 
the data to traffic models are then presented. At the end of the chapter, the evaluation 
metrics for comparing various results are presented.  
 
3.1 FLP Log Data Acquisition  
Using the OpenStack -based CERN Cloud infrastructure and Linux CentOS 7 images, all 
instances of FLPs were created with 4 VCPUs, 7.3 GB of memory, and 40 GB of disk 
space. Each F LP is installed with the FLP Suite, containing many sets of tools that will 
be used for detector readout and quality control. Figure 3.1 shows the logging system 
architecture. Two types of services are installed on every FLP, Filebeat, and Packetbeat. 
The Filebeat monitors the log files and collects log events from the agent while  Packetbeat 
monitors the outbound network traffic of log data from FLPs Logstash server. After  filtering 
the data, the remaining data will be sent to Elasticsearch server and Kibana server for 
further analysis. From the logging system architecture, the loss normally occurs  between 
the FLPs and the Logstash due to the flooding log traffic data from the FLPs.   
 
We assume a single Logstash server in the system which is the worst -case scenario for 
multi -servers as all traffic is concentrated to a single bottleneck switch in the network. 
The collected dataset consists of the single workflow scenario whe re each FLP runs one 
workflow only. The workflow is a set of applications or a service which will be used for 
readout functionality and quality control for the detector. This research analyzes the 
network log, which was collected from a Packetbeat on the F LP. Each log transaction 
contains the timestamp, total bytes transferred, and agent name.  
 11 
  
 
Figure 3.1 Logging system architecture in the open Stack testbed environment  
 
3.2 Framework Design   
The first step  to estimate the packet loss through simulation is to find traffic models which 
can well represent the network traffic data generated from FLPs for the system. The time -
series model and the distribution model are the two key models that we believe could 
represent trace data. The simulation based on the modeled topology will be tested after 
fitting trace data with those models in order to find the model representation. Finally, the 
end-to-end loss, queue utilization, and 95% confidence interval of end -to-end loss will be 
used to assess how well the model compares to the trace data. The representation of 
network traffic data will be completed at the end  
12 
  
 
Figure 3.2 A diagram of overall framework process  
 
Different workflows might run on FLPs, depending on the required operational activities. 
Each FLP is expected to run only a single workflow at a time. The more complex the 
workflow, the more logs will be generated. Because of the resource limitation in the  testbed 
environment, only a few number of FLPs can be created and emulated for traffic generation. 
Four workflows will be considered in this study: readout -stfb-qc, readout -stfb, readout -qc, 
and readout. These workflows are mainly used for reading out and  monitoring the quality of 
the data.  
 
3.3 Data Preprocessing  
The data preprocessing section starts with data cleaning by converting the types of 
collected  data to the appropriate form and unit such as converting both date and time in to 
YYYY/MM/DD HH:MM:SS format. Then, the same timestamp data will be  aggregated 
and measured for the interarrival time of each transaction. Some outliers and  deterministic 
13 
 values in the dataset like the keep alive packet and ping packet will then be deleted. Table  
3.1 shows a ru ndown of data preprocessing.  
 
Table 3.1 Data preprocessing summary  
 
Task  
(Records)  Workflow  
readout -stfb-qc readout -stfb readout -qc readout  
1. Original data  1512  919 949 3477  
2. Group under the same 
timestamp.  1404  835 899 3193  
3. Remove Interarrival time which  
more than 30,000 and some 
deterministic value  in order to 
find the real traffic distribution.  934 625 568 3193  
4. Cleansed data  934 625 568 2532  
 
Finally, the data will be concatenated into cleansed data. The summary statistics of  packet 
interarrival times collected from the four workflows are shown in Table 3.2.  
 
Table 3.2 Interarrival time statistic summary  
 
Stat Summary  Workflow  
readout -stfb-qc readout -stfb readout -qc readout  
Duration (minute)  22.05  88.86  67.51  132.82  
Counts  260 625 200 600 
Mean (millisecond)  4869.76  5724.39  5254.70  7413.99  
Standard deviation  
(millisecond)  5110.14  5726.56  5175.69  6344.55  
Min (millisecond)  7 15 35 7 
25% (millisecond)  1262.75  1033.0  1480.0  1364.75  
50% (millisecond)  2958.50  3634.0  3187.50  6063.50  
75% (millisecond)  6968.50  9077.0  9035.25  12212.75  
Max (millisecond)  21522.00  27134.0  26887.0  28239.0  14 
 Figures 3.3 -3.18 show the interarrival time distribution and packet size distribution of all 
workflows. According to the interarrival time distribution, the majority of the  interarrival times 
of all workflows appears to fall between 0 and 5 seconds. According to the  network byte 
distribution, the bulk of all workflow packet sizes tend to be about 30 bytes and 300 bytes.  
 
 
 
Figure 3.3 The interarrival time distribution of readout -stfb-qc workflow  
 
 
 
Figure 3.4 The packet size distribution of readout -stfb-qc workflow  
 
15 
  
 
Figure 3.5 The interarrival time distribution and packet size distribution of readout -stfb 
workflow  
 
 
 
Figure 3.6 The interarrival time distribution of packet size about 300 bytes of readout -
stfb-qc workflow  
 
 
 
Figure 3.7 The interarrival time distribution of readout -stfb workflow  
16 
  
 
Figure 3.8 The packet size distribution of readout -stfb workflow  
 
 
 
Figure 3.9 The interarrival time  distribution of packet size about 30 bytes of readout -stfb 
workflow  
 
 
 
Figure 3.10 The interarrival time distribution of packet size about 300 bytes of readout -
stfb workflow  
17 
  
 
Figure 3.11 The interarrival time distribution of readout -qc workflow  
 
 
 
Figure 3.12 The packet size distribution of readout -qc workflow  
 
 
 
Figure 3.13 The interarrival time distribution of packet size about 30 bytes of readout -
qc workflow  
18 
  
 
Figure 3.14 The interarrival time distribution of packet size about 300 bytes of readout -
qc workflow  
 
 
Figure 3.15 The interarrival time distribution of readout workflow  
 
 
 
Figure 3.16 The packet size distribution of readout workflow  
 
19 
  
 
Figure 3.17 The interarrival time distribution of packet size equal 30 bytes of readout 
workflow  
 
 
 
Figure 3.18 The interarrival time distribution of packet size equal 300 bytes of readout 
workflow  
 
3.4 Data Analysis  
The study of aggregated interarrival time is considered in the data analysis discussion in 
order to find the recommended bandwidth, which is determined from the maximum 
aggregated bytes range separated by the time interval. Then, in order to check that the 
interarrival period has a connection with itself, autocorrelation in the interarrival 
distribution will be examined. Finally, the time -series model was used to find the fitting 
outcomes of ea ch workflow.  
 
 
20 
 3.4.1  Study of Aggregated Interarrival Time  
Since the aggregated interval time based on 5 milliseconds is the finest unit, it will be 
selected as the suitable bandwidth for production. As shown in Table 3.3, the aggregated  
byte and the recom mended bandwidth was selected base d on modal  value s. The  recommended 
bandwidth was calculated from maximum aggregated bytes range divided by the time 
interval. The recommended bandwidth for readout -stfb, readout -qc, and readout  workflows 
is approximately 500 Kbps at the finest unit.  
 
Table 3.3 The recommended bandwidth  
 
Task  Aggregated 
timestamp  Bin range  Recommended Bandwidth  
(bit per sec)  
readout -stfb-qc 5 ms  16.00 - 27.84  25,600.00 – 44,544.00  
100 ms  288.32 - 300.16  23,065.60 – 24,012.80  
500 ms  284.60 - 300.40  4,553.60 – 4,806.40  
1 s 284.60 - 300.40  2,276.80 – 2,403.20  
readout -stfb 5 ms  290.40 - 300.20  464,640.00 – 480,320.00  
100 ms  294.30 - 306.40  23,544.00 – 24,512.00  
500 ms  287.20 - 305.28  4,595.20 – 4,884.48  
1 s 287.20 - 305.28  2,297.60 – 2,442.24  
readout -qc 5 ms  298.00 - 304.00  476,800.00 – 486,400.00  
100 ms  298.08 - 304.96  23,846.40 – 24,396.80  
500 ms  298.08 - 304.96  4,769.28 – 4,879.36  
1 s 98.08 - 304.96  2,384.64 – 2,439.68  
readout  5 ms  294.30 - 306.40  470,880.00 – 490,240.00  
100 ms  294.30 - 306.40  23,544.00 – 2,4512.00  
500 ms  286.98 - 302.92  4,591.68 – 4,846.72  
1 s 287.20 - 305.28  2,297.60 – 2,442.24  
 
3.4.2  ACF and PACF plots  
The interarrival times of each task are examined for their correlation by plotting the 
autocorrelation function (ACF) and partial autocorrelation function (PACF). The ACF 
and PACF plots can be used to analyzed and specify values for the seasonal  model by 21 
 examining correlations at seasonal lag time steps. The chart of the autocorrelation plot 
and partial autocorrelation plot will be studied in order to determine the  randomness of 
interarrival time. If the interarrival time is not random, the lag value needs to be 
determined. For the readout -stfb-qc, readout -stfb, and readout -qc, only parts of data are 
selected in the analysis . 
 
3.4.3  Time -Series Model Fitting  
To begin, the Dickey -Fuller Test must be performed to confirm the patterns and seasonal 
effects. Sinc e substantial lags in the plots indicate that there is a connection in the  interarrival 
periods, a time -series model such as autoregressive moving average (ARMA) is more 
suitable than simply fitting with a proper probability distribution, even though a fit  would be 
a more convenient choice in terms of simulations. If large lags in the plots indicate a 
seasonal connection in the interarrival periods, a time -series model like the seasonal 
autoregressive integrated moving average (SARIMA) will be chosen.  
 
1. Dickey -Fuller Test  
The Dickey -Fuller test is a common statistical test used to determine whether a given 
time-series is stationary or not. Being stationary is a significant factor in time -series 
which refers to the data having no trend. Normal statistical analysis is incorporates  
hypothesis testing that involves a null and alternate hypothesis, so a test statistic is 
computed. The criteria of the Dickey -Fuller test are focused on p -values.  
 
2. ARMA Model  
The ARMA model is a popular time -series model which c an be characterized by two 
terms which are the order of the AR term (p), the order of the MA term (q) as shown in 
Eq.(3.1) where  Yt  is the data at time t, εt  is an error at time t, βt  is a coefficient of data 
at time t, ϕt  is a coefficient of error at time t and α  is a constant.  
 
Yt= +β1Yt-1+β2Yt-2+..+βpYt-p+ϕ1εt-1+ϕ2εt-2+..+ϕqεt-q (3.1)  
 
3. SARIMA Model  
SARIMA is a well -known extension of ARIMA that supports the seasonal component. 
There are four seasonal elements that a re not part of ARIMA that need to be considered 22 
 which are the seasonal autoregressive order (p), the seasonal difference order (d), the 
seasonal moving average order(q) , and the number of time steps for a single seasonal 
period (s).  
 
3.5 Experimental Scena rios 
After traffic has been fitted to appropriate traffic source models, the models would be 
used to generate 1024 -byte packets to evaluate the end -to-end packet loss in a two -layer  
network of  switches with a large number of FLPs feeding traffic to the net work in the 
topology. The  variables considered for performance modeling are link capacities in the 
network, interarrival time distribution, buffer size , and packet size.  
 
The simulation model represents an interconnection of Ethernet switches connected in a 
tree topology, where each switch has 48 1 GB ports and the output port buffer size is set 
to 0.5 MB based on a commercial medium -size switch. The lowest layer of switches 
connects FLPs and traffic is aggregated to the top switch connecting to the Logstas h 
server, which is the bottleneck switch in the network. Three simulation scenarios were 
created, all of which are similar except for the number of first -layer network switches and 
the number of each FLP. The number of FLPs chosen is determined by the netw ork switch 
input and future system expansion.  
OMNET++, a discrete -event simulation tool, is used for the simulation. Each task 
simulation scenario is repeated for five runs. The average of packet losses with their 
95% confidence intervals and the bottlenec k link utilization are computed. In our 
experiments, the interarrival times will be scaled to saturate the link utilization of the 
top switch to investigate how the packet loss will increase as FLPs generate traffic at 
higher rates. OMNET++ can generate nu mbers based on various built -in probability 
distribution modules but not a time series process like ARMA.   
 
Furthermore, the log is also fitted to exponential and Pareto distributions. These distributions 
represent smooth and highly bursty traffic, respec tively, to benchmark the loss performance. 
In order to obtain a result and compare it to the selected model, the trace data will be tested 
in each scenario.   
 
 23 
 3.5.1  First Scenario: Logstash with 192 FLPs  
In the first scenario, each switch's first -layer ne twork is connected with 48 FLPs, implying 
that the switches of the first -layer network would be fully connected with 192 FLPs. Then, 
as shown in Figure 3. 19, the second -layer switch network is connected to the Logstash server. 
The first scenario parameter f or simulation is listed in Table 3.4.  
 
 
 
Figure 3.19 The first scenario network topology  
 
Table 3.4 Simulation  parameter of the first scenario  
 
Parameter  Value  
Number of FLPs  192 
Default mean interarrival time of readout -stfb-qc. (ARMA, Pareto and 
Exponential)  3.7 
seconds  
Default mean interarrival time of readout -stfb. (ARMA, Pareto and 
Exponential)  4.6 seconds.  
Default mean interarrival time of readout -qc. (ARMA, Pareto and 
Exponential)  4 
seconds.  
Default mean interarrival time of readout.  
(ARMA, Pareto and Exponential)  6.1 
seconds.  
Packet Size (Byte)  1024  
Switch Packet Buffer Size (MB)  
Base on HPE 1620 -48G switch  0.5 
Bandwidth (Mbps)  1 
 
 
 
24 
 3.5.2  Second Scenario: Logstash with 384 FLPs  
Each first -layer network of switch is connected with 48 FLPs, similarly to the first scenario, 
which means that the first -layer network of switches would be fully connected with 384 FLPs. 
Then, as shown in Figure 3.20, the second -layer switch network is connected to the Logstash 
server. The second scenario parameter for simulation is listed in Table 3.5  
 
 
 
Figure 3.20 The second scenario network topology  
 
Table 3.5 Simulation  parameter of the second scenario  
 
Parameter  Value  
Number of FLPs  384 
Default mean interarrival time of readout -stfb-qc. (ARMA, Pareto and 
Exponential)  3.7  
seconds  
Default mean interarrival time of readout -stfb. (ARMA, Pareto and 
Exponential)  4.6  
seconds.  
Default mean interarrival time of readout -qc. (ARMA, Pareto and 
Exponential)  4  
seconds.  
Default mean interarrival time of re adout. (ARMA, Pareto and 
Exponential)  6.1  
seconds.  
Packet Size (Byte)  1024  
Switch Packet Buffer Size (MB)  
Base on HPE 1620 -48G switch  0.5 
Bandwidth (Mbps)  1 
25 
 3.5.3  Third Scenario: Logstash with 2,304 FLPs  
To estimate the system's upper bound, the FLPs node and first -layer network switch are 
increased in this case. The number of first -layer network switches has been increased to 
48 in order to investigate the upper bound sustainability of the network. The first -layer 
network of each switch is also connected with 2,304 FLPs. Then, as shown in Figure 3. 21, 
the second -layer switch network is connected to the Logstash server. Table 3.6 lists the 
second scenario parameter for simulation.  
 
 
 
Figure 3.21 The third scenario network topology  
 
Table 3.6 Simulation  parameter of the third topology  
 
Parameter  Value  
Number of FLPs  2,304  
Default mean interarrival time of readout -stfb-qc. (ARMA, Pareto and 
Exponential)  4.0 
seconds  
Default mean interarrival time of readout -stfb. (ARMA, Pareto and 
Exponential)  4.6 
seconds.  
Default mean interarrival time of readout -qc. (ARMA, Pareto and 
Exponential)  4.4 
seconds.  
Default mean interarrival time of readout.  
(ARMA, Pareto and Exponential)  6.4 
seconds.  
 
26 
 Table 3.6  Simulation parameter of the third topology  (cont.)  
 
Parameter  Value  
Packet Size (Byte)  1024  
Switch Packet Buffer Size (MB)  
Base on HPE 1620 -48G switch  0.5 
Bandwidth (Mbps)  1  
 
3.6 Evaluation  
In this research, the following evaluators will be used to assess  the time -series model and 
the stochastic distribution model.  
 
1. End-to-end Loss is the metric that would be used to measure the loss of a packet that is 
transmitted across a network from source to destination. The less the end -to-end loss, the 
better the choice. The calculation for end -to-end loss is shown in Eq. (3.2) .   
 
End-to-end loss  =  Number  of packet dropped
Source generated packet -All packets currently in queues (3.2) 
 
2. Queue Utilization is the  metric that  would be  used to measure the congestion of the 
queue. High utilization means the network is overloaded, w hile low utilization means 
the queue is not busy. The formula is shown in Eq. (3.3).  
 
Queue Utilization  = Packet size* 8* Number of FLPs
Mean Interarrival time*Link Bandwidth (3.3)  
 
3. 95% Confidence Interval of end-to-end loss  Loss is a metric that is used  to measure 
the true mean value of end -to-end loss. The formula is shown in Eq . (3.4).  
 
95% Confidence Interval  
of End -to-end loss  = Mean End -to-end loss ± Z*Standard Deviaiton of End -to-end loss
√Number of simulation running  (3.4)  
  
 CHAPTER 4  EVALUATION RESULTS  
In this chapter, we first describe the correlation structure in interarrival times, Dickey -
Fuller test of interarrival times and time -series model fitting. At the end of the chapter, 
the distribution result and trace data result are shown. To find the best represented  model 
compared to the real trace result, the model verification must be considered and  performed 
linear regression on the number of FLPs and end -to-end loss to guide the number of FLPs 
that the netw ork can support for various types of tasks operating on FLPs.  
 
4.1 Correlation Structure in Interarrival Times   
The autocorrelation function and partial autocorrelation function plots of the tasks  invalidates 
the independence assumptions of interarrival ti mes for log data generated from the  tasks. The 
significant values of the first few lags in the autocorrelation function plots indicate that 
the interarrival times are dependent and modeling them by just fitting a distribution could 
lead to inaccurate packe t loss in the simulation.   
 
The time series graph of the readout stfb -qc workflow in Figure 4.1 shows that the lag appears 
to have a seasonal pattern, as shown in the autocorrelation plot. There is no pattern in the other 
workflows.  
 
 
 
Figure 4.1 Time series plot of readout -stfb-qc workflow  
28 
  
 
Figure 4.2 ACF plot of readout -stfb-qc workflow  
 
 
 
Figure 4.3 PACF plot of readout -stfb-qc workflow  
 
 
 
Figure 4.4 Time series plot of readout -stfb workflows  
 
29 
  
 
Figure 4.5 ACF plot of readout -stfb workflows  
 
 
 
Figure 4.6 PACF plot of readout -stfb workflow  
 
 
 
Figure 4.7 Time series plot of readout -qc workflows  
 
30 
  
 
Figure 4.8 ACF plot of readout -qc workflows  
 
 
 
Figure 4.9 PACF plot of readout -qc workflows  
 
 
 
Figure 4.10 Time series plot of readout workflows  
31 
  
 
Figure 4.11 ACF plot of readout workflows  
 
 
 
Figure 4.12 PACF plot of readout workflows  
 
4.2 Dickey -Fuller Test of Interarrival Times   
To assess whether each workflow is stationary, the Dickey -Fuller test needs to be 
evaluated. In accordance to the Dickey -Fuller test, the null hypothesis would be defined 
as the presence of a unit root. The p -values from the test shown in Table 4.1 are all less 
than 0.05, implying that all task data ex cept readout -stfb-qc task are stationary. As a 
result, both a stationary and non -stationary time -series model will be examined. In the 
readout -stfb-qc workflow, the stationary time -series will be taken into account. Other 
workflows will take into account t he non -stationary time -series model.  
 
 
 
 
32 
 Table 4.1 Dicke y fuller test  
 
Workflow  Dickey Fuller Test  
ADF statistic  P-value  
readout -stfb-qc -1.125664  0.236449  
readout -stfb -5.630379  0.000001  
readout -qc -4.793392  0.034988  
readout  -4.952048  0.000028  
 
4.3 Time -Series Model Fitting  
In order to identify the best combination of parameters, the AIC (Akaike Information 
Criterion) values are used for the model selection. The range of p and q orders are fitted 
using statsmodels library in Python and the appropriate model parameters are selected 
from those with relatively small AICs and orders such that all the fitted coefficients are 
significa nt. Table 4.2 shows the fitted model orders corresponding to minimum and 
maximum AICs and that of the selected model orders.  
 
Comparing all the AIC values along with significant coefficients, selection based on the 
criteria resulted in the lower AIC value s with less order parameters. Thus, it can be  concluded 
that the suitable order is SARIMA (2, 0, 3) (3,0,1,30) for readout -stfb-qc workflow, 
ARMA (0,0,2) for readout -stfb workflow, ARMA (3,0,2) for readout -qc workflow, and 
ARMA (2,0,0) for readout workflow .  
 
Table 4.2 The summary of fitting parameter  
 
Workflow  Aggregated Information  
Order: Min AIC  Order: Max AIC  Selected Order: 
Selected AIC  
readout -stfb-qc (5, 0, 5)(4, 0, 5, 30): 
2,059.78  (1, 0,5)(1,0,0,30):  
4,496.10  (2, 0, 3)(3,0,1,30):  
3307.71  
readout -stfb (7,0,0):12,340.52  (0,0,0):12,572.97  (0,0,2):12,424.67  
readout -qc (3,0,5):3,859.85  (0,0,0):3,971.88  (3,0,2):3,917.83  
readout  (7,0,0):11,969.76  (0,0,0):12,191.73  (2,0,0):12,083.80  33 
 4.4 Distribution Results  
The packet losses for different mean interarrival times, the number of FLPs, and traffic 
source models fitted from the data was considered. Time -series models, such as ARMA, 
and probabilistic distributions, such as Pareto and exponenti al distributions, were chosen. 
Each workflow simulation scenario was repeated for five runs to calculate average packet 
losses and bottleneck link utilization. The mean interarrival times are reduced to increase 
the top switch queue utilization while prese rving the main statistical properties of the 
traffic data, allowing for the observation of packet losses.  
 
4.4.1  First Scenario: Logstash with 192 FLPs  
Tables 4.3 to 4.6 demonstrate that the system can easily handle 192 FLPs with nearly no 
packe t loss. The  loss was less than 1% from beginning to end. When the traffic intensity 
per FLP increases by about three times and the link utilization at the bottleneck reaches 
over 95 percent, end -to-end packet loss appears. The end -to-end packet loss for  exponential 
and Pareto distribution traffic models is slightly higher than for time -series models . 
 
Table 4.3 Packet loss with readout -stfb-qc task input generated from 192 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of  
End-to-end loss  
3693.3  ARMA  0 0.0004  (0,0)  
Pareto  0 0.0004  (0,0)  
Exponential  0 0.0004  (0,0)  
1.8 ARMA  0.02 0.7903  (0.00,0.04)  
Pareto  0.01 0.7929  (-0.02,0.05)  
Exponential  0.01 0.7924  (0.00,0.03)  
1.6 ARMA  0.55 0.8878  (0.52,0.57)  
Pareto  1.59 0.8916  (1.56,1.62)  
Exponential  0.45 0.8905  (0.40,0.49)  
1.5 ARMA  2.61 0.9823  (2.55,2.66)  
Pareto  5.75 0.9997  (5.67,5.84)  
Exponential  3.77 0.9885  (3.62,3.91)  
 34 
 Table 4.4 Packet loss with readout -stfb task input generated f rom 192 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4560.1  ARMA  0 0.0003  (0,0)  
Pareto  0 0.0003  (0,0)  
Exponential  0 0.0003  (0,0)  
2.3 ARMA  0 0.6383  (0,0)  
Pareto  0 0.6528  (0,0)  
Exponential  0.08 0.6366  (0.06,0.11)  
1.8 ARMA  0.12 0.7981  (0.10,0.14)  
Pareto  0.19 0.8162  (0.13,0.24)  
Exponential  0.91 0.7970  (0.81,1.00)  
1.5 ARMA  4.11 0.9652  (4.05,4.17)  
Pareto  6.55 0.9765  (6.45,6.65)  
Exponential  5.18 0.9592  (4.98,5.38)  
 
Table 4.5 Packet loss with readout -qc task input generated  from 192 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
3930.4  ARMA  0 0.0004  (0,0)  
Pareto  0 0.0004  (0,0)  
Exponential  0 0.0004  (0,0)  
2.0 ARMA  0.10 0.7517  (0.08,0.13)  
Pareto  0 0.7414  (0,0)  
Exponential  0 0.7432  (0,0)  
1.7 ARMA  0.73 0.8458  (0.70,0.76)  
Pareto  0 0.8341  (0,0)  
Exponential  0.02 0.8361  (0.00,0.05)  
1.5 ARMA  3.05 0.9783  (2.93,3.17)  
Pareto  2.15 0.9645  (2.06,2.24)  
Exponential  3.16 0.9665  (2.98,3.35)  35 
 Table 4.6 Packet loss with readout task input generated from 192 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
6119.2  ARMA  0 0.0002  (0,0)  
Pareto  0 0.0002  (0,0)  
Exponential  0 0.0002  (0,0)  
2.4 ARMA  0.12 0.5910  (0.08,0.15)  
Pareto  0 0.6055  (0,0)  
Exponential  0.13 0.5987  (0.08,0.18)  
1.9 ARMA  1.18 0.7700  (1.06,1.30)  
Pareto  1.09 0.7865  (1.05,1.14)  
Exponential  2.26 0.7722  (2.15,2.37)  
1.5 ARMA  6.17 0.9608  (6.03,6.30)  
Pareto  9.39 0.9905  (9.28,9.50)  
Exponential  7.96 0.9640  (7.86,8.05)  
 
4.4.2  Second Scenario: Logstash with 384 FLPs  
Tables 4.7 to 4.10 also show that the system can easily handle 384 FLPs without losing 
any packets. The loss was also less than 1% from beginning to end. When the link  
utilization at the bottleneck exceeds 95%, the end -to-end packet loss begins to rise. End -
to-end packet loss was also higher with exponential and Pareto distribution traffic models 
than with time -series models . 
 
Table 4.7 Packe t loss with readout -stfb-qc task input generated from 384 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
3725.8  ARMA  0 0.0008  (0,0)  
Pareto  0 0.0008  (0,0)  
Exponential  0 0.0008  (0,0)  
7.4 ARMA  0 0.3898  (0,0)  
Pareto  0 0.3946  (0,0)  
Exponential  0.20 0.3952  (0.18,0.22)  36 
 Table 4.7  Packet  loss with readout -stfb-qc task input generated fr om 384 FLPs (cont.)  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
3.7 ARMA  0.41 0.7787  (0.37,0.46)  
Pareto  0.26 0.7893  (0.18,0.34)  
Exponential  0.75 0.7910  (0.71,0.79)  
3.0 ARMA  3.65 0.9672  (3.57,3.72)  
Pareto  7.67 0.9925  (7.51,7.84)  
Exponential  6.79 0.9811  (6.66,6.92)  
 
Table 4.8 Packet loss with readout -stfb task input generated  from 384 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4556.6  ARMA  0 0.0006  (0,0)  
Pareto  0 0.0006  (0,0)  
Exponential  0 0.0006  (0,0)  
4.6 ARMA  0.35 0.6421  (0.29,0.41)  
Pareto  0.02 0.6427  (0.00,0.03)  
Exponential  1.04 0.6446  (0.96,1.13)  
3.6 ARMA  1.91 0.8013  (1.83,2.00)  
Pareto  1.27 0.8043  (1.19,1.35)  
Exponential  3.89 0.8132  (3.69,4.08)  
3.0 ARMA  7.08 0.9659  (6.90,7.25)  
Pareto  8.87 0.9535  (8.72,9.02)  
Exponential  10.01  0.9735  (9.81,10.21)  
 
 
  37 
 Table 4.9 Packe t loss with readout -qc task input generated  from 384 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4016.3  ARMA  0 0.0007  (0,0)  
Pareto  0 0.0007  (0,0)  
Exponential  0 0.0007  (0,0)  
8.0 ARMA  0 0.3661  (0.00,0.01)  
Pareto  0 0.3630  (0,0)  
Exponential  0 0.3651  (0,0)  
4.0 ARMA  0.68 0.7325  (0.61,0.76)  
Pareto  0 0.7260  (0,0)  
Exponential  0.19 0.7304  (0.16,0.23)  
3.0 ARMA  5.32 0.9773  (5.24,5.41)  
Pareto  4.35 0.9684  (4.28,4.43)  
Exponential  5.15 0.9694  (5.03,5.28)  
 
Table 4.10 Packet loss with readout task input generated  from 384 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
6150.4  ARMA  0 0.0005  (0,0)  
Pareto  0 0.0005  (0,0)  
Exponential  0 0.0005  (0,0)  
6.1 ARMA  0.41 0.4756  (0.34,0.49)  
Pareto  0 0.4683  (0,0)  
Exponential  0.30 0.4851  (0.27,0.33)  
4.1 ARMA  1.99 0.7097  (1.90,2.08)  
Pareto  1.32 0.7020  (1.25,1.39)  
Exponential  4.18 0.7316  (4.11,4.24)  
3.0 ARMA  9.13 0.9423  (8.96,9.30)  
Pareto  12.26  0.9675  (12.20,12.33)  
Exponential  11.38  0.9850  (11.27,11.49)  38 
 4.4.3  Third Scenario: Logstash with 2,304 FLPs  
Tables 4.11 to 4.14 also show packet loss, indicating that the system is able to sustain 
2,304 FLPs. Before scaling down, the default mean interarrival time suffered an end -to-
end loss of about 1%, which is acceptable. After scaling down, the scaled mean  interarrival 
time suffered an end -to-end loss more than 5%. When the link utilization at the  bottleneck 
exceeds 50%, end-to-end packet loss begins to appear. Furthermore, with exponential and 
Pareto distribution traffic models, the end -to-end packet loss was higher than with time-
series models. As a result, it can be concluded that a 0.5 MB packet buffer is  insufficient for a 
thousand FLPs when the interarrival time is scaled down . 
 
Table 4.11 Packet loss with readout -stfb-qc task input generated from 2,304 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
3965.4  ARMA  0.79 0.0044  (0.75,0.83)  
Pareto  0.81 0.0044  (0.68,0.95)  
Exponential  0.79 0.0045  (0.72,0.85)  
32.9 ARMA  9.37 0.5204  (4.52,14.22)  
Pareto  10.31  0.5366  (9.51,11.12)  
Exponential  10.59  0.5467  (9.71,11.46)  
23.9 ARMA  13.14  0.7211  (8.80,17.48)  
Pareto  16.05  0.7347  (14.66,17.44)  
Exponential  16.02  0.7484  (15.38,16.66)  
17.8 ARMA  21.56  0.9750  (15.91,27.21)  
Pareto  21.59  0.9883  (19.81,23.37)  
Exponential  20.93  0.9954  (20.07,21.79)  
 
  39 
 Table 4.12 Packet loss with readout -stfb task input generated fro m 2,304 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
4686.9  ARMA  1.21 0.0037  (1.18,1.24)  
Pareto  1.20 0.0037  (1.18,1.23)  
Exponential  1.20 0.0037  (1.19,1.22)  
25.7 ARMA  18.76  0.6645  (16.49,21.03)  
Pareto  19.76  0.6812  (18.39,21.13)  
Exponential  19.01  0.7032  (18.73,19.29)  
22.5 ARMA  21.63  0.7937  (18.89,24.37)  
Pareto  22.55  0.7958  (21.16,23.94)  
Exponential  22.11  0.8026  (21.77,22.45)  
18.8 ARMA  25.39  0.9575  (23.41,27.37)  
Pareto  27.19  0.9385  (25.76,28.62)  
Exponential  27.41  0.9438  (27.07,27.75)  
 
Table 4.13 Packet  loss with readout -qc task input generated fro m 2,304 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
4504.7  ARMA  0.75 0.0039  (0.71,0.78)  
Pareto  0.75 0.0040  (0.70,0.80)  
Exponential  0.75 0.0039  (0.70,0.80)  
28.3 ARMA  8.03 0.6049  (6.77,9.29)  
Pareto  7.73 0.6351  (7.50,7.96)  
Exponential  8.79 0.6289  (6.27,11.31)  
25.3 ARMA  10.01  0.7171  (7.93,12.08)  
Pareto  10.41  0.7511  (9.59,11.23)  
Exponential  12.16  0.7429  (8.35,15.96)  
18.3 ARMA  20.39  0.9807  (16.28,24.50)  
Pareto  19.96  0.9897  (18,42,21.49)  
Exponential  19.37  0.9961  (13.55,25.20)  40 
 Table 4.14 Packet  loss with readout task input generated  from 2,304 FLPs  
 
Mean Interarrival 
Time (ms)  Traffic 
model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of 
End-to-end loss  
6598.2  ARMA  1.38 0.0026  (1.31,1.45)  
Pareto  1.37 0.0027  (1.30,1.44)  
Exponential  1.37 0.0027  (1.29,1.45)  
25.9 ARMA  21.16  0.6576  (20.53,21.79)  
Pareto  23.43  0.6899  (23.24,23.61)  
Exponential  23.73  0.6852  (23.16,24.30)  
22.1 ARMA  23.9 0.7774  (23.37,24.43)  
Pareto  28.03  0.7986  (27.82,28.24)  
Exponential  26.6 0.8088  (25.63,27.58)  
18.6 ARMA  27.46  0.9184  (27.21,27.71)  
Pareto  33.04  0.9570  (32.76,33.31)  
Exponential  30.52  0.9529  (29.79,31.25)  
 
4.5 Trace Data Results  
Trace data must also be tested in each scenario in order to compare which distribution  can 
best represent the real trace data. The criteria for trace data simulations are identical  to those 
for distribution simulations. Packet loss will be considered for various mean  interarrival 
times, the number of FLPs, and traffic source models fitted from the data. Five runs are 
repeated and the average of packet losses was calculated . 
 
4.5.1  First Scenario: Logstash wit h 192 FLPs  
Tables 4.15 to 4.18 demonstrate that the system can easily handle 192 FLPs with no 
packet loss. The default mean interarrival time shows that the end -to-end loss was zero 
before scaling down. When the traffic intensity per FLP increases to the p oint where the 
link utilization at the bottleneck exceeds 95%, end -to-end packet loss appears . 
 
 
 
 41 
 Table 4.15 Packet loss with readout -stfb-qc task input generated fr om 192 FLPs  
 
Mean Interarrival  
Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
3736.8  Default  0 0.0004  (0,0)  
1.9 Default  0.01 0.7840  (-0.01,0.04)  
1.7 Default  0.48 0.8822  (0.39,0.57)  
1.5 Default  2.77 0.9818  (2.55,3.00)  
 
Table 4.16 Packe t loss with readout -stfb task input generated from 1 92 FLPs  
 
Mean Interarrival  
Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4572.7  Default  0 0.0003  (0,0)  
2.3 Default  0 0.6407  (0,0)  
1.8 Default  0.10 0.8169  (0.08,0.13)  
1.5 Default  3.21 0.9719  (3.12,3.29)  
 
Table 4.17 Packet  loss with readout -qc task input generated from 1 92 FLPs  
 
Mean Interarrival  
Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4055.3  Default  0 0.0004  (0,0)  
2.0 Default  0 0.7224  (0,0)  
1.8 Default  1.77 0.8334  (1.65,1.89)  
1.5 Default  7.71 0.9823  (7.60,7.82)  
 
  42 
 Table 4.18 Packet loss with readout task input generated f rom 192 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
6088.6  Default  0 0.0002  (0,0)  
2.4 Default  0 0.6015  (0,0)  
1.8 Default  0 0.7940  (0,0)  
1.5 Default  2.74 0.9872  (2.66,2.82)  
 
4.5.2  Second Scenario: Logstash with 384 FLPs  
Tables 4.19 to 4.22 also show that the system can easily handle 384 FLPs without losing 
any packets. The end -to-end loss is also less than 0%, according to the default mean 
interarrival time. Furthermore, when the link utilization at the bottleneck exceeds 95%, 
end-to-end packet loss begins to rise.  
 
Table 4.19 Packet loss with readout -stfb-qc task input generated from  384 F LPs 
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
3756.5  Default  0 0.0008  (0,0)  
7.5 Default  0 0.3900  (0,0)  
3.8 Default  0.79 0.7793  (0.65,0.92)  
3.0 Default  7.27 0.9720  (6.83,7.70)  
 
Table 4.20 Packet loss with readout -stfb task input generated fro m 384 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4489.2  Default  0 0.0007  (0,0)  
4.5 Default  0.03 0.6527  (0.02,0.04)  
3.6 Default  0.43 0.8167  (0.40,0.45)  
3.0 Default  3.71 0.9842  (3.54,3.88)  
 43 
 Table 4.21 Packet loss with readout -qc task input generated  from 384 FLPs  
 
Mean  
Interarrival Time  (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4119.1  Default  0 0.0007  (0,0)  
8.2 Default  0 0.3556  (0,0)  
3.9 Default  0.80 0.7481  (0.64,0.96)  
3.0 Default  10.13  0.9851  (10.04,10.21)  
 
Table 4.22 Packe t loss with readout task input generated from 384 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
6103.4  Default  0 0.0005  (0,0)  
6.1 Default  0.06 0.4800  (0.05,0.07)  
4.1 Default  0.20 0.7200  (0.19,0.21)  
3.0 Default  4.13 0.9759  (4.05,4,21)  
 
4.5.3  Third Scenario: Logstash with 2,304 FLPs  
The system can also support 2,304 FLPs, as shown in Tables 4.23 to 4.26. The end -to-
end loss is less than 1%, which is acceptable, according to the default mean interarrival 
time. Due to the larger number of FLPs, when the link utilization at the bottlenec k reache s 
over 60%, end -to-end packet loss begins to appear when the mean interarrival time is 
scaled down. As a result, even with more scaling, the packet buffer size is insufficient to 
support a thousand FLPs.  
 
Table 4.23 Packet loss with readout -stfb-qc task input generated  from 2,304 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
6103.4  Default  0 0.0005  (0,0)  
6.1 Default  0.06 0.4800  (0.05,0.07)  
4.1 Default  0.20 0.7200  (0.19,0.21)  
3.0 Default  4.13 0.9759  (4.05,4,21)  44 
 Table 4.24 Packet  loss with readout -stfb task input generated f rom 2,304 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4593.8  Default  0.73 0.0038  (0.71,0.74)  
27.2 Default  17.45  0.6455  (17.24,17.66)  
21.8 Default  20.7 0.8064  (20.55,20.84)  
18.4 Default  23.4 0.9528  (23.21,23.59)  
 
Table 4.25 Packet loss with readout -qc task input generated from  2,304 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
4358.5  Default  0.67 0.0040  (0.66,0.69)  
26.2 Default  11.79  0.6699  (11.50,12.07)  
20.7 Default  17.48  0.8510  (17.33,17.63)  
17.7 Default  23.01  0.9953  (22.82,23.20)  
 
Table 4.26 Packet  loss with readout task input generated from  2,304 FLPs  
 
Mean  
Interarrival Time (ms)  Traffic model  End-to-end 
loss (%)  Queue 
Utilization  95% CI of End -
to-end loss  
6394.6  Default  0.90 0.0027  (0.89,0.91)  
24.3 Default  20.43  0.7221  (20.38,20.49)  
20 Default  23.60  0.8798  (23.49,23.72)  
17.6 Default  25.59  0.9968  (25.45,25.73)  
 
4.6 Model Verification  
In order to find the model that could best represent the real trace result, model verification 
must take into account end-to-end packet loss and queue utilization of each task. The 
horizontal axis on the graph represents queue utilization, while the vertical axis depicts 
end-to-end loss. The solid line depicts real trace data end -to-end loss, while the dashed 
line depicts distribution end -to-end loss. Each point is based on interarrival time scaling . 45 
 When compar ing the results of 192 and 384 FLPs in each task, the end-to-end packet loss  
increases dramatically when the queue utilization reaches 80% in both trace da ta and 
distribution. Figures 4.13 and 4.14  show that the more the interarrival time unit is scaled, 
the more queue utlizaiton occurs, resulting in more loss.  
 
 
 (a) readout -stfb-qc workflow   (b) readout -stfb workflow  
 
 
 (c) readout -qc workflow.   (d) readout workflow  
 
Figure 4.13 Packet loss and queue utilization plot of 192 FLPs  
 
  
 
  
 
  46 
  
 (a) readout -stfb-qc workflow  (b) readout -stfb workflow  
 
 
 (c) readout -qc workflow  (d) readout workflow  
 
Figure 4.14 Packet loss and queue utilization plot of 384 FLPs  
 
Due to the large number of FL Ps in third scenario, Figure 4.15  shows that end-to-end 
packet loss  occurs at the default level and begins to level up when queue utilization reaches  
60% in both trace data and distribution. Large FLPs and a smaller switch packet buffer 
size result in significant losses of approximately 20% in both distribution and trace data. 
The end -to-end loss rose because of sending packets with insufficient space.  Increasing 
the size of the switch packet buffer will help to minimize the increased loss  
  
 
  
 
  47 
  
 (a) readout -stfb-qc workflow  (b) readout -stfb workflow  
 
 
 (c) readout -qc workflow  (d) readout workflow  
 
Figure 4.15 Packet loss and queue utilization plot of 2,304 FLPs  
 
The distribution and trace data are compared. When comparing all workflows with the 
trace data in the first scenario, which included 192 FLPs, the time -series model or  ARMA 
produced the least amount of error. Readout -stfb-qc workflow, readout -stfb workflow,  
and readout workflow were  the most commonly overestimated. The readout -qc workflow 
was often underestimated.  
 
ARMA also had the highest similarity value in most of the workflows when compared to 
the trace data 384 FLPs in the second scenario. Similarly to the first scenario, readout -
stfb-qc workflow, readout -stfb workflow, and readout workflow are all overestimated in 
the second scenario. Finally, the readout -stfb-qc task and readout -qc task in the third 
scenario had the highest similarity values with the exponential distribution, with 2,304 
FLPs. In both the readout -stfb and readout tasks, ARMA had the highest similarity values .
  
 
  
 
 48 
 From all scenarios, the overestimated  values are more accurate than the underestimated 
values. An exact conclusion cannot be drawn from some scenarios. Except for the  readout -qc 
workflow, the time series model and ARMA gave the closest loss performance to that of 
the trace data. As a result, the time series model and ARMA appears to be the best 
representative for all workflows. In addition, the increased number of FLPs and the 
limited size of the buffer in the configuration may affect the level up and fluctuate in end -
to-end loss. As seen from  the results, increasing the FLPs to beyond the steady state with 
the limit switch buffer size caused a significant end -to-end loss . 
 
4.7 Modeling  the Relationship between the Loss and Number of FLPs  
In order to model end -to-end loss from the simulation ex periment design, we perform  
linear regression on the number of FLPs and end -to-end loss to guide the number of FLPs 
that the network can support for various types of tasks operating on FLPs.  
 
The fitted models for all the w orkflows are shown in Figure 4.1 6. In all the cases, the loss 
increases linearly with the number of FLPs. Figure 4. 16 shows how, if ALICE decides to 
expand the number of FLPs to 1000 nodes with 625 packets per second in the future, they 
will be aware that the readout -stfb-qc and readout -qc workflow will suffer a loss of 
around 10%. While the readout -stfb workflow and readout workflow in Figure 4. 16(b) 
and Figure 4. 16(d) will lose about 15%.  
 
  49 
  
 (a) readout -stfb-qc workflow  (b) readout -stfb workflow  
 
 
 (c) readout -qc workflow  (d) readou t workflow  
 
Figure 4.16 Fitted regression model for the number of FLPs and the end -to-end loss 
for different type of workflow  
 
 
 
 
 CHAPTER 5  CONCLUSION  
During the latest maintenance break, the ALICE experiment at the CERN LHC received 
a big update (2018 -2022). To handle the extremely large detector data input, a new 
Online -Offline (O2) computing system has been created. It consists of over 500 nodes that 
are responsible for data collection, aggr egation, and processing. CERN and King 
Mongkut's University of Technology Thonburi (KMUTT) have collaborated to develop 
a future AI -based logging system that monitors and identifies abnormal events using log 
data created from FLPs in ALICE O2 facilities. T he FLPs will be the primary target of 
the logging scheme.  
 
Our goal is to look at packet loss efficiency under various conditions, such as the numbe r of 
FLPs and the characteristics of generated log traffic and see if the network can sustain 
minimal or no  data loss. Since access to CERN's production facilities is restricted, a few 
FLPs were mounted in an OpenStack testbed area, and traffic produced by FLPs is  collected 
using the Packetbeat. After that, the traffic data is fitted to a variety of traffic -source 
models, which would be used to produce inputs for the simulation models. Our findings 
will help in determining a sustainable number of FLPs to operate various services for 
system growth, as well as how much traffic to expect.  
  
We hope to find out whic h traffic models will accurately reflect network traffic data 
generated by FLPs and use them to assess packet loss efficiency in this study. The  
OpenStack testbed environment, data acquisition from FLPs installed in the testbed, data 
preprocessing, and sta tistical properties are all described first. Then, using time -series and 
probabilistic models such as Pareto and exponential distribution, we  evaluated and matched 
the data to the chosen traffic models. Some data analysis also took into account factors 
such as the study of aggregated interarrival time in order to determine the recommended 
bandwidth and the interarrival times of each task are analyzed for correlation using 
autocorrelation function (ACF) and partial autocorrelation function (PACF) plots. To 
check for variations and seasonal results, the Dickey -Fuller Test was used. The collected 
data was then fitted into the chosen model.  
 51 
 The simulation tool used in the experiment is OMNET++, which is a discrete -event 
simulation tool. The average of packet lo sses with their 95 percent confidence intervals, 
as well as the bottleneck connection utilizations, was computed for each task simulation 
scenario after five runs. The chosen distribution and trace data was checked in the three 
scenarios defined by the exp eriment: 192 FLPs, 384 FLPs, and 2,304 FLPs.  
 
The results show that the default mean interarrival time experienced an acceptable end -
to-end loss of about 1% before scaling down. As the connection utilization at the  bottleneck 
approaches over 50%, end -to-end packet loss appears as the mean interarrival time is 
scaled down due to the greater number of FLPs. The time series model, also known as 
ARMA, is the most similar to trace data in most situations. As a result, ARMA tends to 
be the most accurate represen tation of all workflows. We conclude by deriving the 
summary equation of all workflows.  
 
