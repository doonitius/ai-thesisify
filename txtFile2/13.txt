  RISK-SENSITIVE PORTFOLIO MANAGEMENT BY USING DISTRIBUTIONAL REINFORCEMENT LEARNING
  The financial problem is a challenging task for many investors and researchers. In the last decade, many pieces of research adopted deep learning to build an autonomous trading system. Most researches applied the variance of supervised-learning techniques to forecast the stock prices and to build the trading systems based on the predicted results. Another widely used paradigm is reinforcement learning due to its ability to straightforwardly discover the relation between the market regime with the optimal trading signals. However, it is still unclear how to build risk-averse reinforcement learning to trade in the financial market. Many of previous works combined the moving average of the variance of the returns to the reward function to make the reinforcement learning to be risk-sensitive. Alternatively, this research proposed a novel methodology to build the risk-sensitive reinforcement learning by embedding the risk aversion into the policy rather than combining the risk to the reward function. To this end, this work utilized the categorical reinforcement learning (C51 algorithm) with an action selection method based on the Sharpe ratio to build the risk-averse trading system. The approach name was C21-SR because it used 21-bin categorical reinforcement learning with the Sharpe ratio policy. The empirical results in this work revealed that using the Sharpe ratio policy could gain more wealth than using the profit policy by average. This work also presented the comparative study to examine the effects of the exploration strategy methods and the separated neural network techniques on the performance of our proposed method.  Keywords: Deep Learning/ Distributional Reinforcement Learning / Portfolio Management
  CHAPTER 1 INTRODUCTION   Financial trading has been interesting by the investors and the financial institutes for a long time. Profitable and automatic systematic trading is the achievement of many traders.  In the last decades, deep learning or artificial intelligence has emerged. Due to its ability to model the complex function, it has become popular to solve several problems, as well as financial trading. The use of Artificial Intelligence (AI) with financial trading can be achieved in two main approaches; using supervised learning and using reinforcement learning.  In the supervised-learning area, the researchers built a trading strategy based on the predicted results. Although, the supervised-learning performance has been proved to be good for financial trading by many pieces of research (Bao, et al., 2017: Gudelek, et al., 2018: Obeidat, et al., 2018). There are the literatures which have presented that the reinforcement learning is better than the supervised-learning for addressing the financial trading problems (Moody, et al., 1997: Li, 2019: Dang, 2019). Reinforcement learning is the area of the artificial intelligence algorithm in addition to the supervise-learning and unsupervised-learning. It is the algorithm to model the relation between the state or the input features with the optimal actions. Thus, it can directly map the status of the financial market to the optimal trading signal, which leads us to the highest cumulative returns.  In the real-life trading, many investors may accept to decrease some of their expected profits for controlling the risks because the investors have the risk aversion behaviours. Unfortunately, it is still unclear how to build reinforcement learning to be risk-averse. The various of previous works attempted to embed the risk within the reward function of the reinforcement learning (Bertoluzzo, et al., 2012: Corazaa, et al., 2014: Jin, et al., 2016: Liang 2018).  This research proposed an alternative method to construct the risk-averse reinforcement learning by integrating the risk sensitivity into the policy rather than the reward function. To this end, this research applied a categorical reinforcement learning, which could model the distributions of the cumulative returns, and defined the policy by the Sharpe ratio of the distribution.  Hence, the contributions of this research were two-folds. First, to the best of our knowledge, this work was the first work which demonstrated the use of categorical reinforcement learning with financial portfolio management. Second, this research presented the risk-sensitive action selection approach by using the Sharpe ratio and performed the experiments to compare the performance among the action selection approaches.     2 The remaining parts of this report are organized as follow. Chapter 2 explains the preliminary theories and the previous researches in this field. Chapter 3 clarifies the proposed methodology and the experiment setup. Chapter 4 contains the results from the experiments and the analysis. Chapter 5 concludes the overall of this research.  Appendix contains the toy experiment for proving the concept of the proposed methodology, as well as the tables containing the full results from all experiments.  1.1 Statements of Problems - How to implement the categorical reinforcement learning to control the financial portfolio. - Is the policy by Sharpe ratios able to outperform the policy by the profits.  1.2 Objectives - To study the possibility of using the categorical reinforcement learning algorithm to deal with the portfolio management problems. - To investigate the performance among the methods to control the risks in the investments. - To explore the effects of the exploration strategies to the performance of categorical reinforcement learning in the financial market. - To examine the effects of using the separated neural network techniques to the performance of categorical reinforcement learning in the financial market.  1.3 Scopes This research is studied under the below restrictions to bound the scopes of the study. - This research is implemented under the simplified simulation financial market, which does not include the transaction fee, the leftover cash, the moment-to-moment bids&offers in the financial market into account. - The proposed method can manage the portfolio, which contains only 2 stocks.  1.4 Expected Benefits - This work is the initial work that uses distributional reinforcement learning to control the risk of financial investment. - This work provides the empirical results about the effects of each reinforcement learning factors to the performance of categorical reinforcement learning algorithm in the financial market. CHAPTER 2 LITERATURE & THEORY REVIEW   This chapter composes of two sections. The first section contains the preliminary theories involving in this research. The second section reviews the previous literature in the area of using artificial intelligence to trade in the financial market.  2.1  Preliminary Knowledge This subsection describes the preliminary, including the Markov Decision Process, Reinforcement learning, Q-learning, Deep Q Learning, and Categorical Reinforcement Learning (C51) algorithm.  2.1.1 The Markov Decision Process (MDP) To solve the problems with reinforcement learning, it is necessary to formalize the problem as the Markov decision process. The Markov decision process is the sequential stochastic control process which is the extension from the Markov chain by including the action and the reward. The action acts as the choices to move in the next state; each action gives the different state-transition and the reward probability.   The MDPs process can be formalized with 4-tuple, 01,3,4$(6,6%),8$(6,6%)9, where 1 is  the set of possible state, 3 is the set of possible action, 4$(6,6%) and 8$(6,6%) are the probability and the immediate reward of the state transition from s to s’ according to action :, respectively.  For more details, each time step in the MDP, the agent obtains the state. It then has to desire the action and take action to the environment. Each action provides different state-transition probability and the reward. The core problem of the MDP is to design the decision-maker system called the agent that can select the optimal action, which leads to the optimal long-term reward  The objective of the MDPs is shown in the equation 2.1, where ;&	is the reward at time = and γ is the reward discounted rate which is used to adjust the importance of the reward in the future.  >&=@A&;&'&()(2.1)  2.1.2 The Reinforcement Learning (RL) Reinforcement learning is the area of machine learning algorithm besides the supervised-learning and unsupervised-learning. The objective of the reinforcement learning algorithm is to discover the relationship between the state or the input feature and the   4 optimal action. The function that maps the state and the optimal action is policy function denoted as π.  Thus, the policy function is the function of the state (6&) that returns the optimal action (:&∗) as a result, as shown in the equation 2.2.  :&∗=E(6&)(2.2)  The reinforcement learning algorithms can be categorized to two main categories, namely the policy-based reinforcement learning and the value-based reinforcement learning. The policy-based reinforcement learning derives the policy directly from the trial-and-error experiences. The example policy-based algorithms are the policy gradient, REINFORCE algorithm. Whereas, the value-based reinforcement approximates the value function, which is the expected value of the cumulative return. The value function can be either the value of only the state or the state-action pair. Then, the value-based reinforcement learning derives the policy by the estimated value function. The example of value-based reinforcement learning algorithms are SARSA, Q-learning, Categorical reinforcement learning (C51) algorithm.   This research mainly focused on using the C51 algorithms, which is the extension from the Q-learning algorithm, to trade in the financial market and to control the risk of investment.  2.1.3  The Deep Q Learning The deep Q learning (DQN) algorithm is the most widely used value-based reinforcement learning. It is implemented based on the Q-learning algorithm. Thus, it would be beneficial to clarify the Q-learning firstly.   The Q-learning algorithm aims to estimate the state-action value called the Q value. Hence, the Q values can be formalized as the function of the state and action,  F(6&,:&). To approximate the Q value, the Bellman equation (2.3) and dynamic programming are used.  F(6&,:&)=G(>&|6	=	6&,:	=:&)=;&+AJ:K+F(6&,-,:)=;&+AF(6&,-,:∗)(2.3)  The steps to train the Q-learning algorithms are as follows: The Q table is firstly initiated with the random value. Then, the agent iteratively interacts with the MDP environments. In each iteration, the agent observes the state 6&; after that the agent has to choose the action :& and applies it to the environment. The environment returns the reward ;& and the next state 6&,-. The agent gathers all variables to an experience tuple, (6&,:&,;&,6&,-).  It then uses the experience tuple to calculate the target Q value by the Bellman equation and to calculate the temporal difference error (TD error) as shown in the equation below, where F(6&,:&) is the current Q values of state 6& and action :& in the table.  MNO;;P;=;&+F(6&,-,:∗)−F(6&,:&)(2.4)   5  The agent next gradually updates the Q value in the table by using the equation below, where S is the learning rate.  F(6&,:&)=F(6&,:&)+S(MN O;;P;)(2.5)  Given the satisfied Q values, the policy of Q-learning is to simply select the action with the highest Q values for the given input state. Thus, it can be formulate as in the equation below.  :&∗=E(6&)=:;VJ:K$!F(6&,:&)(2.6)  However, the Q-learning algorithm has a severe limitation. The limitation is that the Q-learning algorithm can solve only the finite-state problem, which rarely exists in the real world. Fortunately, Minh, et al. (2015) proposed the deep Q-learning to break the limitation of the Q learning algorithm. They utilized deep learning techniques to estimate the Q values. The neural network of deep Q-learning algorithm receives the state as the input. It predicts the Q values of each action according to the input state to the separated output nodes. For more clearly, the DQN architecture is shown in Figure 2.1.  
 Figure 2.1 The overview architecture of DQN algorithm  The Q value of each state-action, which is approximated by the neural network weight X, is typically denoted as F(6&,:&;X). The policy corresponding to F(6&,:&;X) is generally denoted as π(6&;θ).  The training process to predict the satisfied Q value in deep Q-learning is nearly similar to the training process of Q learning except the method to select the experience to update the network for predicting Q values. The authors of the DQN paper also proposed the experience replay method. This method is to store the experience tuple from each iteration in the memory, denoted as Z, and uniformly sample the batch of experience tuple from the memory to update the neural network instead of using the latest experience to update the neural network. 
  6  There are two essential advantages of using the experience replay method over using the latest experience to update the neural network. The first advantage is that it is more sample-efficient because one experience tuple can be used several times to update the neural network. The second reason is that the experience replay can break the correlation among the samples fed to the neural network. It also builds the independent and identically distributed (iid.) property of the sample to provoke the neural network efficiency (Minh, et al., 2015).  To update the neural network, the Bellman equation is used to calculate the target Q values of each experience tuple in the batch. Then, the gradient descent is performed on the mean squared error loss between the target and the current predicted Q values. The formula of network updating is shown in the equation below, where :∗ represents the optimal action in the next time step. Note that π(6&,-;θ) acts as the optimal action in the state of the next timestep.  X=X+S[.O(0!,$!,2!,0!"#)~5\]0;&+AF(6&,-,π(6&,-;θ);X)9−F(6&,:&;X)^6_(2.7)  However, the equation above has a critical problem that can cause instability in the training process. The problem is the moving of the target. It occurs because the target, 0;&+AF(6&,-,π(6&,-;θ);X)9, and the predicted value, F(6&,:&;X), are computed by the the same neural network, X.		Thus, every time the neural network is updated to shift the current predicted value to be close to the target value, it also moves the target value to the unknown direction.  To tackle this problem, Minh, et al. (2015) suggested using two identical neural networks, including the online network X and the target network X7.  At the beginning, both online and target network have identical weight. In each training iteration, the online network is used to desire the action and updated. The target network is only used for computing the target value. Therefore, the new updating function can be shown as below.  X=X+S[.O(0!,$!,2!,0!"#)~5\]0;&+AF(6&,-,π(6&,-;θ);X7)9−F(6&,:&;X)^6_(2.8)  There are two methods to sync the online network with the target network. The first method is hard updating. In the hard updating, the weight of target network will be fully replaced by the weight of online network at each defined interval. Another method is soft updating. The soft updating method is to gradually update the target network by the equation below, where b is the hyper-parameter which control the rate of change of the target network.  X7=(1−b)X7+bX(2.9)    7 2.1.4  Distributional Reinforcement learning Distributional reinforcement learning has been proposed by Bellemare, et al. (2017). It is an extension of the DQN by allowing the neural network to predict the vector representing the probability mass function of the cumulative reward, denoted Z, rather than only the scalar value of the expected cumulative reward value (e.g. Q values). Figure 2.2 shows the architecture of the C51 algorithm.  
 Figure 2.2 The overview architecture of the C51 algorithm  They used the probability mass function (PMF), which is discrete, instead of the probability density function (PDF), which is continuous, because the discrete distribution can model any shape of distribution (i.e. the bimodal distribution, the skewed distribution, etc.) and easily to model by using only the neural network. In opposition to the discrete distribution, the continuous distribution has to use the neural network to estimate the parametric of each distribution type.  To implement their algorithm, they binarized the range of possible cumulative reward to 51 bins. Then, they represented the probability mass function of the cumulative rewards by the 51-element vector. Each element in the vector represents the probability that the cumulative reward will fall into the corresponding value range of each bin.     To compute the target distribution used for computing the loss function, they also proposed the Bellman equation in the distributional scheme, as shown in the equation below.   d(6&,:&)=;&+Ad(6&,-,:∗)(2.10)  More obviously, the equation above contracts the optimal cumulative reward distribution of the next state, d(6&,-,:∗), with A factor. Then, it shifts the shrunk distribution, Ad(6&,-,:∗),  with the reward ;&.  
  8 To update the neural network, the current predicted distribution will be projected onto the target distribution for computing the the cross-entropy loss between two distribution which is used to perform the gradient descent. Figure 2.3 demonstrates the process of computing the target distribution.  
 Figure 2.3 The process for computing the target distribution (Source: Bellemare, et al. (2017))  The policy of this algorithm is to select the action with the highest expected cumulative return calculated by the distribution. The policy of this algorithm is shown in the equation below, where f8 denotes the center of g&9 bin and h8(6&,:) represents the probability that the cumulative reward from taking action : in state 6& will fall in g&9 bin.  E(6&)=:;VJ:K$G0d(6&,:)9=:;VJ:K$@f8h8(6&,:):-8()(2.11)  Note that equation 2.10 is just a mathematic representation. The original C51 algorithm paper also provided the pseudo code of the network updating process which is shown in Figure 2.4.  
 Figure 2.4 Pseudo code of C51 algorithm   (Source: Bellemare, et al. 2017) 
  9  The pseudo code can be explained in the step-by-step as follows: 1. Calculate the Q value of each action in the next state, F(K&,-,:) by equation ∑h8j8(K&,-,:),8 where h8 is the center of g&9 bin and j8(K&,-,:) is the probability that the cumulative reward from taking action : in state K&,- will fall in g&9 bin. In other words, j8(K&,-,:) is the values of g&9 bin in d(K&,-,:). 2. Select the optimal action :∗in the next state by the equation :;VJ:K$F(K&,-,:)  3. Initialize vector size k	with zeros (k is the number of bins). This vector, denoted as J, is used for storing the target distribution. 4. Iterate along each bin to calculate the target value of each bin by the following steps: 4.1. Shift the bin center by the equation ;&+Ah;, where l denotes the bin which is currently focused in each iteration; the new bin center denotes as Mmh;. 4.2. Clip the value of the new bin center with the n<8= and  n<$> to ensure that it is still within the range of the probability mass function.  4.3. Calculate the bin number in floating-point, denoted as f;, by equation (Mmh;−n_Jgp)/rh, where rh represents the bin size. 4.4. Compute the lower bin, denoted as s, and upper bin, denoted as t, by rounding down and up the bin number, respectively. 4.5. Increase the value of s&9 element in vector J by j;(K&,-,:∗)0t−f;9 4.6. Increase the value of t&9	element in vector J by j;(K&,-,:∗)0f;−s9 5. Predict the d(K&,:&), Note that each element in d(K&,:&) denotes as j8(K&,:&) 6. Compute the cross-entropy loss between vector J and d(K&,:&) by the equation −∑J8sPVj8(K&,:&)8.  2.1.5  The Exploration Strategies in Reinforcement Learning In several heuristic optimization algorithms, there are two prominent tasks for building the optimal solution for the given problem. The two tasks are exploration and exploitation. The exploration is to discover the new possible solution which may be better or worse than the current solution for ensuring that the current solution is not in the local-optimal or suboptimal. The exploitation is to make the benefit and the solid solution from the current knowledge from the exploration.  Especially in reinforcement learning, the agent has to explore the new path in the action space for discovering the new optimal sequential action. However, the agent cannot forever explore the environment due to the limitation of training time. The agent has to reduce their exploration and move forward to the optimal path for building the solid policy. Thus, the schedule of the exploration or the exploration strategies to control the agent is needed.   Two exploration strategies, including the epsilon greedy policy and the action augmentation, were focally studied.     10 The epsilon greedy policy is the classical exploration method. The method is to select the random action with u probability and select the optimal action with (1−u) probability, as shown in the equation below.  :&=v;:pwPJxg=ℎ	u&	j;Pf:fgs=zπ(6&)xg=ℎ(1−u&)j;Pf:fgs=z(2.12)  The value of u usually starts at 1.0. During the training, it will be gradually decreased by the defined schedule for leading the agent to the solid solution. To reduce the epsilon, we can reduce it linearly by subtracting it with the small amount, or exponentially by multiplying it with the real number, which is less than 1, as shown in the equation 2.13 (a) and (b), respectively.  															ϵ&=ϵ)−=∗|.			xℎ};}	|	g6	6J:ss	~:st}6	(g.}.0.001)(2.13a) 													ϵ&=ϵ&ω!		xℎ};}	|	g6	~:st}	s}66	=ℎ:p	1.0	(g.}.0.99)(2.13f)  The action augmentation was proposed by Huang (2018). They presented this method to particularly work with the financial trading problem. The action augmentation method is based on the “what-if” concept. It makes use of the fact that we exactly know the stock price in the next timestep because we were training the agent with the historical price. Thus, in each state, the agent does not only choose the optimal action but also chooses all actions and stores the experience received from each action within the memory. However, the next state of the agent only depends on the optimal action determined by the current policy. Figure 2.1 shows the illustration of the action augmentation exploration.  
 Figure 2.5 The action augmentation illustration  
  11 2.2 Literature Review In this section, the literature which used the artificial intelligence to trade in the financial market will be reviewed. From the literature survey, the literature can be categorized into two groups. The first group is the work that uses supervised-learning techniques, and the second group is the work that uses reinforcement learning techniques.  2.1.1 Trading Based on Supervised-Learning To make uses of supervised learning with the financial trading, many literatures have the same main task. The first task is to use the supervised learning to predict future information. The second task is to derive the trading action according to the prediction outcome. The literature in this field can be classified to two groups according to the type of prediction, including the classification, and the regression.  1. Classification Based Approach In the area of the classification-based approach, the researchers aimed to predict the trading signal or the trend of the financial asset by applying the various machine learning and deep learning. For instance, Alavi, et al. (2015) studied the ability of the machine learning algorithms to predict the future trend of the stock.  In the field of deep learning, Gudelek, et al. (2018) utilized the 2-dimension convolution neural network (CNN) to predict the trend of the stocks in the next timestep. Then, they used their defined trading rule to manage the multiple stock in the portfolio. Their rule is to sell all stocks which the forecast result is in a downtrend and buy the stock which has the maximum probability of being in an uptrend from the prediction result. Their algorithm performance can beat the Buy and Hold strategy. Nguyen, et al. (2019) used the LSTM with transfer learning to predict the price movement. The transfer learning in their work is based on the model which training on several stock data. Their method can surpass the traditional machine learning method such as the random forest classifier.   There are the literatures which uses the deep learning to extract the high-level feature from the market and uses the unsupervised-learning techniques to trade in the stock market. For example, Hu, et al. (2018) used CNN to extract the high-level feature from the image of several stock charts and clustering the vector. Then, they selected stock from each cluster to the portfolio by considering the historical Sharpe ratio.  2. Regression Based Approach In the field of price prediction by deep learning, both of recurrent neural network and CNN were used. Bao, et al. (2017) used the stacked autoencoders to extract the hidden feature in the financial time series. They next used the long-short term memory neural network (LSTM) to predict the closing price of the next day. They tested their proposed method in term of accuracy and profitability. They constructed the simple trading strategy which is to buy the stock if the predicted price is higher than the current price and sell if otherwise. Selvin et al. (2017) compared the performance of RNN, LSTM and CNN to   12 forecast the future price. Their results told us that the CNN outperforms the LSTM because CNN can learn the change in the trend, whereas the LSTM suffered from the dramatic change in the historical price.  Although many literatures focused on trading just one asset, there are still having the literatures which adopted the regression model to manage the financial portfolio with multiple assets. Obeidat, et al. (2018) proposed the framework to control the weight of the stock in the portfolio by using the prediction result. They separated their approach into two tasks. The first task is to utilize the Long-Short Term Memory neural network (LSTM) to predict the return of each asset in the future. Then, they applied the Mean-Variance Optimization framework to compute the weight of the assets according to the predicted result. They can gain more annualized profit and Sharpe ratio than the classical passive portfolio management approach.    2.1.1 Trading Based on Reinforcement Learning In this subsection, the researches which studied the financial trading based on the value-based reinforcement learning were examined. Most of the literature adopted the deep Q-Learning and its variance to trade the only single asset (long or short an asset) because the value-based reinforcement learning can solve only the finite action problem. However, each literature proposed their unique approach for improving the ability of the deep Q-learning in the financial market.  Li, et al. (2019) denoised the stock data with the autoencoder and used the LSTM to build the deep Q-learning for building the robust trading system. Wang, et al. (2017) used the long-term return as the reward function instead of the immediate return for stabilizing the training process. Huang (2018) adopt the deep recurrent Q-learning, which is an extension of deep Q-learning to trade in the Forex market. They also proposed the exploration method named action augmentation to training the agent. For instance, Lee applied multiple deep Q-learning to construct the full trading system which can provide the buy or sell signal and price.  Although the deep Q-learning has been widely used for a trading in an only single asset, there is the literature using the deep Q-learning to manage several stocks in the portfolio. Jin, et al. (2016) proposed the framework that uses the deep Q-learning to manage the portfolio between two stocks. They provided the discrete action set to manage the proportion of each stock in the portfolio. The action set in their work is the set of proportion to switch the capital between two assets. To manage the portfolio with more than two stocks, Gao, et al. (2020) divided the portfolio values into several parts and using the concept of permutation to design the action set. Thus, the action space in their work is very large which is not suitable with the DQN algorithm.   Another challenge in this field is to incorporate the risk in the Q-learning algorithm. As mentioned in the preliminary knowledge, the Q learning is the method to model the expected cumulative return, which can be calculated by the average of the cumulative   13 return from many experiences. Unfortunately, the risk is modelled by the variance of the return, which is not able to be computed directly by the Q-learning (Neuneier, 1998). Thus, many previous works attempted to embed the risk into the reward function. For example, Neuneier (1998) suggested using the logarithm of the ratio of the portfolio value between two consecutive days, instead of the percentage change of portfolio value because the logarithm penalizes the loss stronger than the gain. Jin, et al. (2016) and Liang (2018) punished the return by the variance of return in the last specific window as shown in equation 2.14, where 8& is the return and λ defines the weight of the punishment.  ;&=8@−λσ(8&);=∈[1,M](2.14)  The other risk-averse reward function is the Sharpe ratio over the last specific window, as shown in the equation below. It is the ratio between the mean and the standard deviation of the return in the last window. The Sharpe ratio reward was used by Bertoluzzo et al., (2012), Corazaa, et al. (2014), and Jin, et al. (2016).  ;&=Ñ(8&)σ(8&);=∈[1,M](2.15)  The effect of the reward function to the performance of the deep Q-learning was studied by Jin, et al. (2016). They compared the reward function shown in the equation 2.14 and 2.15. Their study revealed that the performance of the deep Q-Learning in the portfolio management problem significantly varies according to the weight of the risk penalty, the window length to calculate the reward function, and the design of reward function. Another comparative study of the reward function was shown in Liang (2018). In their work, they compared the reward function shown in the equation 2.14 and the percentage change of the portfolio value. Their result from Liang (2018) showed that the reward function shown in the equation 2.14 has no impact on the performance of their agent. In other words, the reward function from the equation 2.14 and the percentage return were not significantly different.  2.1.2 The Comparison between Supervised Learning and Reinforcement     Learning in Financial Market Although the trading strategy based on supervised learning gave a good performance in much literature, it has two downsides comparing to reinforcement learning in the financial trading fields (Moody, et al., 1997). The first drawback is that the supervised learning is an indirect method to optimize the cumulative profit. Note that, the supervised learning aims to minimize the loss function or the prediction error. Whereas, the reinforcement learning aims to maximize the given objective function, for instance, the profit. The second disadvantage is that the supervised learning method can optimize the profit in only a single timestep. In contrast, reinforcement learning is the method to build the decision-maker system, which is capable of maximizing the long-term or the cumulative profit.    14 There are some literatures showing the comparative studies between reinforcement learning and the supervised learning method to trade in the financial market. Moody, et al. (1997) compared between the Q-learning algorithm, which maximizes the profit, with the trading strategy based on the predicted price. Their result revealed that the former can significantly outperform the latter. Another comparative study was shown in Li, et al.  (2019). They compared the deep Q-learning with the trading strategy based on the prediction result by utilizing the LSTM for both Q-learning and supervised-learning. Their results revealed that the deep Q-learning could beat the supervised-learning in many experiments. CHAPTER 3 METHODOLOGY   In this chapter, the designs of problem or environment in this research is firstly explained. Secondly, the formulation of each MDP components, as well as the other factors of reinforcement learning, are described. Thirdly, the development process for training the agent to trade in the financial market are explained. Finally, the setup of the experiment are clarified.  This research proposed the alternative method to control the risk of an investment. Instead of combining the risk into the reward function, this research integrated the risk aversion into the policy by utilizing the categorical reinforcement learning to compute the distribution of the expected returns. From the distribution, the agent can determine the optimal action by using the Sharpe ratio rather than the expected profit. The name of proposed method is C21-SR because it uses 21-bin categorical reinforcement learning with the action selection strategy based on the Sharpe ratio.  This research also conducted comparative studies between the profit and Sharpe ratio maximization policy. Moreover, this research also explored the effects of using the separated network and the effect of exploration methods with the performance of categorical reinforcement learning in the financial market.  3.1  Problem Design The problem that this research needed the reinforcement agent to solve was the portfolio management problem.  However, the problem was necessary to be simplified due to the limitation of value-based reinforcement learning, which was the action space, had to be finite. Therefore, this research allowed the agent to manage the portfolio with just two stocks, including a volatile and a stable stock. Investment in volatile stock could gain much more profit than the investment in stable stock in the bull market. Whereas, the investment in the stable stock could save the capital when the market was in the downtrend. The example chart of the volatile stock and the stable stock is shown in Figure 3.1.  
 Figure 3.1 The example of a volatile and a stable stock 
  16  Hence, this research selected ten pairs of high- and low-volatility stocks in the large-cap segment from the  New York Stock Exchange (NYSE) market. Each stock in each pair was in a different segment to ensure the low correlations. The beta index was used to indicate the volatility of each stock; the high beta index meaned the high volatility and vice versa. The list of stock pairs used in this research is shown in Table 3.1. This research used the beta index provided by ABG Analytics website.  Table 3.1 The list of stock-pair used in this research Low-Beta Stock Beta Index High-Beta Stock SO 0.18 1.30 GS SPG 0.64 1.40 BLK VZ 0.55 1.27 COF WMT 0.55 1.30 HAL NEE 0.31 1.50 ME MCD 0.55 1.44 COP PG 0.56 1.48 MET EXC 0.50 1.44 CAT DUK 0.25 1.28 BAC CL 0.69 1.4 C  The historical data of all stock had been provided by Yahoo finance. The data composes of the daily Open, High, Low, Close prices, and the trading volume (OHLCV) of each stock from the year 2000 to 2019.  The data had been separated into three parts by time. The first part was the data from Jan 2000 to Dec 2009, which were used for training the reinforcement learning agent. The second part was the data from Jan 2010 to Dec 2013, which were used as the validation set to find a suitable number of episodes to train the agent. The third part was the data from Jan 2014 to Mar 2019, which were used for testing and measuring the performance.  However, the real stock market had several things to be considered; for instance, the amount and the price of bid and offer in the market. To simplify the problems, this research had been developed under three assumptions which were used in many previous works. The three assumptions were as follows: - The agent has no impacts on the market. - The agent was always able to buy the stock at the close price of last day and sell the stock at the close price in the current day. - The commission of trading was zero.    17 3.2  Markov Decision Process Formulation As mentioned in the relating theories, the problems need to be formulated to the MDPs for solving with reinforcement learning. Therefore, this section clarified the design of each MDP component of the problem in this research.  3.2.1 State (!!) The states or the input features used in this research composed of two components. The first component was the matrix size 20x10 containing the OHLCV of both stocks over the last 20 days. Each column in the matrix was standardized by Z-score. The second component was the 2-element vector indicating the current portfolio weight of both stocks.  3.2.2 Action ("!) As mentioned before, the value-based reinforcement learning was limited to work only with the finite action space. Hence, this research needed to design the discrete action which could adjust the stock weights in the portfolio. This research applied the similar concept of action set from Jin, et al. (2016) work. However, the number of possible actions had been reduced from seven actions in the original work to just three action in this work for decreasing the possible paths in the action space and accelerating the learning process.  The action sets used in this research were as follows: - Sell half of the low-volatility stock and buy the high-volatility stock with the corresponding amount. - Sell half of the high-volatility stock and buy the low-volatility stock with the corresponding amount. - Maintain the current portfolio weight or do nothing.  3.2.3 Reward (#!) The reward used in this research was the logarithm return. It was the fraction of the portfolio values on 2 successive days. The reward function is shown in the equation 3.1, where ~& was portfolio value at time =.  ;&=sPV\~&~&7-_(3.1)  The logarithm return had two advantages over the simple percentage return. The first advantage was that the logarithm return was more risk-averse than the percentage return, as suggested in Neuneier (1998). In this research, the logarithm return stretched the negative tail in the distribution so that the negative tail caused the variance of the returns than the positive tail. The second benefit of log return was that it had the time-additivity property, which matched with the objective function of the MDPs problem. Note that, the   18 objective function of the MDPs problem was the summation of the return across time, as shown the equation 2.1. 3.2.4 Policy ($) There were two choices of the policy function in this research. Both policies receive the expected distribution of the cumulative return from the categorical reinforcement learning as the input and return the optimal action according to their objective as the output. The first policy was to maximize profit. It could be archived by using the policy which is shown in the equation 2.11.   The second policy was proposed by this research. It aimed to maximize the Sharpe ratio. The Sharpe ratio was the portfolio performance indicator computed by the ratio between the expected profit and the risk of the investment. This was an alternative approach to control the risk by merging the risk sensitivity with the policy rather than embedding the risk into the reward function which has been done in the previous literatures. The equation 3.2 shows the policy by Share ratio.  π(6&)=:;VJ:K$G0d(6&,:)9σ0d(6&,:)9(3.2)  3.3  The Exploration Strategies This research also studied the ability of three exploration strategies which were epsilon-greedy policy, the action augmentation, and the combination of both methods. In the combination strategy, the agent chose the main action with the epsilon-greedy policy and explored the other actions with the action augmentation. The main path of the agent only depended on the main action. The combination method was attractive to be studied because it provided more experience of nonoptimal path than the other two methods. In other words, it explored both of the experience sequences of nonoptimal paths and the experience from all alternative action belonging to those paths, as shown in Figure 3.2.  Note that the epsilon-greedy explored only the sequence of the nonoptimal path. In contrast, the action-augmentation provided just the experience sequence in the optimal path and the experience from all nonoptimal action in the optimal path. Thus, the combination could gather more information on the nonoptimal path and might make the agent explore the environment faster than the other two options as shown in Fig. 3.2.   19 
 Figure 3.2 The combination approach between epsilon-greedy and  action augmentation  3.4  The Use of Separated Network This research also examined the influence of using separated network techniques to stabilize the training process. In the case of using a single network, this research used the equation below to compute the target distribution of the cumulative return, dÖ(6&,:&;X). Remind that E(6&,-;X) acted as the optimal action of the next state defined by the policy E corresponding to the value function estimator X, where E could be either profit maximization (equation 2.11) or Sharpe maximization (equation 3.2) policy.  dÖ(6&,:&;X)=;&+Ad(6&,-,E(6&,-;X);X)(3.3)  Note that the equation above was just the mathematical representation. The actual paradigm to compute the target distribution is shown by the pseudo-code in Figure 2.5.  Then, this research updated the network with the equation below.  X←X−S∇.O(0!,$!,2!,0!"#)∼5à;P66_}p=;Pjz]d(6&,:&;X),dÖ(6&,:&;X)^(3.4)  In the case of using the separated networks, the target distribution in the equation above was substituted by dÖ(6&,:&;X7), which was the target distribution computed by the target network, as shown in the equation below.  dÖ(6&,:&;X7)=;&+Ad(6&,-,E(6&,-;X);X7)(3.5)  This research used the hard-updating method, which was to clone the online network to the target network at every defined interval, to sync the target network with the online network.  
  20 3.5  The Neural Network Architecture. This research implemented the categorical reinforcement learning by using the neural network as shown in Figure 3.3.   
 Figure 3.3 The network architecture used in this research  First, the matrix of standardized OHLCV  would be fed to the 1-dimension convolution with the ReLu activation function to extract and to learn the relationship among each component in each column of the matrix. The Dropout layers, with a 25% dropout rate, was placed next to the CNN layer for reducing the overfitting problem. Later, the output from the CNN layer would be flatten to 1-dimension vector and merged with the input representing the current portfolio status. The concatenated vector then fed to the 200-node fully connected layer (FCC). To implement the categorical reinforcement algorithm, the output layer was separated to 3 groups according to the number of actions. Each output groups have 21 nodes which denoted the 21-bin probability mass function of the cumulative return from taking each action. The activation function of each output group was the Softmax function for making the summation of each output group equal to 1.0. To accelerate the training process, the Adam optimizer was used to update the neural network.  3.6  The Development Process This research classified the process to three phases, including the training, the validation, and the testing. All processes were done separately for each stock pairs. The development process of this research was specially developed for building the robust reinforcement learning agent to trade in the financial market. 
  21  3.6.1 Training Process In the training process, the agent had to trade in the training data through 1 million days. The 1 million days were divided into several episodes. Each episode began at the random day in the training set and started with a random portfolio fraction. The reason for separating the training day to the several episodes was to break the correlation among the samples in the experience memory, as suggested by the original DQN paper. The comparison between training the agent with the full length of data and with several small episodes is shown in Appendix A.  In each day of each episode, the agent selected the action with the exploration method for exploring the environment. After the agent took action each day, the agent then obtained the rewards and the next states. The agent next stored the experience tuples into its memory and uniformly sampled the batch of experience in memory to update the neural network. During the training, the agent was saved every 10,000 days. Thus, there were 100 saved agents after this process finished. Figure 3.4 shows the pseudo-code in the case of using epsilon-greedy exploration and separated network with hard-updating method.  
 Figure 3.4 The pseudo code of training process  
  22 3.6.2 Validation and Testing Process Note that training the reinforcement learning to trade in the stock market was not similar to training the reinforcement learning in the simulated environment such as the Atari game. In the latter case, the agent performance stably increased while the number of training episodes increased. Unfortunately, the performance between the training set and the unseen dataset might not been correlated in this case. The number of 1,000,000 days was actually too much to train the agent to learn the generalized trading strategy. It could lead the agent to overfit with the noise in the training set and make the agent poorly perform in the unseen market environment. Moreover, the appropriate number of the training day was difficult to be estimated.   To address this problem, all saved agents from the training periods were tested with the validation set. During testing the agent with the validation set, this research also trained the agent with the validation set as suggested by Wang, et. al (2017). The training procedure in this process was almost similar to which was used in the training period. But, the exploration strategies were different. In the validation process, the agent could not use the epsilon-greedy policy or the combination method. It selected only the optimal action because the objective of this process was to measure the performance of each model. Nonetheless, the action augmentation was used as the exploration strategy to maintain the exploration while testing the agent with validation. After finishing this process, the agent which were trained by both training and validation set were saved.  Moreover, this research also tested all saved agents without training it with the validation and observed its performance in the validation set. It was for ensuring that the validation process would never lose the good model from updating it with the new experience. In other words, if the model from the training set was good enough, it did not need to be updated again with the validation set. Note that updating the network did not always increase the performance of the agent. Particularly in the situations that the agent had to trade in the long episode as the validation set, the training process was quite unstable due to the high correlation among the samples in the experience memory.  Thus, there were 200 models after the validation process finished; 100 models were trained from only the training set, and another 100 models were trained from both training set validation set. The model which gained the highest corresponding objective value in the validation set would be selected to trade in the test set for measuring the performance. In other words, the models which were trained by the profit policy would be selected by considering the profit gained in the validation process; the models which were trained by the Sharpe policy would be selected by considering the Sharpe ratio gained in the validation process  3.7  Experiment Setup This section explains the experiment in this research. Firstly, it shows the list of all treatments to study the effects of each factor. Secondly, it illustrates the neural network   23 used in this research and the list of parameter setting. Finally, it presents all performance metrics and benchmark strategies.  3.7.1 List of All Treatments As mentioned before, this research explored the effects of three factors, including the policy, the use of the separated network, and the exploration strategies in the training process. There were 2 alternative policies, 3 exploration choices, and 2 levels of using the separated network. Thus, this research implemented 12 treatments which were all combinations of all factors, as shown in Table 3.2. Each treatment was done with ten blocks by using the development process presented in section 3.5. The blocks were the stock-pairs as shown in Table 3.1. Note that the word ‘Both’ in table below means the combination method.  Table 3.2 List of treatments Treatment Number Policy Separated Network Exploration Strategies in Training Process 1 Sharpe Ratio Yes Epsilon-Greedy 2 Sharpe Ratio Yes Action Augmentation 3 Sharpe Ratio Yes Both 4 Sharpe Ratio No Epsilon-Greedy 5 Sharpe Ratio No Action Augmentation 6 Sharpe Ratio No Both 7 Profit Yes Epsilon-Greedy 8 Profit Yes Action Augmentation 9 Profit Yes Both 10 Profit No Epsilon-Greedy 11 Profit No Action Augmentation 12 Profit No Both  3.7.2 Parameter setting The parameter setting of both deep learning and reinforcement learning used in this research are shown in Table 3.3.  Table 3.3 The parameters setting Parameter Value Episode Length 10 days in training process full period of data in the validation and testing process Size of Replay Memory 2,000 Interval for updating the target network 3,000 days in training process 100 days in validation process (Use only in case of using the separated network) Initial Epsilon 1.0   24 (In case of using the epsilon-greedy and the combination method) Minimum Epsilon 0.01 (In case of using the epsilon-greedy and the combination method) Epsilon decay schedule linearly decrease from 1.0 to 0.01 within first 300,000 training days (In case of using the epsilon-greedy and the combination method) The range of possible cumulative reward -0.1054 to 0.0953 (logarithm of -10.0% to +10.0%) Number of bins of the probability mass function (d(6,:)) 21 Learning rate 10-5 Batch size 32 Reward discounted rate 0.9  3.7.3 The Performance Metrics The performance of each experiment and benchmark were measured by using 3 metrics. - The profit: this was for measuring the profitability of the trading strategy. - The Sharpe ratio (SR): it was the risk-adjusted portfolio performance indicator. It could be computed by dividing the average by the standard deviation of the logarithm return over the trading period. The annual Sharpe ratio could be computed by multiplying √252 with the daily Sharpe ratio. - The Maximum Draw Down (MDD): this was to measure the risk of the trading strategy. It was the maximum percentage loss during the trading periods.  3.7.4 Benchmark Strategies To illustrate the performance of each experiment,  the result from this research would be compared with the result sfrom each one with four benchmark strategies. - Buy and Hold (BAH): this strategy started with the fifty-fifty percentage of each stock and maintained the number of shares along the trading period. It was used for measuring market performance.  - Constant Rebalance Portfolio (CRP): this method was the traditional portfolio management strategy. It began with the fifty-fifty percentage of each stock and maintained the fifty-fifty ratio of portfolio value along the trading period. More precisely, it sold the rising stock and bought the falling to maintain the ratio among the values of each stock in the portfolio. - The DQN algorithm with weighted-objective reward (DQN-WR): this approach was implemented following the Jin, et al. (2017) work. The reward function in this approach was risk-averse calculated by the equation 2.14. The weight to penalize the reward from the variance was 0.5, and the window   25 length to calculate the reward was 7 days. The neural network and parameter setting were similar to the treatment 1. It had just one different at output layer. The output layer of this benchmark had just 3 nodes according to 3 actions. - The DQN algorithm with Sharpe reward (DQN-SR): this approach was implemented similarly to the DQN-WR benchmark, except the reward function. The reward function of this work was the Sharpe ratio, as shown in the equation 2.15, computed by the return over the last 7 days. - The individual High-Beta and Low-Beta stocks: it indicated the performance of individual high- and low-volatility stock in the portfolio. CHAPTER 4 RESULTS AND DISCUSSIONS   This chapter contains the results of the experiments and the analysis. The 12 treatments and two benchmarks were run in order to study the impact of the policy, the exploration strategy, and the separated network on the performance of categorical reinforcement learning algorithm in the stock market. The results shown in this chapter were the average value of the result from ten blocks (ten stock-pairs) by each treatment. The individual results of each stock-pairs by each treatment are shown in Appendix B. The average of each performance metric by each treatment in the test set is shown in Table 4.1.  Table 4.1 The performance in test set ID Policy Separate Network Exploration  Strategy Avg. Profit Avg. Sharpe Avg. MDD 1 Sharpe y Epsilon Greedy 96.85% 0.680 26.40% 2 Sharpe y AA 74.50% 0.612 24.46% 3 Sharpe y Both 67.69% 0.531 21.80% 4 Sharpe n Epsilon Greedy 66.48% 0.538 26.38% 5 Sharpe n AA 79.17% 0.643 23.10% 6 Sharpe n Both 68.12% 0.530 24.56% 7 Profit y Epsilon Greedy 81.55% 0.596 25.68% 8 Profit y AA 55.48% 0.440 28.18% 9 Profit y Both 52.02% 0.401 27.39% 10 Profit n Epsilon Greedy 88.09% 0.610 25.08% 11 Profit n AA 71.97% 0.532 25.44% 12 Profit n Both 64.09% 0.507 25.90% 13 DQN-WR y Epsilon Greedy 71.44% 0.567 24.75% 14 DQN-SR y Epsilon Greedy 48.57% 0.376 32.44% Buy and Hold (BAH) 57.3% 0.540 24.95% Constant Rebalance Portfolio (CRP) 62.7% 0.569 25.14% Low Volatile Stock (Low-Beta) 77.4% 0.634 23.67% High Volatile Stock (High-Beta) 37.2% 0.231 40.89%  4.1  The Comparison between the C21 and the Benchmark Strategies. From Table 4.1, it showed that the categorical reinforcement learning could gain the profit and the Sharpe ratio more than the BAH. More precisely, the average profit of the treatment 1 to 12 was 72.17% ± 7.36%, while the profit gained by BAH was only 57.3%. Particularly, all of the profit gained by the categorical reinforcement learning with the   27 Sharpe ratio maximization (treatment 1 to 6) were more than the profit gained by the BAH and CRP. Whereas, the categorical reinforcement learning with the profit maximization could win the BAH in 4 treatments (treatment 7,10,11,12) from 6 treatments. Furthermore, Table 4.1 also reveals that the Sharpe policy could gain more profit than the BAH while maintaining the average MDD around 24.45% (average value from treatment 1 to 6) which was in the same level as the MDD of the Buy and Hold strategies.   Comparing to the DQN, both Sharpe and the profit policy could beat the trading algorithms based on the deep Q-learning in the same setting; treatment 1 and 7 gaining more profit and Sharpe ratio than treatment 13 and 14.   The example of the trading behavior of the proposed methodology and the other benchmarks in the testing set are shown in Figure 4.1. The upper graph illustrates the relative portfolio value versus trading day. The middle graph shows the relative price of both stocks in the portfolio. The bottom graph depicts the portfolio weight of the low volatile stocks. The solid line is the moving average, and the shaded line is the actual portfolio weight.  It disclosed how C21-SR algorithms optimally manage the portfolio. For example, it could escape from the loss by heavily investing in DUK at ~500th day. Moreover, it could follow the uptrend by transferring the capital to invest in BAC, which is shown at ~700th day.   
 Figure 4.1 The example trading chart of the proposed algorithm 
  28 4.2  The Impact of Each Factor The statistical analysis for the Sharpe ratio and the profit  response from the result of all treatment to study the effect of each factor to the performance of categorical reinforcement learning in the financial application was conducted. Table 4.2 and Figure 4.2 respectively illustrate the ANOVA table for the Sharpe ratio response and the effect of each factor on the Sharpe ratio.  Table 4.3 and Figure 4.3 demonstrate the ANOVA table for the profit response and the effects of each factor on the profit, respectively.  Table 4.2 The analysis of variance (ANOVA) for the Sharpe ratio response Source of Variation Degree of Freedom Sum of Square Mean Square F0 P-Value Block 9 5.54 0.62 13.39 0.0000 Policy (A) 1 0.17 0.17 3.65 0.0589 Separate Network (B) 1 0.01 0.01 0.18 0.6722 Exploration Method (C) 2 0.26 0.13 2.85 0.0627 AB 1 0.09 0.09 1.92 0.1690 AC 2 0.09 0.05 1.00 0.3727 BC 2 0.10 0.05 1.07 0.3463 ABC 2 0.01 0.01 0.12 0.8856 Error 99 4.55 0.05   Total 119 10.83      Table 4.3 The analysis of variance (ANOVA) for the profit response Source of Variation Degree of Freedom Sum of Square Mean Square F0 P-Value Block 9 110965.25 12329.47 9.12 0.0000 Policy (A) 1 1306.21 1306.21 0.97 0.3281 Separate Network (B) 1 80.31 80.31 0.06 0.8080 Exploration Method (C) 2 8422.23 4211.11 3.11 0.0488 AB 1 3037.01 3037.01 2.25 0.1372 AC 2 1480.51 740.25 0.55 0.5802 BC 2 2849.76 1424.88 1.05 0.3525 ABC 2 1056.30 528.15 0.39 0.6777 Error 99 133879.36 1352.32   Total 119 263076.93        29 
 Figure 4.2 The effect of each factor on the Sharpe ratio response   
 Figure 4.3 The effect of each factor on the profit response  4.2.1 The Impact of the Policy The results shown in Table 4.1 presented that the Sharpe ratio policy could gain more profit and Sharpe ratio than the profit policy in most cases, except the case of using the single network with epsilon greedy exploration. Figure 4.2(A) and Figure 4.3(A) respectively show the effect of the policy on the Sharpe ratio and the profit. Both figures shared the same pattern, which was that using the Sharpe policy was better than using the profit policy.   However, the policy significantly affected only the Sharpe response. In other words, it was not a significant factor for profit response. Table 4.2 shows that the policy affected the Sharpe ratio with P-Value 0.0589; it was significant at the 10 percent level. Whereas, Table 4.3 shows that the policy did not have significant effect on the profit as the high P-Value, 0.321.  
  30 4.2.2 The Impact of the Separated Network Figure 4.2(B) and 4.3(B) show nearly horizontal lines. It meant that the influence of the separated network on the Sharpe ratio and the profit was relatively small as comparing to the effect of the other factors.   Table 4.2 and Table 4.3 show that the P-Value of the separate network for the Sharpe and profit response, respectively, were 0.672 and 0.8080; it meant that the separate network technique had no significant effect on both responses.  However, Table 4.2 and Table 4.3 also reveal that the P-Value of the interaction between the policy and the separated network (AB) were 0.169 and 0.1372 which were relatively low as comparing to the P-Value of the other interaction effects. Thus, the interaction effect of the two factors was attractive to be explored.  
 Figure 4.4 The interaction effect of the policy and the separated network  Figure 4.4 depicts the interaction effect of the policy and the use of the separated networks. The green line in Figure 4.4 unveils that using the separated network could improve the performance of the categorical reinforcement learning with the Sharpe ratio policy. The average profit from treatment using the separate network with the Sharpe policy (treatment 1 to 3) was 79.68%, while the average profit from the treatment using the single network with the Sharpe policy (treatment 4 to 6) was 71.25%.  However, the effect of using the separated networks was in the opposite direction if the categorical reinforcement learning used the profit policy instead of the Sharpe policy. The red line in Figure 4.4 presents that using the single network with the profit policy was better than using the separated networks with profit policy. The average profit from the treatment which used the profit policy with the separated networks (treatment 7 to 9) was 
  31 63.07%. It was lower than the average profit from the treatments which use the profit policy with a single network (treatment 10 to 12), which was 74.72%.  The other interesting result was that the profit response is more sensitive to the use of the separate network than the Sharpe response. There was an intercept between the red and the green line in Figure 4.4(A), while Figure 4.4(B) shows no cross. It might be the cause of the high P-Value of the effect of policy on the profit, as shown in Table 4.3.  4.2.3 The Impact of the Exploration Method Table 4.1 shows that the epsilon-greedy could outperform the other two methods in many cases, except the case of using the Sharpe policy with a single network of which the action augmentation was the best method (treatment 5). In many cases, the epsilon-greedy was followed by the action augmentation and the combination method. For example, in the case of using Sharpe policy with the separated network (treatment 1 to 3), the treatment 1 which used the epsilon-greedy had more the profit and Sharpe ratio than the treatment 2 and 3 which used the action augmentation and the combination method, respectively. Table 4.2 and 4.3 indicates that exploration method was significant for the Sharpe response at 10 percent level and the profit response at 5 percent level.  On average, the epsilon-greedy exploration (treatment 1,4,7,10) could gain profit 83.24%, while the action augmentation (treatment 2,5,8,11) and the combination method (treatment 3,6,9,12) could obtain the profit 70.28% and 62.98%, respectively.   CHAPTER 5 CONCLUSIONS   This research investigated the possibility to adapt the distributional reinforcement learning algorithms to solve the simplified version of the portfolio management problem. This research also proposed a method of controlling the risk of an investment by defining the reinforcement learning policy as the Sharpe ratio.  The empirical results revealed that the categorical reinforcement learning with the Sharpe ratio policy could gain more Sharpe ratio than both risk-sensitive deep Q-learning and the profit maximization categorical reinforcement learning in the out-of-sample datasets. This research also studied the effect of using the separated networks and the three exploration strategies with the categorical reinforcement learning in financial trading. The result depicted that the epsilon-greedy policy could enhance the performance of the categorical reinforcement learning in the financial market. It also showed that using the separated networks could improve the performance of Sharpe maximization categorical reinforcement learning algorithm, whereas it degraded the performance of the profit maximization algorithms.  However, the proposed approach in this research still had several limitations. A critical limitation was that it could trade just two stocks. Moreover, it was tested with the simplified environment, which eliminated many of factors in the real financial environment. Thus, it needed to be further investigated under the real financial market.  This work can be improved in many directions. Firstly, it can be applied with the deep deterministic policy gradient (DDPG) algorithms, which is the actor-critic reinforcement learning. The DDPG algorithm allows the agent to work with continuous action-space, which is more suitable than the discrete space in the portfolio management problem. The Sharpe ratio policy can be adopted with the DDPG by changing the critic network to be the categorical reinforcement learning (Barth-Maron, et al. 2018) and to compute the gradient of the Sharpe ratio from the critic network to update the actor-network. Secondly, there are several extensions of deep Q-learning which does not in this study, for instance, the prioritized replay which can improve the sample efficient, the dueling deep Q-Learning which can enhance the stability of training process. Thirdly, there are varieties of the risk-sensitive performance indicators, which are interesting to be used with the categorical reinforcement learning instead of the Sharpe ratio, for example, the Sortio ratio. Lastly, there are many extensions of the categorical reinforcement learning in the present, for instance, the quantile regression and the implicit quantile regression which were proved to be better the categorical reinforcement learning in the Atari environment (Debney, et al. 2018). Especially, the implicit quantile regression was presented to work with the conditional value at the risk to build the risk-sensitive reinforcement learning. 